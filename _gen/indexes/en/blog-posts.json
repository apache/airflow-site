[{"content":"## The story behind the Airflow Breeze tool\nInitially, we started contributing to this fantastic open-source project [Apache Airflow] with a team of three which then grew to five. When we kicked it off a year ago, I realized pretty soon where the biggest bottlenecks and areas for improvement in terms of productivity were. Even with the help of our client, who provided us with a “homegrown” development environment it took us literally days to set it up and learn some basics.\n\nThat is how the journey to increased productivity in Apache Airflow began. The result? The Airflow Breeze open-source tool. Jarek Potiuk, an Airflow Committer, will tell you all about it.\n\nYou can learn [how and why it’s a \"Breeze\" to Develop Apache Airflow](https://higrys.medium.com/its-a-breeze-to-develop-apache-airflow-bf306d3e3505).\n","url":"Its-a-breeze-to-develop-apache-airflow","title":"It's a \"Breeze\" to develop Apache Airflow","linkTitle":"It's a \"Breeze\" to develop Apache Airflow","author":"Jarek Potiuk","twitter":"higrys","github":"potiuk","linkedin":"jarekpotiuk","description":"A Principal Software Engineer's journey to developer productivity. Learn how Jarek and his team sped up and simplified Airflow development for the community.","tags":["Development"],"date":"2019-11-22T00:00:00.000Z"},{"content":"","url":"_index","title":"Blog","linkTitle":"Blog","menu":{"main":{"weight":25}}},{"content":"Airflow 1.10.10 contains 199 commits since 1.10.9 and includes 11 new features, 43 improvements, 44 bug fixes, and several doc changes.\n\n**Details**:\n\n* **PyPI**: [https://pypi.org/project/apache-airflow/1.10.10/](https://pypi.org/project/apache-airflow/1.10.10/)\n* **Docs**: [https://airflow.apache.org/docs/1.10.10/](https://airflow.apache.org/docs/1.10.10/)\n* **Changelog**: [http://airflow.apache.org/docs/1.10.10/changelog.html](http://airflow.apache.org/docs/1.10.10/changelog.html)\n\nSome of the noteworthy new features (user-facing) are:\n\n- [Allow user to chose timezone to use in the RBAC UI](https://github.com/apache/airflow/pull/8046)\n- [Add Production Docker image support](https://github.com/apache/airflow/pull/7832)\n- [Allow Retrieving Airflow Connections & Variables from various Secrets backend](http://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html)\n- [Stateless Webserver using DAG Serialization](http://airflow.apache.org/docs/1.10.10/dag-serialization.html)\n- [Tasks with Dummy Operators are no longer sent to executor](https://github.com/apache/airflow/pull/7880)\n- [Allow passing DagRun conf when triggering dags via UI](https://github.com/apache/airflow/pull/7312)\n\n### Allow user to chose timezone to use in the RBAC UI\n\nBy default the Web UI will show times in UTC. It is possible to change the timezone shown by using the menu in the top\n right (click on the clock to activate it):\n\n**Screenshot**:\n![Allow user to chose timezone to use in the RBAC UI](rbac-ui-timezone.gif)\n\nDetails: https://airflow.apache.org/docs/1.10.10/timezone.html#web-ui\n\n**Note**: This feature is only available for the RBAC UI (enabled using `rbac=True` in `[webserver]` section in your `airflow.cfg`).\n\n### Add Production Docker image support\n\nThere are brand new production images (alpha quality) available for Airflow 1.10.10. You can pull them from the\n[Apache Airflow Dockerhub](https://hub.docker.com/r/apache/airflow) repository and start using it.\n\nMore information about using production images can be found in https://github.com/apache/airflow/blob/master/IMAGES.rst#using-the-images. Soon it will be updated with\ninformation how to use images using official helm chart.\n\nTo pull the images you can run one of the following commands:\n\n- `docker pull apache/airflow:1.10.10-python2.7`\n- `docker pull apache/airflow:1.10.10-python3.5`\n- `docker pull apache/airflow:1.10.10-python3.6`\n- `docker pull apache/airflow:1.10.10-python3.7`\n- `docker pull apache/airflow:1.10.10` (uses Python 3.6)\n\n### Allow Retrieving Airflow Connections & Variables from various Secrets backend\n\nFrom Airflow 1.10.10, users would be able to get Airflow Variables from Environment Variables.\n\nDetails: https://airflow.apache.org/docs/1.10.10/concepts.html#storing-variables-in-environment-variables\n\nA new concept of Secrets Backend has been introduced to retrieve Airflow Connections and Variables.\n\nFrom Airflow 1.10.10, users can retrieve Connections & Variables using the same sy","url":"airflow-1.10","title":"Apache Airflow 1.10.10","linkTitle":"Apache Airflow 1.10.10","author":"Kaxil Naik","twitter":"kaxil","github":"kaxil","linkedin":"kaxil","description":"We are happy to present Apache Airflow 1.10.10","tags":["release"],"date":"2020-04-09T00:00:00.000Z"},{"content":"Airflow 1.10.12 contains 113 commits since 1.10.11 and includes 5 new features, 23 improvements, 23 bug fixes,\nand several doc changes.\n\n**Details**:\n\n* **PyPI**: [https://pypi.org/project/apache-airflow/1.10.12/](https://pypi.org/project/apache-airflow/1.10.12/)\n* **Docs**: [https://airflow.apache.org/docs/1.10.12/](https://airflow.apache.org/docs/1.10.12/)\n* **Changelog**: [http://airflow.apache.org/docs/1.10.12/changelog.html](http://airflow.apache.org/docs/1.10.12/changelog.html)\n\n\n**Airflow 1.10.11 has breaking changes with respect to\nKubernetesExecutor & KubernetesPodOperator so I recommend users to directly upgrade to Airflow 1.10.12 instead**.\n\nSome of the noteworthy new features (user-facing) are:\n\n- [Allow defining custom XCom class](https://github.com/apache/airflow/pull/8560)\n- [Get Airflow configs with sensitive data from Secret Backends](https://github.com/apache/airflow/pull/9645)\n- [Add AirflowClusterPolicyViolation support to Airflow local settings](https://github.com/apache/airflow/pull/10282)\n\n### Allow defining Custom XCom class\n\nUntil Airflow 1.10.11, the XCom data was only stored in Airflow Metadatabase. From Airflow 1.10.12, users\nwould be able to define custom XCom classes. This will allow users to transfer larger data between tasks.\nAn example here would be to store XCom in S3 or GCS Bucket if the size of data that needs to be stored is larger\nthan `XCom.MAX_XCOM_SIZE` (48 KB).\n\n**PR**: https://github.com/apache/airflow/pull/8560\n\n### Get Airflow configs with sensitive data from Secret Backends\n\nUsers would be able to get the following Airflow configs from Secrets Backend like Hashicorp Vault:\n\n   - `sql_alchemy_conn` in [core] section\n   - `fernet_key` in [core] section\n   - `broker_url` in [celery] section\n   - `flower_basic_auth` in [celery] section\n   - `result_backend` in [celery] section\n   - `password` in [atlas] section\n   - `smtp_password` in [smtp] section\n   - `bind_password` in [ldap] section\n   - `git_password` in [kubernetes] section\n\nFurther improving Airflow's Secret Management story, from Airflow 1.10.12, users don't need to hardcode\nthe **sensitive** config value in airflow.cfg nor then need to use an Environment variable to set this config.\n\nFor example, the metadata database connection string can either be set in airflow.cfg like this:\n\n```ini\n[core]\nsql_alchemy_conn_secret = sql_alchemy_conn\n```\nThis will retrieve config option from the set Secret Backends.\n\nAs you can see you just need to add a `_secret` suffix at the end of the actual config option\nand the value needs to be the **key** which the Secrets backend will look for.\n\nSimilarly, `_secret` config options can also be set using a corresponding environment variable. For example:\n\n```\nexport AIRFLOW__CORE__SQL_ALCHEMY_CONN_SECRET=sql_alchemy_conn\n```\n\nMore details: http://airflow.apache.org/docs/1.10.12/howto/set-config.html\n\n### Add AirflowClusterPolicyViolation support to airflow_local_settings.py\n\nUsers can use Cluster Policies to apply clust","url":"airflow-1.10","title":"Apache Airflow 1.10.12","linkTitle":"Apache Airflow 1.10.12","author":"Kaxil Naik","twitter":"kaxil","github":"kaxil","linkedin":"kaxil","description":"We are happy to present Apache Airflow 1.10.12","tags":["release"],"date":"2020-08-25T00:00:00.000Z"},{"content":"Airflow 1.10.8 contains 160 commits since 1.10.7 and includes 4 new features, 42 improvements, 36 bug fixes, and several doc changes.\n\nWe released 1.10.9 on the same day as one of the Flask dependencies (Werkzeug) released 1.0 which broke Airflow 1.10.8.\n\n**Details**:\n\n* **PyPI**: [https://pypi.org/project/apache-airflow/1.10.9/](https://pypi.org/project/apache-airflow/1.10.9/)\n* **Docs**: [https://airflow.apache.org/docs/1.10.9/](https://airflow.apache.org/docs/1.10.9/)\n* **Changelog (1.10.8)**: [http://airflow.apache.org/docs/1.10.8/changelog.html#airflow-1-10-8-2020-01-07](http://airflow.apache.org/docs/1.10.8/changelog.html#airflow-1-10-8-2020-01-07)\n* **Changelog (1.10.9)**: [http://airflow.apache.org/docs/1.10.9/changelog.html#airflow-1-10-9-2020-02-10](http://airflow.apache.org/docs/1.10.9/changelog.html#airflow-1-10-9-2020-02-10)\n\nSome of the noteworthy new features (user-facing) are:\n\n- [Add tags to DAGs and use it for filtering in the UI (RBAC only)](https://github.com/apache/airflow/pull/6489)\n- [New Executor: DebugExecutor for Local debugging from your IDE](http://airflow.apache.org/docs/1.10.9/executor/debug.html)\n- [Allow passing conf in \"Add DAG Run\" (Triggered Dags) view](https://github.com/apache/airflow/pull/7281)\n- [Allow dags to run for future execution dates for manually triggered DAGs (only if `schedule_interval=None`)](https://github.com/apache/airflow/pull/7038)\n- [Dedicated page in documentation for all configs in airflow.cfg](https://airflow.apache.org/docs/1.10.9/configurations-ref.html)\n\n### Add tags to DAGs and use it for filtering in the UI\n\nIn order to filter DAGs (e.g by team), you can add tags in each dag. The filter is saved in a cookie and can be reset by the reset button.\n\nFor example:\n\nIn your Dag file, pass a list of tags you want to add to DAG object:\n\n```python\ndag = DAG(\n    dag_id='example_dag_tag',\n    schedule_interval='0 0 * * *',\n    tags=['example']\n)\n```\n\n**Screenshot**:\n![Add filter by DAG tags](airflow-dag-tags.png)\n\n**Note**: This feature is only available for the RBAC UI (enabled using `rbac=True` in `[webserver]` section in your `airflow.cfg`).\n\n\n## Special Note / Deprecations\n\n### Python 2\nPython 2 has reached end of its life on Jan 2020. Airflow Master no longer supports Python 2.\nAirflow 1.10.* would be the last series to support Python 2.\n\nWe strongly recommend users to use Python >= 3.6\n\n### Use Airflow RBAC UI\nAirflow 1.10.9 ships with 2 UIs, the default is non-RBAC Flask-admin based UI and Flask-appbuilder based UI.\n\nThe Flask-AppBuilder (FAB) based UI is allows Role-based Access Control and has more advanced features compared to\nthe legacy Flask-admin based UI. This UI can be enabled by setting `rbac=True` in `[webserver]` section in your `airflow.cfg`.\n\nFlask-admin based UI is deprecated and new features won't be ported to it. This UI will still be the default\nfor 1.10.* series but would no longer be available from Airflow 2.0\n\n\n## List of Contributors\n\nAccording to git shortlog, the f","url":"airflow-1.10.8-1.10","title":"Apache Airflow 1.10.8 & 1.10.9","linkTitle":"Apache Airflow 1.10.8 & 1.10.9","author":"Kaxil Naik","twitter":"kaxil","github":"kaxil","linkedin":"kaxil","description":"We are happy to present the new 1.10.8 and 1.10.9 releases of Apache Airflow.","tags":["release"],"date":"2020-02-23T00:00:00.000Z"},{"content":"I'm happy to announce that Apache Airflow 2.10.0 is now available, bringing an array of noteworthy enhancements and new features that will greatly serve our community.\n\n**Details**:\n\n📦 PyPI: <https://pypi.org/project/apache-airflow/2.10.0/> \\\n📚 Docs: <https://airflow.apache.org/docs/apache-airflow/2.10.0/> \\\n🛠 Release Notes: <https://airflow.apache.org/docs/apache-airflow/2.10.0/release_notes.html> \\\n🐳 Docker Image: \"docker pull apache/airflow:2.10.0\" \\\n🚏 Constraints: <https://github.com/apache/airflow/tree/constraints-2.10.0>\n\n## Airflow now collects Telemetry data by default\n\nWith the release of Airflow 2.10.0, we’ve introduced the collection of basic telemetry data, as outlined [here](https://airflow.apache.org/docs/apache-airflow/2.10.0/faq.html#does-airflow-collect-any-telemetry-data). This data will play a crucial role in helping Airflow maintainers gain a deeper understanding of how Airflow is utilized across various deployments. The insights derived from this information are invaluable in guiding the prioritization of patches, minor releases, and security fixes. Moreover, this data will inform key decisions regarding the development roadmap, ensuring that Airflow continues to evolve in line with community needs.\n\nFor those who prefer not to participate in data collection, deployments can easily opt-out by setting the `[usage_data_collection] enabled` option to `False` or by using the `SCARF_ANALYTICS=false` environment variable.\n\n\n## Multiple Executor Configuration (formerly \"Hybrid Execution\")\n\nEach executor comes with its unique set of strengths and weaknesses, typically balancing latency, isolation, and compute efficiency. Traditionally, an Airflow environment is limited to a single executor, requiring users to make trade-offs, as no single executor is perfectly suited for all types of tasks.\n\nWe are introducing a new feature that allows for the concurrent use of multiple executors within a single Airflow environment. This flexibility enables users to take advantage of the specific strengths of different executors for various tasks, improving overall efficiency and mitigating weaknesses. Users can set a default executor for the entire environment and, if necessary, assign particular executors to individual DAGs or tasks.\n\nTo configure multiple executors we can pass comma separated list in airflow configuration. The first executor in the list will be the default executor for the environment.\n\n```\n[core]\nexecutor = 'LocalExecutor,CeleryExecutor'\n```\n\nTo make it easier for dag authors, we can also specify aliases for executors that can be specified in the executor configuration\n\n```commandline\n[core]\nexecutor = 'LocalExecutor,KubernetesExecutor,my.custom.module.ExecutorClass:ShortName'\n```\n\nDAG authors can specify executors to use at the task\n\n```python\nBashOperator(\n    task_id=\"hello_world\",\n    executor=\"ShortName\",\n    bash_command=\"echo 'hello world!'\",\n)\n\n@task(executor=\"KubernetesExecutor\")\ndef hello_world():\n    print(\"hello ","url":"airflow-2.10","title":"Apache Airflow 2.10.0 is here","linkTitle":"Apache Airflow 2.10.0 is here","author":"Utkarsh Sharma","github":"utkarsharma2","linkedin":"utkarsh-sharma-5791ab8a","description":"Apache Airflow 2.10.0 is a game-changer, with powerful Dataset improvements and the groundbreaking Hybrid Executor, set to redefine your workflow capabilities!","tags":["Release"],"date":"2024-08-08"},{"content":"I’m proud to announce that Apache Airflow 2.2.0 has been released. It contains over 600 commits since 2.1.4 and includes 30 new features, 84 improvements, 85 bug fixes, and many internal and doc changes.\n\n**Details**:\n\n📦 PyPI: https://pypi.org/project/apache-airflow/2.2.0/ \\\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.2.0/ \\\n🛠️ Changelog: https://airflow.apache.org/docs/apache-airflow/2.2.0/changelog.html \\\n🐳 Docker Image: docker pull apache/airflow:2.2.0 \\\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.2.0\n\nAs the changelog is quite large, the following are some notable new features that shipped in this release.\n\n## Custom Timetables (AIP-39)\n\nAirflow has historically used cron expressions and timedeltas to represent when a DAG should run. This worked for a lot of use cases, but not all. For example, running daily on Monday-Friday, but not on weekends wasn’t possible.\n\nTo provide more scheduling flexibility, determining when a DAG should run is now done with Timetables. Of course, backwards compatibility has been maintained - cron expressions and timedeltas are still fully supported, however, timetables are pluggable so you can add your own custom timetable to fit your needs! For example, you could write a timetable to schedule a DagRun\n\n`execution_date` has long been confusing to new Airflowers, so as part of this change a new concept has been added to Airflow to replace it named `data_interval`, which is the period of data that a task should operate on. The following are now available:\n\n- `logical_date` (aka `execution_date`)\n- `data_interval_start` (same value as `execution_date` for cron)\n- `data_interval_end` (aka `next_execution_date`)\n\nIf you write your own timetables, keep in mind they should be idempotent and fast as they are used in the scheduler to create DagRuns.\n\nMore information can be found at: [Customizing DAG Scheduling with Timetables](https://airflow.apache.org/docs/apache-airflow/stable/howto/timetable.html)\n\n## Deferrable Tasks (AIP-40)\n\nDeferrable tasks allows operators or sensors to defer themselves until a light-weight async check passes, at which point they can resume executing. Most importantly, this results in the worker slot, and most notably any resources used by it, to be returned to Airflow. This allows simple things like monitoring a job in an external system or watching for an event to be much cheaper.\n\nTo support this feature, a new component has been added to Airflow, the triggerer, which is the daemon process that runs the asyncio event loop.\n\nAirflow 2.2.0 ships with 2 deferrable sensors, `DateTimeSensorAsync` and `TimeDeltaSensorAsync`, both of which are drop-in replacements for the existing corresponding sensor.\n\nMore information can be found at:\n\n[Deferrable Operators & Triggers](https://airflow.apache.org/docs/apache-airflow/stable/concepts/deferring.html)\n\n## Custom `@task` decorators and `@task.docker`\n\nAirflow 2.2.0 allows providers to create custom `@task` decora","url":"airflow-2.2","title":"What's new in Apache Airflow 2.2.0","linkTitle":"What's new in Apache Airflow 2.2.0","author":"Jed Cunningham","github":"jedcunningham","linkedin":"jedidiah-cunningham","description":"We're proud to announce that Apache Airflow 2.2.0 has been released.","tags":["Release"],"date":"2021-10-11"},{"content":"Apache Airflow 2.3.0 contains over 700 commits since 2.2.0 and includes 50 new features, 99 improvements, 85 bug fixes, and several doc changes.\n\n**Details**:\n\n📦 PyPI: https://pypi.org/project/apache-airflow/2.3.0/ \\\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.3.0/ \\\n🛠️ Release Notes: https://airflow.apache.org/docs/apache-airflow/2.3.0/release_notes.html \\\n🐳 Docker Image: docker pull apache/airflow:2.3.0 \\\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.3.0\n\nAs the changelog is quite large, the following are some notable new features that shipped in this release.\n\n## Dynamic Task Mapping(AIP-42)\n\nThere's now first-class support for dynamic tasks in Airflow. What this means is that you can generate tasks dynamically at runtime. Much like using a `for` loop\nto create a list of tasks, here you can create the same tasks without having to know the exact number of tasks ahead of time.\n\nYou can have a `task` generate the list to iterate over, which is not possible with a `for` loop.\n\nHere is an example:\n\n```python\n@task\ndef make_list():\n    # This can also be from an API call, checking a database, -- almost anything you like, as long as the\n    # resulting list/dictionary can be stored in the current XCom backend.\n    return [1, 2, {\"a\": \"b\"}, \"str\"]\n\n\n@task\ndef consumer(arg):\n    print(list(arg))\n\n\nwith DAG(dag_id=\"dynamic-map\", start_date=datetime(2022, 4, 2)) as dag:\n    consumer.expand(arg=make_list())\n```\n\nMore information can be found here: [Dynamic Task Mapping](https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html)\n\n## Grid View replaces Tree View\n\nGrid view replaces tree view in Airflow 2.3.0.\n\n**Screenshots**:\n![The new grid view](grid-view.png)\n\n## Purge history from metadata database\n\nAirflow 2.3.0 introduces a new `airflow db clean` command that can be used to purge old data from the metadata database.\n\nYou would want to use this command if you want to reduce the size of the metadata database.\n\nMore information can be found here: [Purge history from metadata database](https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#purge-history-from-metadata-database)\n\n## LocalKubernetesExecutor\n\nThere is a new executor named LocalKubernetesExecutor. This executor helps you run some tasks using LocalExecutor and run another set of tasks using the KubernetesExecutor in the same deployment based on the task's queue.\n\nMore information can be found here: [LocalKubernetesExecutor](https://airflow.apache.org/docs/apache-airflow/2.3.0/executor/local_kubernetes.html)\n\n\n## DagProcessorManager as standalone process (AIP-43)\n\nAs of 2.3.0, you can run the DagProcessorManager as a standalone process. Because DagProcessorManager runs user code, separating it from the scheduler process and running it as an independent process in a different host is a good idea.\n\nThe `airflow dag-processor` cli command will start a new process that will run the DagProcessorManager in a separate pr","url":"airflow-2.3","title":"Apache Airflow 2.3.0 is here","linkTitle":"Apache Airflow 2.3.0 is here","author":"Ephraim Anierobi","github":"ephraimbuddy","linkedin":"ephraimanierobi","description":"We're proud to announce that Apache Airflow 2.3.0 has been released.","tags":["Release"],"date":"2022-04-30"},{"content":"Apache Airflow 2.4.0 contains over 650 \"user-facing\" commits (excluding commits to providers or chart) and over 870 total. That includes 46 new features, 39 improvements, 52 bug fixes, and several documentation changes.\n\n**Details**:\n\n📦 PyPI: https://pypi.org/project/apache-airflow/2.4.0/ \\\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.4.0/ \\\n🛠️ Release Notes: https://airflow.apache.org/docs/apache-airflow/2.4.0/release_notes.html \\\n🐳 Docker Image: docker pull apache/airflow:2.4.0 \\\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.4.0\n\n## Data-aware scheduling (AIP-48)\n\nThis one is big. Airflow now has the ability to schedule DAGs based on other tasks updating datasets.\n\nWhat does this mean, exactly? This is a great new feature that lets DAG authors create smaller, more self-contained DAGs, which chain together into a larger data-based workflow. If you are currently using `ExternalTaskSensor` or `TriggerDagRunOperator` you should take a look at datasets -- in most cases you can replace them with something that will speed up the scheduling!\n\nBut enough talking, lets have a short example. First lets write a simple DAG with a task called `my_task` that produces a dataset called `my-dataset`:\n\n```python\nfrom airflow import Dataset\n\n\ndataset = Dataset(uri='my-dataset')\n\nwith DAG(dag_id='producer', ...)\n    @task(outlets=[dataset])\n    def my_task():\n        ...\n```\n\nDatasets are defined by a URI. Now, we can create a second DAG (`consumer`) that gets scheduled whenever this dataset changes:\n\n```python\n\nfrom airflow import Dataset\n\n\ndataset = Dataset(uri='my-dataset')\n\nwith DAG(dag_id='dataset-consumer', schedule=[dataset]):\n    ...\n```\n\nWith these two DAGs, the instant `my_task` finishes, Airflow will create the DAG run for the `dataset-consumer` workflow.\n\nWe know that what exists right now won't fit all use cases that people might wish for datasets, and in the coming minor releases (2.5, 2.6, etc.) we will expand and improve upon this foundation.\n\nDatasets represent the abstract concept of a dataset, and (for now) do not have any direct read or write capability - in this release we are adding the foundational feature that we will build upon in the future - and it's part of our goal to have smaller releases to get new features in your hands sooner!\n\nFor more information on datasets, see the [documentation on Data-aware scheduling][data-aware-scheduling]. That includes details on how datasets are identified (URIs), how you can depend on multiple datasets, and how to think about what a dataset is (hint: don't include \"date partitions\" in a dataset, it's higher level than that).\n\n[data-aware-scheduling]: https://airflow.apache.org/docs/apache-airflow/2.4.0/concepts/datasets.html\n\n## Easier management of conflicting python dependencies using the new ExternalPythonOperator\n\nAs much as we wish all python libraries could be used happily together that sadly isn't the world we live in, and sometimes there are conflicts when ","url":"airflow-2.4","title":"Apache Airflow 2.4.0: That Data Aware Release","linkTitle":"Apache Airflow 2.4.0","author":"Ash Berlin-Taylor","github":"ashberlin","twitter":"ashberlin","linkedin":"ashberlin-taylor","description":"We're proud to announce that Apache Airflow 2.4.0 has been released with many exciting improvements.","tags":["Release"],"date":"2022-09-19"},{"content":"Apache Airfow 2.5 has just been released, barely two and a half months after 2.4!\n\n\n**Details**:\n\n📦 PyPI: https://pypi.org/project/apache-airflow/2.5.0/ \\\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.5.0/ \\\n🛠️ Release Notes: https://airflow.apache.org/docs/apache-airflow/2.5.0/release_notes.html \\\n🐳 Docker Image: docker pull apache/airflow:2.5.0 \\\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.5.0\n\nThis quicker release cadence is a departure from our previous habit of releasing every five-to-seven months and was a deliberate effort to listen to you, our users, and get the changes and improvements into your workflows earlier.\n\n## Usability improvements to the Datasets UI\n\nWhen we released Dataset aware scheduling in September we knew that the tools we gave to manage the Datasets were very much a Minimum Viable Product, and in the last two months the committers and contributors have been hard at work at making the UI much more usable when it comes to Datasets.\n\nBut we we aren't done yet - keep an eye out for more improvements coming over the next couple of releases too.\n\n## Greatly improved `airflow dags test` command\n\nThis airflow subcommand has been rethought and re-optimized to make it much easier to test your DAGs locally - the major changes are:\n\na. Task logs are visible right there in the console, instead of hidden away inside the task log files\nb. It is about an order of magnitude quicker to run the tasks than before (i.e. it gets to running the task code so much quicker)\nc. Everything runs in one process, so you can put a breakpoint in your IDE, and configure it to run `airflow dags test <mydag>` then debug code!\n\n## Auto tailing task logs in the Grid view\n\nHopefully the headline says enough. It's lovely, go check it out.\n\n## More improvments to Dynamic-Task mapping\n\nIn a similar vein to the improvements to the Dataset (UI), we have continued to iterate on and improve the feature we first added in Airflow 2.3, Dynamic Task Mapping, and 2.5 includes [dozens of improvements](https://github.com/apache/airflow/pulls?q=is%3Apr+author%3Auranusjr+is%3Aclosed+milestone%3A%22Airflow+2.5.0%22).\n\n\n## Thanks to the contributors\n\nAndrey Anshin, Ash Berlin-Taylor, blag, Bolke de Bruin, Brent Bovenzi, Chenglong Yan, Daniel Standish, Dov Benyomin Sohacheski, Elad Kalif, Ephraim Anierobi, Jarek Potiuk, Jed Cunningham, Jorrick Sleijster, Michael Petro, Niko, Pierre Jeambrun, Tzu-ping Chung and many more, over 75 of you. Thank you!\n\nAnd a special thank you to Ephraim who tirelessly worked behind the scenes as release manager!\n\nA much shorter change log than 2.4, but I think you'll agree, some great changes.\n","url":"airflow-2.5","title":"Apache Airflow 2.5.0: Tick-Tock","linkTitle":"Apache Airflow 2.5.0","author":"Ash Berlin-Taylor","github":"ashberlin","twitter":"ashberlin","linkedin":"ashberlin-taylor","description":"We're proud to announce that Apache Airflow 2.5.0 has been released with many quality of life changes.","tags":["Release"],"date":"2022-12-02"},{"content":"I am excited to announce that Apache Airflow 2.6.0 has been released, bringing many minor features and improvements to the community.\n\nApache Airflow 2.6.0 contains over 500 commits, which include 42 new features, 58 improvements, 38 bug fixes, and 17 documentation changes.\n\n**Details**:\n\n📦 PyPI: https://pypi.org/project/apache-airflow/2.6.0/ \\\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.6.0/ \\\n🛠 Release Notes: https://airflow.apache.org/docs/apache-airflow/2.6.0/release_notes.html \\\n🐳 Docker Image: \"docker pull apache/airflow:2.6.0\" \\\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.6.0\n\nAs the changelog is quite large, the following are some notable new features that shipped in this release.\n\n## Trigger logs can now be viewed in webserver\n\nTrigger logs have now been added to task logs. They appear right alongside the rest of the logs from your task.\n\n![Trigger logs shown in task log](trigger_logging.png)\n\nAdding this feature required changes across the entire Airflow logging stack, so be sure to update your providers if you are using remote logging.\n\n## Grid view improvements\n\nThe grid view has received a number of minor improvements in this release.\n\nMost notably, there is now a graph tab in the grid view. This offers a more integrated graph representation of the DAG, where choosing a task in either the grid or graph will highlight the same task in both views.\n\n![The new graph view](graph.png)\n\nYou can also filter upstream and downstream from a single task. For example, in the screenshot above, `describe_integrity` is the selected task. If you choose to filter downstream, this is the result:\n\n![The new graph view can be filtered to show downstream tasks only](filter_downstream.png)\n\n## Trigger UI based on DAG level params\n\nA user-friendly form is now shown to users triggering runs for DAGs with DAG level params.\n\n![Form shown for params in UI when triggering a DAG](trigger_dag_form.png)\n\nSee the [Params docs](https://airflow.apache.org/docs/apache-airflow/2.6.0/core-concepts/params.html#use-params-to-provide-a-trigger-ui-form) for more details.\n\n## Consolidation of handling stuck queued tasks\n\nAirflow now has a single configuration, `[scheduler] task_queued_timeout`, to handle tasks that get stuck in queued for too long. With a simpler implementation than the outgoing code handling these tasks, tasks stuck in queued will no longer slip through the cracks and stay stuck.\n\nFor more details, see the [Unsticking Airflow: Stuck Queued Tasks are No More in 2.6.0](https://medium.com/apache-airflow/unsticking-airflow-stuck-queued-tasks-are-no-more-in-2-6-0-6f40a1a22835) Medium post.\n\n## Cluster Policy hooks can come from plugins\n\nCluster policy hooks (e.g. `dag_policy`), can now come from Airflow plugins in addition to Airflow local settings. By allowing multiple hooks to be defined, it makes it easier for more than one team to run hooks in a single Airflow instance.\n\nSee the [cluster policy docs](https://airflow.a","url":"airflow-2.6","title":"what's new in Apache Airflow 2.6.0","linkTitle":"what's new in Apache Airflow 2.2.0","author":"Jed Cunningham","github":"jedcunningham","linkedin":"jedidiah-cunningham","description":"Apache Airflow 2.6.0 has been released!","tags":["Release"],"date":"2023-04-30"},{"content":"I’m happy to announce that Apache Airflow 2.7.0 has been released! Some notable features have been added that we are excited for the community to use.\n\nApache Airflow 2.7.0 contains over 500 commits, which include 40 new features, 49 improvements, 53 bug fixes, and 15 documentation changes.\n\n**Details**:\n\n📦 PyPI: https://pypi.org/project/apache-airflow/2.7.0/ \\\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.7.0/ \\\n🛠 Release Notes: https://airflow.apache.org/docs/apache-airflow/2.7.0/release_notes.html \\\n🐳 Docker Image: \"docker pull apache/airflow:2.7.0\" \\\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.7.0\n\nAirflow 2.7.0 is a release that focuses on security. The Airflow security team, working together with security researchers, identified a number of areas that required strengthening of security. This resulted in, among others things, an improved description of the [Airflow security model](https://airflow.apache.org/docs/apache-airflow/stable/security/security_model/), a better explanation of our [security policy](https://github.com/apache/airflow/security/policy) and the disabling of certain, potentially dangerous, features by default - like, for example, connection testing (#32052).\n\nAirflow 2.7.0 is also the first release that drops support for end-of-life Python 3.7. This allows Airflow users and maintainers to make use of features and improvements in Python 3.8, and unlocks newer versions of our dependencies.\n\n## Setup and Teardown (AIP-52)\n\nAirflow now has first class support for the concept of setup and teardown tasks. These tasks have special behavior in that:\n\n* Teardown tasks will still run, no matter what state the upstream tasks end up in\n* Teardown tasks failing won’t, by default, cause the DAG run to fail\n* Automatically clear setup/teardown tasks when clearing a dependent task\n\nYou can read more about setup and teardown in the [Introducing Setup and Teardown tasks blog post]({{< ref \"blog/introducing_setup_teardown/index.md\" >}}), or in the [setup and teardown docs](https://airflow.apache.org/docs/apache-airflow/2.7.0/howto/setup-and-teardown.html).\n\n## Cluster Activity UI\n\nThere is a new top level page in Airflow, the Cluster Activity page. This gives an overview of the cluster, including component health, dag and task state counts, and more!\n\n![New cluster activity page](cluster_activity.png)\n\n## Graph and gantt views moved into the Grid view UI\n\nThe graph and gantt views have been rewritten and moved into the now familiar grid view. This makes it easier to jump between task details, logs, graph, and gantt views without losing your place in a complicated DAG.\n\n![Graph in grid view](graph_in_grid.png)\n\n## Enable deferrable mode for all deferable tasks with 1 config setting\n\nAirflow 2.7.0 comes with a new config option, `default_deferrable`, which allows admins to enable deferrable mode for all deferrable tasks without requiring any DAG modifications. Simply set it in your config and enjoy async ","url":"airflow-2.7","title":"Apache Airflow 2.7.0 is here","linkTitle":"Apache Airflow 2.7.0 is here","author":"Jed Cunningham","github":"jedcunningham","linkedin":"jedidiah-cunningham","description":"Apache Airflow 2.7.0 has been released!","tags":["Release"],"date":"2023-08-18"},{"content":"I am thrilled to announce the release of Apache Airflow 2.8.0, featuring a host of significant enhancements and new features that will greatly benefit our community.\n\n**Details**:\n\n📦 PyPI: https://pypi.org/project/apache-airflow/2.8.0/ \\\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.8.0/ \\\n🛠 Release Notes: https://airflow.apache.org/docs/apache-airflow/2.8.0/release_notes.html \\\n🐳 Docker Image: \"docker pull apache/airflow:2.8.0\" \\\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.8.0\n\n## Airflow Object Storage (AIP-58)\n\n*This feature is experimental and subject to change.*\n\nAirflow now offers a generic abstraction layer over various object stores like S3, GCS, and Azure Blob Storage, enabling the use of different storage systems in DAGs without code modification.\n\nIn addition, it allows you to use most of the standard Python modules, like shutil, that can work with file-like objects.\n\nHere is an example of how to use the new feature to open a file:\n\n```python\nfrom airflow.io.path import ObjectStoragePath\n\nbase = ObjectStoragePath(\"s3://my-bucket/\", conn_id=\"aws_default\")  # conn_id is optional\n\n@task\ndef read_file(path: ObjectStoragePath) -> str:\n    with path.open() as f:\n        return f.read()\n```\n\nThe above example is just the tip of the iceberg. The new feature allows you to configure an alternative backend for a scheme or protocol.\n\nHere is an example of how to configure a custom backend for the `dbfs` scheme:\n\n```python\nfrom airflow.io.path import ObjectStoragePath\nfrom airflow.io.store import attach\n\nfrom fsspec.implementations.dbfs import DBFSFileSystem\n\nattach(protocol=\"dbfs\", fs=DBFSFileSystem(instance=\"myinstance\", token=\"mytoken\"))\nbase = ObjectStoragePath(\"dbfs://my-location/\")\n```\n\nFor more information: [Airflow Object Storage](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/objectstorage.html)\n\nThe support for a specific object storage system depends on the installed providers,\nwith out-of-the-box support for the file scheme.\n\n## Ship logs from other components to Task logs\n\nThis feature seamlessly integrates task-related messages from various Airflow components, including the Scheduler and\nExecutors, into the task logs. This integration allows users to easily track error messages and other relevant\ninformation within a single log view.\n\nPresently, suppose a task is terminated by the scheduler before initiation, times out due to prolonged queuing, or transitions into a zombie state. In that case, it is not recorded in the task log. With this enhancement, in such situations,\nit becomes feasible to dispatch an error message to the task log for convenient visibility on the UI.\n\nThis feature can be toggled, for more information [see “enable_task_context_logger” in the logging configuration documentation](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#enable-task-context-logger).\n\n## Listener hooks for Datasets\n\n*Please note that listeners are still ex","url":"airflow-2.8","title":"Apache Airflow 2.8.0 is here","linkTitle":"Apache Airflow 2.8.0 is here","author":"Ephraim Anierobi","github":"ephraimbuddy","linkedin":"ephraimanierobi","description":"Introducing Apache Airflow 2.8.0: Enhanced with New Features and Significant Improvements","tags":["Release"],"date":"2023-12-15"},{"content":"I’m happy to announce that Apache Airflow 2.9.0 has been released! This time around we have new features for data-aware scheduling and a bunch of UI-related improvements.\n\nApache Airflow 2.9.0 contains over 550 commits, which include 38 new features, 70 improvements, 31 bug fixes, and 18 documentation changes.\n\n**Details**:\n\n📦 PyPI: https://pypi.org/project/apache-airflow/2.9.0/ \\\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.9.0/ \\\n🛠 Release Notes: https://airflow.apache.org/docs/apache-airflow/2.9.0/release_notes.html \\\n🐳 Docker Image: \"docker pull apache/airflow:2.9.0\" \\\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.9.0\n\nAirflow 2.9.0 is also the first release that supports Python 3.12. However, Pendulum 2 does not support Python 3.12, so you’ll need to use [Pendulum 3](https://pendulum.eustace.io/blog/announcing-pendulum-3-0-0.html) if you upgrade to Python 3.12.\n\n## New data-aware scheduling options\n\n### Logical operators and conditional expressions for DAG scheduling\n\nWhen Datasets were added in Airflow 2.4, DAGs only had scheduling support for logical AND combinations of Datasets. Simply, you could schedule against more than one Dataset, but a DAG run would only be created once all the Datasets were updated after the last run. Now in Airflow 2.9, we support logical OR and even arbitrary combinations of AND and OR.\n\nAs an example, you can schedule a DAG whenever `dataset_1` or `dataset_2` are updated :\n\n```python\nwith DAG(schedule=(dataset_1 | dataset_2), ...):\n    ...\n```\n\nYou can have arbitrary combinations:\n\n```python\nwith DAG(schedule=((dataset_1 | dataset_2) & dataset_3), ...):\n    ...\n```\n\nYou can read more about this new functionality in the [data-aware scheduling docs](https://airflow.apache.org/docs/apache-airflow/2.9.0/authoring-and-scheduling/datasets.html#advanced-dataset-scheduling-with-conditional-expressions).\n\n### Combining Dataset and Time-Based Schedules\n\nAirflow 2.9 comes with a new timetable, `DatasetOrTimeSchedule`, that allows you to schedule DAGs based on both dataset events and a timetable. Now you have the best of both worlds.\n\nFor example, to run whenever `dataset_1` updates and at midnight UTC:\n\n```python\nwith DAG(\n    schedule=DatasetOrTimeSchedule(\n        timetable=CronTriggerTimetable(\"0 0 * * *\", timezone=\"UTC\"),\n        datasets=[dag1_dataset],\n    ),\n    ...\n):\n    ...\n\n```\n\n### Dataset Event REST API endpoints\n\nNew REST API endpoints have been introduced for creating, listing, and deleting dataset events. This makes it possible for external systems to notify Airflow about dataset updates and unlocks management of event queues for more sophisticated use cases.\n\nSee the [Dataset API docs](https://airflow.apache.org/docs/apache-airflow/2.9.0/stable-rest-api-ref.html#tag/Dataset) for more details.\n\n\n### Dataset UI Enhancements\n\nThe DAG's graph view has been enhanced to display both the datasets it is scheduled on and those in the task outlets, providing a comprehensive ove","url":"airflow-2.9","title":"Apache Airflow 2.9.0: Dataset and UI Improvements","linkTitle":"Apache Airflow 2.9.0: Dataset and UI Improvements","author":"Jed Cunningham","github":"jedcunningham","linkedin":"jedidiah-cunningham","description":"Apache Airflow 2.9.0 is here! Lots of exciting new Dataset and UI features/improvements this time around.","tags":["Release"],"date":"2024-04-08"},{"content":"# Apache Airflow Survey 2019\n\nApache Airflow is [growing faster than ever](https://www.astronomer.io/blog/why-airflow/).\nThus, receiving and adjusting to our users’ feedback is a must. We created\n[survey](https://forms.gle/XAzR1pQBZiftvPQM7) and we got **308** responses.\nLet’s see who Airflow users are, how they play with it, and what they miss.\n\n# Overview of the user\n\n**What best describes your current occupation?**\n\n|                         |No.|  %   |\n|-------------------------|---|------|\n|Data Engineer            |194|62.99%|\n|Developer                | 34|11.04%|\n|Architect                | 23|7.47% |\n|Data Scientist           | 19|6.17% |\n|Data Analyst             | 13|4.22% |\n|DevOps                   | 13|4.22% |\n|IT Administrator         |  2|0.65% |\n|Machine Learning Engineer|  2|0.65% |\n|Manager                  |  2|0.65% |\n|Operations               |  2|0.65% |\n|Chief Data Officer       |  1|0.32% |\n|Engineering Manager      |  1|0.32% |\n|Intern                   |  1|0.32% |\n|Product owner            |  1|0.32% |\n|Quant                    |  1|0.32% |\n\n\n**In your day to day job, what do you use Airflow for?**\n\n|                                                      |No.|  %   |\n|------------------------------------------------------|---|------|\n|Data processing (ETL)                                 |298|96.75%|\n|Artificial Intelligence and Machine Learning Pipelines| 90|29.22%|\n|Automating DevOps operations                          | 64|20.78%|\n\nAccording to the survey, most of the Airflow users are the “data” people. Moreover,\n28.57% uses Airflow to both ETL and ML pipelines meaning that those two fields\nare somehow connected. Only five respondents use Airflow for DevOps operations only,\nThat means that other 59 people who use Airflow for DevOps stuff use it also for\nETL / ML  purposes.\n\n**How many active DAGs do you have in your largest Airflow instance?**\n\n|       |No.|  %   |\n|-------|---|------|\n|0-20   |115|37.34%|\n|21-40  | 65|21.10%|\n|41-60  | 44|14.29%|\n|61-100 | 28|9.09% |\n|101-200| 28|9.09% |\n|201-300|  7|2.27% |\n|301-999|  8|2.60% |\n|1000+  | 13|4.22% |\n\n\nThe majority of users do not exceed 100 active DAGs per Airflow instance. However,\nas we can see there are users who exceed thousands of DAGs with a maximum number 5000.\n\n**What is the maximum number of tasks that you have used in one DAG?**\n\n|       |No.|  %   |\n|-------|---|------|\n|0-10   | 61|19.81%|\n|11-20  | 60|19.48%|\n|21-30  | 31|10.06%|\n|31-40  | 21|6.82% |\n|41-50  | 26|8.44% |\n|51-100 | 36|11.69%|\n|101-200| 28|9.09% |\n|201-500| 21|6.82% |\n|501+   | 24|11.54%|\n\n\nThe given maximum number of tasks in a single DAG was 10 000 (!). The number of tasks\ndepends on the purposes of a DAG, so it’s rather hard to say if users have “simple”\nor “complicated” workflows.\n\n**When onboarding new members to Airflow, what is the biggest problem?**\n\n|                                                               |No.|  %   |\n|----------------------------------------------------","url":"airflow-survey","title":"Airflow Survey 2019","linkTitle":"Airflow Survey 2019","author":"Tomek Urbaszek","twitter":"Nuclearriot","github":"nuclearpinguin","linkedin":"tomaszurbaszek","description":"Receiving and adjusting to our users’ feedback is a must. Let’s see who Airflow users are, how they play with it, and what they miss.","tags":["community","survey","users"],"date":"2019-12-11"},{"content":"# Apache Airflow Survey 2020\n\nWorld of data processing tools is growing steadily. Apache Airflow seems to be already considered as\ncrucial component of this complex ecosystem. We observe steady growth in number of users as well as in\nan amount of active contributors. So listening and understanding our community is of high importance.\n\nIt's worth to note that the 2020 survey was still mostly about 1.10.X version of Apache Airflow and\npossibly many drawbacks were addressed in the 2.0 version that was released in December 2020. But if this\nis true, we will learn next year!\n\n## Overview of the user\n\n![](What_best_describes_your_current_occupation.png)\n\n**What best describes your current occupation? (single choice)**\n\n|                     | No. | %     |\n| ------------------- | --- | ----- |\n| Data Engineer       | 115 | 56.65 |\n| Developer           | 28  | 13.79 |\n| DevOps              | 17  | 8.37  |\n| Solutions Architect | 14  | 6.9   |\n| Data Scientist      | 12  | 5.91  |\n| Other               | 10  | 4.93  |\n| Data Analyst        | 4   | 1.97  |\n| Support Engineer    | 3   | 1.48  |\n\nThose results are not a surprise as Airflow is a tool dedicated to data-related tasks. The majority of\nour users are data engineers, scientists or analysts. The 2020 results are similar to [those from 2019](https://airflow.apache.org/blog/airflow-survey/) with\nvisible slight increase in ML use cases.\n\nAdditionally, 79% of users uses Airflow on daily basis and 16% interacts with it at least once a week.\n\n**How many people work in your company? (single choice)**\n\n|        | No. | %     |\n| ------ | --- | ----- |\n| 200+   | 107 | 52.71 |\n| 51-200 | 44  | 21.67 |\n| 11-50  | 37  | 18.23 |\n| 1-10   | 15  | 7.39  |\n\n**How many people in your company use Airflow? (single choice)**\n\n|       | No. | %     |\n| ----- | --- | ----- |\n| 1-5   | 84  | 41.38 |\n| 6-20  | 75  | 36.95 |\n| 21-50 | 23  | 11.33 |\n| 50+   | 21  | 10.34 |\n\nAirflow is a software that is used and trusted by big companies. We can also see that Airflow can work\nfine for teams of different sizes. However, in some cases users may use multiple Airflow instances.\n\n**Are you considering moving to other workflow engines? (single choice)**\n\n|                               | No. | %     |\n| ----------------------------- | --- | ----- |\n| No, we are happy with Airflow | 174 | 85.71 |\n| Yes                           | 29  | 14.29 |\n\nNearly 1 out of 7 users is considering migrating to other workflow engines. Their decision is usually\njustified by need of **easier workflow writing experience** (12.32%), **better UI/UX** and **faster scheduler**\n(8.37% both).\n\nWhile the first point may be addressed by [TaskFlow API](http://airflow.apache.org/docs/apache-airflow/stable/concepts.html#taskflow-api) in Airflow 2.0 the other two are definitely addressed\nin the new major version. And the early feedback from 2.0 users seems to be confirming it.\n\nThe alternative engines considered by users are mainly Prefect and Argo. Some parti","url":"airflow-survey-2020","title":"Airflow Survey 2020","linkTitle":"Airflow Survey 2020","author":"Tomek Urbaszek","twitter":"turbaszek","github":"turbaszek","linkedin":"tomaszurbaszek","description":"We observe steady growth in number of users as well as in an amount of active contributors. So listening and understanding our community is of high importance.","tags":["community","survey","users"],"date":"2021-03-09"},{"content":"# Airflow User Survey 2022\n\nThis year’s survey has come and gone, and with it we’ve got a new batch of data for everyone! We collected 210 responses over two weeks. We continue to see growth in both contributions and downloads over the last two years, and expect that trend will continue through 2022.\n\nThe raw response data will be made available here soon, in the meantime, feel free to email john.thomas@astronomer.io for a copy.\n\n## TL;DR\n\n### Overview of the user\n\n- Like previous years, more than half of the Airflow users are Data Engineers (54%). Solutions Architects (13%), Developers (12%), DevOps (6%) and Data Scientists (4%) are also active Airflow users! There was a slight increase in the representation of Solutions Architect roles compared to results from [2020](https://airflow.apache.org/blog/airflow-survey-2020/#overview-of-the-user) and [2019](https://airflow.apache.org/blog/airflow-survey/) .\n- Airflow is used and popular in bigger companies, 64% of Airflow users work for companies with 200+ employees which is an 11 percent increase compared to [2020](https://airflow.apache.org/blog/airflow-survey-2020/#overview-of-the-user).\n- 62% of the survey participants have more than 6 Airflow users in their company.\n- More Airflow users (65.9%) are willing to recommend Apache Airflow compared to the survey results in [2020](https://airflow.apache.org/blog/airflow-survey-2020/#overview-of-the-user) and [2019](https://airflow.apache.org/blog/airflow-survey/). There is a general positive trend in a willingness to recommend Airflow, 93% of surveyed Airflow users are willing to recommend Airflow ( 85.7% in [2019](https://airflow.apache.org/blog/airflow-survey/) and 92% in [2020](https://airflow.apache.org/blog/airflow-survey-2020/#overview-of-the-user) ), only 1% of users are not likely to recommend (3.6% in [2019](https://airflow.apache.org/blog/airflow-survey/) and 3.5% in [2020](https://airflow.apache.org/blog/airflow-survey-2020/#overview-of-the-user)).\n- Airflow documentation is a critical source of information, with more than 90% (15% increase compared to results from [2020](https://airflow.apache.org/blog/airflow-survey-2020/#overview-of-the-user)) of survey participants using the documentation. Airflow documentation is also one of the top areas to improve! What’s interesting, also Stack Overflow usage is critical, with about 60% users declaring to use it as a source of information (24% increase compared to results from [2020](https://airflow.apache.org/blog/airflow-survey-2020/#overview-of-the-user)).\n\n### Deployments\n\n- 85% of the Airflow users have between 1 to 7 active Airflow instances. 62.5% of the Airflow users have between 11 to 250 DAGs in their largest Airflow instance. 75% of the surveyed Airflow users have between 1 to 100 tasks per DAG.\n- Close to 85% of users use one of the Airflow 2 versions, 9.2% users still use 1.10.15, while the remaining 6.3% are still using olderAirflow 1 versions. The good news is that the majority of user","url":"airflow-survey-2022","title":"Airflow Survey 2022","linkTitle":"Airflow Survey 2022","author":"John Thomas, Ewa Tatarczak","github":"Tohn Jhomas","linkedin":"john-e-thomas","description":"2021 saw rapid adoption of Airflow 2, and continued growth of the community. This annual survey helps us understand how people are using Airflow, and where we can best focus our efforts going forward.","tags":["community","survey","users"],"date":"2022-06-17"},{"content":"I am proud to announce that Apache Airflow 2.0.0 has been released.\n\nThe full changelog is about 3,000 lines long (already excluding everything backported to 1.10), so for now I'll simply share some of the major features in 2.0.0 compared to 1.10.14:\n\n## A new way of writing dags: the TaskFlow API (AIP-31)\n\n(Known in 2.0.0alphas as Functional DAGs.)\n\nDAGs are now much much nicer to author especially when using PythonOperator. Dependencies are handled more clearly and XCom is nicer to use\n\nRead more here:\n\n[TaskFlow API Tutorial](http://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html) \\\n[TaskFlow API Documentation](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#decorated-flows)\n\nA quick teaser of what DAGs can now look like:\n\n```python\nfrom airflow.decorators import dag, task\nfrom airflow.utils.dates import days_ago\n\n@dag(default_args={'owner': 'airflow'}, schedule_interval=None, start_date=days_ago(2))\ndef tutorial_taskflow_api_etl():\n   @task\n   def extract():\n       return {\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}\n\n   @task\n   def transform(order_data_dict: dict) -> dict:\n       total_order_value = 0\n\n       for value in order_data_dict.values():\n           total_order_value += value\n\n       return {\"total_order_value\": total_order_value}\n\n   @task()\n   def load(total_order_value: float):\n\n       print(\"Total order value is: %.2f\" % total_order_value)\n\n   order_data = extract()\n   order_summary = transform(order_data)\n   load(order_summary[\"total_order_value\"])\n\ntutorial_etl_dag = tutorial_taskflow_api_etl()\n```\n\n## Fully specified REST API (AIP-32)\n\nWe now have a fully supported, no-longer-experimental API with a comprehensive OpenAPI specification\n\nRead more here:\n\n[REST API Documentation](http://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html).\n\n## Massive Scheduler performance improvements\n\nAs part of AIP-15 (Scheduler HA+performance) and other work Kamil did, we significantly improved the performance of the Airflow Scheduler. It now starts tasks much, MUCH quicker.\n\nOver at Astronomer.io we've [benchmarked the scheduler—it's fast](https://www.astronomer.io/blog/airflow-2-scheduler) (we had to triple check the numbers as we don't quite believe them at first!)\n\n## Scheduler is now HA compatible (AIP-15)\n\nIt's now possible and supported to run more than a single scheduler instance. This is super useful for both resiliency (in case a scheduler goes down) and scheduling performance.\n\nTo fully use this feature you need Postgres 9.6+ or MySQL 8+ (MySQL 5, and MariaDB won't work with more than one scheduler I'm afraid).\n\nThere's no config or other set up required to run more than one scheduler—just start up a scheduler somewhere else (ensuring it has access to the DAG files) and it will cooperate with your existing schedulers through the database.\n\nFor more information, read the [Scheduler HA documentation](http://airflow.apache.org/docs/apache-airflow/stable/scheduler.htm","url":"airflow-two-point-oh-is-here","title":"Apache Airflow 2.0 is here!","linkTitle":"Apache Airflow 2.0 is here!","author":"Ash Berlin-Taylor","github":"ashb","linkedin":"ashberlin","description":"We're proud to announce that Apache Airflow 2.0.0 has been released.","tags":["Release"],"date":"2020-12-17"},{"content":"## Airflow Summit 2021 is here!\n\nThe summit will be held online, July 8-16, 2021. Join us from all over the world to find\nout how Airflow is being used by leading companies, what is its roadmap and how you can\nparticipate in its development.\n\n## Useful information:\n- The official website: https://airflowsummit.org\n- Call for proposals is open until **12 April 2021**. To submit your talk go to https://sessionize.com/airflow-summit-2021/\n- In case of any questions reach out to us via info@airflowsummit.org\n","url":"airflow_summit_2021","title":"Airflow Summit 2021","linkTitle":"Airflow Summit 2021","author":"Tomasz Urbaszek","description":"We are thrilled about Airflow Summit 2021!","tags":["Community","Airflow Summit"],"date":"2021-03-21"},{"content":"The biggest Airflow Event of the Year returns May 23–27! Airflow Summit 2022 will bring together the global\ncommunity of Apache Airflow practitioners and data leaders.\n\n### What’s on the Agenda\nDuring the free conference, you will hear about Apache Airflow best practices, trends in building data\npipelines, data governance, Airflow and machine learning, and the future of Airflow. There will also be\na series of presentations on non-code contributions driving the open-source project.\n\n### How to Attend\nThis year’s edition will include a variety of online sessions across different time zones.\nAdditionally, you can take part in local in-person events organized worldwide for data\ncommunities to watch the event and network.\n\n### Interested?\n\n 🪶 [Register for Airflow Summit 2022](https://www.crowdcast.io/e/airflowsummit2022/register?utm_campaign=Astronomer_marketing&utm_source=Astronomer%20website&utm_medium=website&utm_term=Airflow%20Summit) today\n\n 🤝 [Check out the in-person events](https://airflowsummit.org/in-person-events/) planned for Airflow Summit 2022.\n","url":"airflow_summit_2022","title":"Airflow Summit 2022","linkTitle":"Airflow Summit 2022","author":"Jarek Potiuk","description":"Airflow Summit 2022 is here","tags":["Community","Airflow Summit"],"date":"2022-05-16"},{"content":"The brand [new Airflow website](https://airflow.apache.org/) has arrived! Those who have been following the process know that the journey to update [the old Airflow website](https://airflow.readthedocs.io/en/1.10.6/) started at the beginning of the year.\nThanks to sponsorship from the Cloud Composer team at Google that allowed us to\ncollaborate with [Polidea](https://www.polidea.com/) and with their design studio [Utilo](https://utilodesign.com/), and deliver an awesome website.\n\nDocumentation of open source projects is key to engaging new contributors in the maintenance,\ndevelopment, and adoption of software. We want the Apache Airflow community to have\nthe best possible experience to contribute and use the project. We also took this opportunity to make the project\nmore accessible, and in doing so, increase its reach.\n\nIn the past three and a half months, we have updated everything: created a more efficient landing page,\nenhanced information architecture, and improved UX & UI. Most importantly, the website now has capabilities\nto be translated into many languages. This is our effort to foster a more inclusive community around\nApache Airflow, and we look forward to seeing contributions in Spanish, Chinese, Russian, and other languages as well!\n\nWe built our website on Docsy, a platform that is easy to use and contribute to. Follow\n[these steps](https://github.com/apache/airflow-site/blob/master/README.md) to set up your environment and\nto create your first pull request. You may also use\nthe new website for your own open source project as a template.\nAll of our [code is open and hosted on GitHub](https://github.com/apache/airflow-site/tree/master).\n\nShare your questions, comments, and suggestions with us, to help us improve the website.\nWe hope that this new design makes finding documentation about Airflow easier,\nand that its improved accessibility increases adoption and use of Apache Airflow around the world.\n\nHappy browsing!\n","url":"announcing-new-website","title":"New Airflow website","linkTitle":"New Airflow website","author":"Aizhamal Nurmamat kyzy","description":"We are thrilled about our new website!","tags":["Community"],"date":"2019-12-11"},{"content":"Apache Airflow is a platform to programmatically author, schedule, and monitor workflows.\nA workflow is a sequence of tasks that processes a set of data. You can think of workflow as the\npath that describes how tasks go from being undone to done. Scheduling, on the other hand, is the\nprocess of planning, controlling, and optimizing when a particular task should be done.\n\n### Authoring Workflow in Apache Airflow.\nAirflow makes it easy to author workflows using python scripts. A [Directed Acyclic Graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)\n(DAG) represents a workflow in Airflow. It is a collection of tasks in a way that shows each task's\nrelationships and dependencies. You can have as many DAGs as you want, and Airflow will execute\nthem according to the task's relationships and dependencies. If task B depends on the successful\nexecution of another task A, it means Airflow will run task A and only run task B after task A.\nThis dependency is very easy to express in Airflow. For example, the above scenario is expressed as\n```python\ntask_A >> task_B\n```\nAlso equivalent to\n```python\ntask_A.set_downstream(task_B)\n```\n![Simple Dag](Simple_dag.png)\n\nThat helps Airflow to know that it needs to execute task A before task B. Tasks can have far more complex\nrelationships to each other than expressed above and Airflow figures out how and when to execute the tasks following\ntheir relationships and dependencies.\n![Complex Dag](semicomplex.png)\n\nBefore we discuss the architecture of Airflow that makes scheduling, executing, and monitoring of\nworkflow an easy thing, let us discuss the [Breeze environment](https://github.com/apache/airflow/blob/master/BREEZE.rst).\n\n### Breeze Environment\nThe breeze environment is the development environment for Airflow where you can run tests, build images,\nbuild documentations and so many other things. There are excellent\n[documentation and video](https://github.com/apache/airflow/blob/master/BREEZE.rst) on Breeze environment.\nPlease check them out. You enter the Breeze environment by running the ``./breeze`` script. You can run all\nthe commands mentioned here in the Breeze environment.\n\n### Scheduler\nThe scheduler is the component that monitors DAGs and triggers those tasks whose dependencies have\nbeen met. It watches over the DAG folder, checking the tasks in each DAG and triggers them once they\nare ready. It accomplishes this by spawning a process that runs periodically(every minute or so)\nreading the metadata database to check the status of each task and decides what needs to be done.\nThe metadata database is where the status of all tasks are recorded. The status can be one of running,\n success, failed, etc.\n\nA task is said to be ready when its dependencies have been met. The dependencies include all the data\nnecessary for the task to be executed. It should be noted that the scheduler won't trigger your tasks until\nthe period it covers has ended. If a task's ``schedule_interval`` is ``@daily``, the scheduler tr","url":"apache-airflow-for-newcomers","title":"Apache Airflow For Newcomers","linkTitle":"Apache Airflow For Newcomers","author":"Ephraim Anierobi","twitter":"ephraimbuddy","github":"ephraimbuddy","description":"","tags":["Community"],"date":"2020-08-17","draft":false},{"content":"Is it possible to create an organization that delivers tens of projects used by millions, nearly no one is paid for doing their job, and still, it has been fruitfully carrying on for more than 20 years? Apache Software Foundation proves it is possible. For the last two decades, ASF has been crafting a model called the Apache Way—a way of organizing and leading tech open source projects. Due to this approach, which is strongly based on the “community over code” motto, we can enjoy such awesome projects like Apache Spark, Flink, Beam, or Airflow (and many more).\n\nAfter this year’s ApacheCon, Polidea’s engineers talked with Committers of Apache projects, such as—Aizhamal Nurmamat kyzy, Felix Uellendall, and Fokko Driesprong—about insights to what makes the ASF such an amazing organization.\n\nYou can read the [insights after the ApacheCon 2019](https://higrys.medium.com/apachecon-europe-2019-thoughts-and-insights-by-airflow-committers-9ff5f6938c99).\n","url":"apache-con-europe-2019-thoughts-and-insights-by-airflow-committers","title":"ApacheCon Europe 2019 — Thoughts and Insights by Airflow Committers","linkTitle":"ApacheCon Europe 2019 — Thoughts and Insights by Airflow Committers","author":"Polidea","description":"Here come some thoughts by Airflow committers and contributors from the ApacheCon Europe 2019. Get to know the ASF community!","tags":["Community"],"date":"2019-11-22"},{"content":"## Documenting local development environment of Apache Airflow\n\nFrom Sept to November, 2019 I have been participating in a wonderful initiative, [Google Season of Docs](https://developers.google.com/season-of-docs).\n\nI had a pleasure to contribute to the Apache Airflow open source project as a technical writer.\nMy initial assignment was an extension to the GitHub-based Contribution guide.\n\nFrom the very first days I have been pretty closely involved into inter-project communications\nvia emails/slack and had regular 1:1s with my mentor, Jarek Potiuk.\n\nI got infected with Jarek’s enthusiasm to ease the on-boarding experience for\nAirflow contributors. I do share this strategy and did my best to improve the structure,\nlanguage and DX. As a result, Jarek and I extended the current contributor’s docs and\nended up with the Contributing guide navigating the users through the project\ninfrastructure and providing a workflow example based on a real-life use case;\nthe Testing guide with an overview of a complex testing infrastructure for Apache Airflow;\nand two guides dedicated to the Breeze dev environment and local virtual environment\n(my initial assignment).\n\nI’m deeply grateful to my mentor and Airflow developers for their feedback,\npatience and help while I was breaking through new challenges\n(I’ve never worked on an open source project before),\nand for their support of all my ideas! I think a key success factor for any contributor\nis a responsive, supportive and motivated team, and I was lucky to join such\na team for 3 months.\n\nDocuments I worked on:\n\n* [Breeze development environment documentation](https://github.com/apache/airflow/blob/master/BREEZE.rst)\n* [Local virtualenv environment documentation](https://github.com/apache/airflow/blob/master/LOCAL_VIRTUALENV.rst)\n* [Contributing guide](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst)\n* [Testing guide](https://github.com/apache/airflow/blob/master/TESTING.rst)\n","url":"documenting-using-local-development-environments","title":"Documenting using local development environment","linkTitle":"Documenting using local development environment","author":"Elena Fedotova","github":"efedotova","linkedin":"elena-fedotova-039294110","description":"The story behind documenting local development environment of Apache Airflow","tags":["Development"],"date":"2019-11-22T00:00:00.000Z"},{"content":"I came across [Google Season of Docs][1] (GSoD) almost by accident, thanks to my extensive HackerNews and Twitter addiction.  I was familiar with the Google Summer of Code but not with this program.\nIt turns out it was the inaugural phase. I read the details, and the process felt a lot like GSoC except that this was about documentation.\n\n## About Me\nI have been writing tech articles on medium as well as my blog for the past 1.5 years.  Blogging helps me test my understanding of the concepts as untangling the toughest of ideas in simple sentences requires a considerable time investment.\n\nAlso, I have been working as a Software Developer for the past three years, which involves writing documentation for my projects as well. I completed my B.Tech from  IIT Roorkee. During my stay in college, I applied for GSoC once but didn’t make it through in the final list of selected candidates.\n\nI saw GSoD as an excellent opportunity to improve my technical writing skills using feedback from the open-source community. I contributed some bug fixes and features to Apache Superset and Apache Druid, but this would be my first contribution as a technical writer.\n\n## Searching for the organization\nAbout 40+ organizations were participating in the GSoD. However, there were two which came as the right choice for me in the first instant. The first one was [Apache Airflow][2] because I had already used Airflow extensively and also contributed some custom operators inside the forked version of my previous company.\n\nThe second one was [Apache Cassandra][3], on which I also had worked extensively but hadn’t done any code or doc changes.\n\nConsidering the total experience, I decided to go with the Airflow.\n\n## Project selection\nAfter selecting the org, the next step was to choose the project. Again, my previous experience played a role here, and I ended up picking the **How to create a workflow** . The aim of the project was to write documentation which will help users in creating complex as well as custom DAGs.  \nThe final deliverables were a bit different, though. More on that later.\n\nAfter submitting my application, I got involved in my job until one day, I saw a mail from google confirming my selection as a Technical Writer for the project.\n\n## Community Bonding\nGetting selected is just a beginning.  I got the invite to the Airflow slack channel where most of the discussions happened.\nMy mentor was [Ash-Berlin Taylor][4] from Apache Airflow. I started talking to my mentor to get a general sense of what deliverables were expected. The deliverables were documented in [confluence][5].\n\n- A page for how to create a DAG that also includes:\n    - Revamping the page related to scheduling a DAG\n    - Adding tips for specific DAG conditions, such as rerunning a failed task\n- A page for developing custom operators that includes:\n    - Describing mechanisms that are important when creating an operator, such as template fields, UI color, hooks, connection, etc.\n    - Describing the r","url":"experience-in-google-season-of-docs-2019-with-apache-airflow","title":"Experience in Google Season of Docs 2019 with Apache Airflow","linkTitle":"Experience in Google Season of Docs 2019 with Apache Airflow","author":"Kartik Khare","twitter":"khare_khote","github":"KKcorps","linkedin":"kharekartik","description":"","tags":["Documentation"],"date":"2019-12-20T00:00:00.000Z"},{"content":"[Outreachy](https://www.outreachy.org/) is a program which organises three months paid internships with FOSS\nprojects for people who are typically underrepresented in those projects.\n\n### Contribution Period\nThe first thing I had to do was choose a project under an organisation. After going through all the projects\nI chose “Extending the REST API of Apache Airflow”, because I had a good idea of what  REST API(s) are, so I\nthought it would be easier to get started with the contributions. The next step was to set up Airflow’s dev\nenvironment which thanks to [Breeze](https://github.com/apache/airflow/blob/master/BREEZE.rst), was a breeze.\nSince I had never contributed to FOSS before so this part was overwhelming but there were plenty of issues\nlabelled “good first issues” with detailed descriptions and some even had code snippets so luckily that nudged\nme in the right direction. These things about Airflow and the positive vibes from the community were the reasons\nwhy I chose to stick with Airflow as my Outreachy project.\n\n### Internship Period\nMy first PR was followed by many new experiences one of them being that I introduced a\n[bug](https://github.com/apache/airflow/pull/7680#issuecomment-619763051) in it;).\nBut nonetheless it made me familiar with the feedback loop and the feedback on my subsequent\n[PRs](https://github.com/apache/airflow/pulls?q=is%3Apr+author%3AOmairK+) was the focal point of the overall\nlearning experience I went through, which boosted my confidence to contribute more and move out of my comfort zone.\nI wanted to learn more about the things that happen under the Airflow’s hood so I started filtering out recent PRs\ndealing with different components and I would go through the code changes along with discussion that would help me\nget a better understanding of the whole workflow. [Airflow’s mailing list](https://lists.apache.org/list.html?dev@airflow.apache.org)\nwas also a great source of knowledge.\n\nThe API related PRs that I worked on helped me with some of the important concepts like:\n\n  1) [Pool CRUD endpoints](https://github.com/apache/airflow/pull/9329) where pools limit the execution parallelism.\n\n  2) [Tasks](https://github.com/apache/airflow/pull/9597) determine the actual work that has to be carried out.\n\n  3) [DAG](https://github.com/apache/airflow/pull/9473) which represents the structure for a collection\n  of tasks. It keeps track of tasks, their dependencies and the sequence in which they have to run.\n\n  4) [Dag Runs](https://github.com/apache/airflow/pull/9473) that are the instantiation of DAG(s) in time.\n\nThrough actively and passively participating in discussions I learnt that even if there is a difference of opinion\none could always learn from the different approaches, and [this PR](https://github.com/apache/airflow/pull/8721) with\nmore than 300+ comments is the proof of it. I also started reviewing small PRs which gave me the amazing opportunity\nto interact with new people. Throughout my internship I learnt a lot","url":"experience-with-airflow-as-an-outreachy-intern","title":"Journey with Airflow as an Outreachy Intern","linkTitle":"Journey with Airflow as an Outreachy Intern","author":"Omair Khan","github":"OmairK","linkedin":"omairkhan64","description":"","tags":["Community"],"date":"2020-08-30"},{"content":"# Vulnerability in long deprecated OpenID authentication method in Flask AppBuilder\n\nRecently [Islam Rzayev](https://www.linkedin.com/in/islam-rzayev) made us aware of a vulnerability in the\nlong deprecated OpenID authentication method in Flask AppBuilder. This vulnerability allowed a malicious user\nto take over the identity of any Airflow UI user by forging a specially crafted request and implementing\ntheir own OpenID service. While this is an old, deprecated and almost not used authentication method, we still\ntook the issue seriously.\n\nThis issue ONLY affects users who have ``AUTH_OID`` set in their ``webserver_config.py`` file as\n``AUTH_TYPE``. This is a very old and deprecated authentication method that is unlikely to be used by anyone.\n\nWe would like to advise even the small number of our users that still use this\nauthentication method to take an immediate action and either upgrade to Apache Airflow 2.8.2 or switch to\nanother authentication method (or apply a workaround we provide if they cannot do either of the above\nimmediately).\n\nImportant to stress, because many of the users might get confused by the name, OpenID is NOT the same as\nOpenID Connect. Those are completely different protocols and while OpenID Connect (also known as OIDC) is\na modern, widely used  protocol, OpenID is a legacy protocol that has been deprecated more than 10 years\nago and since then has been abandoned by almost everyone in the community, including all services in\nFlask AppBuilder example services that supported it, so it is highly unlikely someone is still using it.\n\nDue to this highly unlikely configuration the [Flask AppBuilder CVE](https://www.cve.org/CVERecord?id=CVE-2024-25128)\nis just \"Moderate\" not \"Critical\". It affects a very small (if any) number of users and it's not likely\nto be a target for an attack. However, we still advise our users who still use AUTH_OID to apply remediation.\n\nThis vulnerability is fixed in Flask Appbuilder 4.3.11 and Apache Airflow 2.8.2 uses that version of Flask\nApplication Builder. We advise users who still use this authentication method to either switch to another\nauthentication method or upgrade to Apache Airflow 2.8.2. If they cannot do either\nof these solutions quickly, they should apply the workaround provided below.\n\n## Impact\n\nWhen Flask-AppBuilder is set to ``AUTH_TYPE`` set to ``AUTH_OID``, it allows an attacker to forge an HTTP\nrequest that could deceive the backend into using any requested OpenID service. This vulnerability\ncould grant an attacker unauthorised privilege access if a custom OpenID service is deployed\nby the attacker and accessible by the backend.\n\nThis vulnerability is only exploitable when the application is using OpenID (not OpenID Connect also known\nas OIDC). Currently, this protocol is regarded as legacy, with significantly reduced usage.\n\n## Possible remediation\n\n* Change your authentication method - if you are using ``AUTH_OID``, there are almost no commercial services\n  supporting it, it was d","url":"fab-oid-vulnerability","title":"Vulnerability in long deprecated OpenID authentication method in Flask AppBuilder","linkTitle":"Vulnerability in long deprecated OpenID authentication method in Flask AppBuilder","author":"Jarek Potiuk","github":"potiuk","linkedin":"jarek.potiuk","description":"Advising users who still use a long-deprecated OpenID authentication method in Flask AppBuilder to upgrade to Apache Airflow 2.8.2","tags":["Vulnerabilities"],"date":"2024-02-26"},{"content":"My [Outreachy internship](https://outreachy.org) is coming to its ends which is also the best time to look back and\nreflect on the progress so far.\n\nThe goal of my project is to Extend and Improve the Apache Airflow REST API. In this post,\nI will be sharing my progress so far.\n\nWe started a bit late implementing the REST API because it took time for the OpenAPI 3.0\nspecification we were to use for the project to be merged. Thanks to [Kamil](https://github.com/mik-laj),\nwho paved the way for us to start implementing the REST API endpoints. Below are the endpoints I\nimplemented and the challenges I encountered, including how I overcame them.\n\n### Implementing The Read-Only Connection Endpoints\nThe [read-only connection endpoints](https://github.com/apache/airflow/pull/9095) were the first endpoint I implemented. Looking back,\nI can see how much I have improved.\n\nI started by implementing the database schema for the Connection table using [Marshmallow 2](https://marshmallow.readthedocs.io/en/2.x-line/).\nWe had to use Marshmallow 2 because Flask-AppBuilder was still using it and Flask-AppBuilder\nis deeply integrated to Apache Airflow. This meant I had to unlearn Marshmallow 3 that I had\n been studying before this realization, but thankfully, [Marshmallow 3](https://marshmallow.readthedocs.io/en/stable/index.html) isn't too\n different, so I was able to start using Marshmallow 2 in no time.\n\nThis first PR would have been more difficult than it was unless there had been any reference\nendpoint to look at. [Kamil](https://github.com/mik-laj) implemented a [draft PR](https://github.com/apache/airflow/pull/9045) in which I took inspiration from.\nThanks to this, It was easy for me to write the unit tests. It was also in this endpoint that\n I learned using [parameterized](https://github.com/wolever/parameterized) in unit tests :D.\n\n### Implementing The Read-Only DagRuns Endpoints\n\nThis [endpoint](https://github.com/apache/airflow/pull/9153) came with its many challenges, especially on filtering with `datetimes`.\nThis was because the `connexion` library we were using to build the REST API was not validating\ndate-time format in OpenAPI 3.0 specification, what I eventually found out, was intentional.\nConnexion dropped `strict-rfc3339` because of the later license which is not compatible with\nApache 2.0 license.\n\nI implemented a workaround on this, by defining a function called `conn_parse_datetime` in the\nAPI utils module. This was later refactored and thankfully, [Kamil](https://github.com/mik-laj)\n implemented a decorator that allowed us to have cleaner code on the views while using this function.\n\nThen we tried using `rfc3339-validator` whose license is compatible with Apache 2.0 licence but\n later discarded this because with our custom date parser we were able to use duration and\n not just date times.\n\n### Other Endpoints\nI implemented some different other endpoints. One peculiar issue I faced was because of Marshmallow 2\nnot giving error when extra fields a","url":"implementing-stable-API-for-Apache-Airflow","title":"Implementing Stable API for Apache Airflow","linkTitle":"Implementing Stable API for Apache Airflow","author":"Ephraim Anierobi","twitter":"ephraimbuddy","github":"ephraimbuddy","description":"An Outreachy intern's progress report on contributing to Apache Airflow REST API.","tags":["REST API"],"date":"2020-07-19"},{"content":"In data pipelines, commonly we need to create infrastructure resources, like a cluster or GPU nodes in an existing cluster, before doing the actual “work” and delete them after the work is done. Airflow 2.7 adds “setup” and “teardown” tasks to better support this type of pipeline. This blog post aims to highlight the key features so you know what’s possible. For full documentation on how to use setup and teardown tasks, see the [setup and teardown docs](https://airflow.apache.org/docs/apache-airflow/2.7.0/howto/setup-and-teardown.html).\n\n## Why setup and teardown?\n\nBefore we dig into examples, let me state at high level what setup and teardown bring to the table.\n\n### More expressive dependencies\n\nBefore setup and teardown, upstream and downstream relationships could only mean one thing: “this comes before that”. With setup and teardown, in effect we can say “this requires that”. And what it means in practice is, if you clear your task, and it requires a setup, that setup will be cleared too. And if that setup has a teardown, that will run again as well.\n\n### Separating the work from the infra\n\nSometimes the part of the dag you care about is not, say, the cleanup task. For example, suppose you have a dag that loads some data and then deletes temp files. As long as the data loads, you want your dag to be marked successful. By default, this is how teardown tasks work; that is, they are ignored when determining dag run state.\n\n## Simple case\n\nA simple example is one setup / teardown pair, and one normal or “work” task.\n\n![Simple setup and teardown example](simple.png)\n\nSetups and teardowns are indicated by the up and down arrows, respectively. From that we can see that .`create_cluster` is a setup task and `delete_cluster` is a teardown. The link between a setup and a teardown is always dotted to highlight the special relationship.\n\nSome things to observe:\n\n* If `create_cluster` fails, neither `run_query` nor `delete_cluster` will run.\n* If `create_cluster` succeeds and `run_query` fails, then `delete_cluster` will still run.\n* If `create_cluster` is skipped, `run_query` and `delete_cluster` will be skipped\n* By default, if `run_query` succeeds, and `delete_cluster` fails, then the dag run will still be marked successful. (This behavior can be overridden).\n\n## Authoring with task groups\n\nWhen we set something downstream of a task group, any teardowns in the task group are ignored. This reflects the assumption that in general, we probably don’t want to stop dag execution just because a teardown fails. So, let’s wrap the above dag in a task group and see what happens:\n\n![Setup and teardown in task groups](task-group-arrow.png)\n\nAnd here’s how we linked those groups in the code:\n\n```python\nwith TaskGroup(\"do_emr\") as do_emr:\n    create_cluster_task = create_cluster()\n    run_query(create_cluster_task) >> delete_cluster(create_cluster_task)\n\nwith TaskGroup(\"load\") as load:\n    create_config_task = create_configuration()\n    load_data(create_config_task)","url":"introducing_setup_teardown","title":"Introducing Setup and Teardown tasks","linkTitle":"Introducing Setup and Teardown tasks","author":"Daniel Standish","github":"dstandish","linkedin":"daniel-standish-12197714","description":"An introduction to Setup and Teardown tasks, which are new in Apache Airflow 2.7.0","date":"2023-08-18"}]