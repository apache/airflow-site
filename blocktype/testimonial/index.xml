<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Testimonial on Apache Airflow</title>
    <link>/blocktype/testimonial/</link>
    <description>Recent content in Testimonial on Apache Airflow</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="/blocktype/testimonial/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>adjoe</title>
      <link>/use-cases/adjoe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/adjoe/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;Before adopting Airflow at adjoe, we handled job scheduling in two main ways: by setting up Kubernetes cronjobs or building AWS Lambda functions. While both approaches had their benefits, they also came with limitations, especially when it came to managing more complex workloads. As our data science teams needs evolved, it became clear that we needed a more robust and flexible orchestration tool.&lt;/p&gt;&#xA;&lt;h5 id=&#34;how-did-apache-airflow-help-to-solve-this-problem&#34;&gt;How did Apache Airflow help to solve this problem?&lt;/h5&gt;&#xA;&lt;p&gt;With the creation of a new AWS environment for the data science teams, we introduced Airflow on Kubernetes as our primary orchestration solution, addressing both stability and scalability requirements.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adobe</title>
      <link>/use-cases/adobe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/adobe/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;Modern big data platforms need sophisticated data pipelines connecting to many backend services enabling complex workflows. These workflows need to be deployed, monitored, and run either on regular schedules or triggered by external events. Adobe Experience Platform component services architected and built an orchestration service to enable their users to author, schedule, and monitor complex hierarchical (including sequential and parallel) workflows for Apache Spark (TM) and non-Spark jobs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adyen</title>
      <link>/use-cases/adyen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/adyen/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;Many years ago we started out with our own orchestration framework. Due to all the required custom functionality it made sense at the time. However, quickly we realized creating an orchestration tool is not to be underestimated.  With the quickly increasing number of users and teams, time spent on fixing issues increased, severely limiting development speed. Furthermore, due to it not being open source, we constantly had to make the effort ourselves to stay up to date with the industry standards and tools. We needed a tool for our Big Data Platform to schedule and execute many ETL jobs while at the same time, giving our users the possibility to redo or undo their tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Big Fish Games</title>
      <link>/use-cases/big-fish-games/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/big-fish-games/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;The main challenge is the lack of standardized  ETL workflow orchestration tools. PowerShell and Python-based ETL frameworks built in-house are currently used for scheduling and running analytical workloads. However, there is no web UI through which we can monitor these workflows and it requires additional effort to maintain this framework. These scheduled jobs based on external dependencies are not well suited to modern Big Data platforms and their complex workflows. Although we experimented with Apache Oozie for certain workflows, it did not handle failed jobs properly. For late data arrival, these tools are not flexible enough to enforce retry attempts for the job failures.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dish</title>
      <link>/use-cases/dish/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/dish/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;We faced increasing complexity managing lengthy crontabs with scheduling being an issue, this required carefully planning timing due to resource constraints, usage patterns, and especially custom code needed for retry logic.  In the last case, having to verify success of previous jobs and/or steps prior to running the next.  Furthermore, time to results is important, but we were increasingly relying on buffers for processing, where things were effectively sitting idle and not processing, waiting for the next stage, in an effort to not rely as much on custom code/logic.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Experity</title>
      <link>/use-cases/experity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/experity/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;We had to deploy our complex, flagship app to multiple nodes in multiple ways. This required tasks to communicate across Windows nodes and coordinate timing perfectly. We did not want to buy an expensive enterprise scheduling tool and needed ultimate flexibility.&lt;/p&gt;&#xA;&lt;h5 id=&#34;how-did-apache-airflow-help-to-solve-this-problem&#34;&gt;How did Apache Airflow help to solve this problem?&lt;/h5&gt;&#xA;&lt;p&gt;Ultimately we decided flexible, multi-node, DAG capable tooling was key and airflow was one of the few tools that fit that bill. Having it based on open source and python were large factors that upheld our core principles. At the time, Airflow was missing a windows hook and operator so we contributed the WinRM hook and operator back to the community. Given its flexibility we also use DAG generators to have our metadata drive our DAGs and keep maintenance costs down.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Onefootball</title>
      <link>/use-cases/onefootball/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/onefootball/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;With millions of daily active users, managing the complexity of data engineering at Onefootball is a constant challenge. Lengthy crontabs, multiplication of custom API clients, erosion of confidence in the analytics served, increasing heroism (&amp;ldquo;only one person can solve this issue&amp;rdquo;). Those are the challenges that most teams face unless they consciously invest in their tools and processes.&lt;/p&gt;&#xA;&lt;p&gt;On top of that, new data tools appear each month: third party data sources, cloud providers solutions, different storage technologies&amp;hellip; Managing all those integrations is costly and brittle, especially for small data engineering teams that are trying to do more with less.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Plarium Krasnodar</title>
      <link>/use-cases/plarium-krasnodar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/plarium-krasnodar/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;Our Research &amp;amp; Development department carries out various experiments, and in all of them, we need to create workflow orchestrations for solving tasks in game dev. Previously, we didn&amp;rsquo;t have any suitable tools with a sufficient number of built-in functions, and we had to orchestrate processes manually and entirely from scratch every time. This led to difficulties with dependencies and monitoring when building complex workflows. We needed a tool that would provide a more centralized approach so that we could see all the logs, the number of retries, and the task performance time. The most important thing that we lacked was the ability to backfill historical data and restart failed tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RancherBySUSE</title>
      <link>/use-cases/suse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/suse/</guid>
      <description>&lt;h4 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h4&gt;&#xA;&lt;p&gt;Our aim was to build, package, test and distribute curated and trusted containers at scale in an automated way. Those containers can be of any nature, meaning that we need a solution that allows us to build any kind of software with any kind of building tools like Maven, Rust, Java, Ant, or Go.&lt;/p&gt;&#xA;&lt;p&gt;The construction of these containers requires the installation of several libraries (which may even conflict) and the orchestration of complex workflows with several integrations, executed either on a scheduled basis or triggered by events from external systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Seniorlink</title>
      <link>/use-cases/seniorlink/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/seniorlink/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;Here at Seniorlink, we provide services, support, and technology that engages family caregivers. One of our focuses is using data to bolster our knowledge and improve the experience of our users. Like many looking to build an effective data stack, we adopted a Python, Spark, Redshift, and Tableau core toolset.&lt;/p&gt;&#xA;&lt;p&gt;We had built a robust stack of batch processes to deliver value to the business, deploying these data services in AWS using a mixture of EMR, ECS, Lambda, and EC2. Moving fast, as many new endeavors do, we ultimately ended up with one monolithic batch process with many smaller satellite jobs. Given the scale and quantity of jobs, we began to lose transparency as to what was happening. Additionally, many jobs were launched in a single EMR cluster and so tightly coupled that a failure in one job required the recompute of all the jobs run on that cluster. These behaviors are highly inefficient, difficult to debug and result in long iteration periods given the duration of these batch jobs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sift</title>
      <link>/use-cases/sift/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/sift/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;At Sift, we’re constantly training machine learning models that feed into the core of Sift’s Digital Trust &amp;amp; Safety platform. The platform gives our customers a way to discern suspicious online behavior from trustworthy behavior, allowing our customers to protect their online transactions, maintain the integrity of their content platforms, and keep their users’ accounts secure. To make this possible, we’ve built model training pipelines that consist of hundreds of steps in MapReduce and Spark, with complex requirements between them.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Snapp</title>
      <link>/use-cases/snapp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/use-cases/snapp/</guid>
      <description>&lt;h5 id=&#34;what-was-the-problem&#34;&gt;What was the problem?&lt;/h5&gt;&#xA;&lt;p&gt;As the Map team at Snapp, one of the largest and fastest-growing internet companies in the Middle East, we have experienced significant growth over the past couple of years, expanding from a team of 7 to a team of 60. However, with this growth came the realization that some of our crucial tasks were being performed manually. This manual approach consumed valuable time and hindered our ability to execute these tasks efficiently.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
