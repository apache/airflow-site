<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Airflow â€“ use-case</title>
    <link>/blocktype/use-case/</link>
    <description>Recent content in use-case on Apache Airflow</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/blocktype/use-case/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Use-Cases: Business Operations</title>
      <link>/use-cases/business_operations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/business_operations/</guid>
      <description>
        
        
        &lt;div style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;h1 id=&#34;use-airflow-for-business-operations-pipelines&#34;&gt;Use Airflow for Business operations pipelines&lt;/h1&gt;
&lt;/div&gt;
&lt;p&gt;Airflow can be the starting point for your business idea! For many companies, Airflow delivers the data that powers their core business applications. Whether you need to aggregate user data to power personalized recommendations, display analytics in a user-facing dashboard, or prepare the input data for an LLM, Airflow is the perfect orchestrator.&lt;/p&gt;
&lt;p&gt;This video shows an example of using Airflow to run the pipelines that power a customer-facing analytics dashboard. You can find the code shown in this example &lt;a href=&#34;https://github.com/astronomer/business-operations-structure-example&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;videoContainer&#34; style=&#34;display: flex; justify-content: center; align-items: center; border: 2px solid #ccc; width: 75%; margin: auto; padding: 20px;&#34;&gt;
    &lt;a href=&#34;https://www.youtube.com/embed/2CEApKN0z1U?autoplay=1&#34;&gt;
        &lt;img id=&#34;videoPlaceholder&#34; src=&#34;/usecase-video-placeholders/placeholder_business_ops_video.png&#34; style=&#34;cursor: pointer; width: 100%; max-width: 560px;&#34; alt=&#34;Click to play a one minute video showing the use case&#34; title=&#34;Click to play video&#34;/&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;/br&gt;
&lt;h2 id=&#34;why-use-airflow-for-business-operations&#34;&gt;Why use Airflow for Business Operations?&lt;/h2&gt;
&lt;p&gt;Airflow is trusted and tested by many companies to deliver their data on time. Airflow is a popular choice to build your business upon, because it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tool agnostic&lt;/strong&gt;: Using Airflow future-proofs your business, as it can be used to orchestrate actions in nearly any external tool or service. This means you can always switch to the newest and best tools, without needing to change your whole orchestration layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: There are many Airflow modules available to connect to popular data tools, and you can write your own custom operators and hooks for specific use cases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: In Airflow you can define &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html&#34;&gt;dynamic tasks&lt;/a&gt;, which serve as placeholders to adapt at runtime based on changing input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Airflow can be scaled to handle infinite numbers of tasks and workflows, given enough computing power. If you choose Airflow, your business will be able to grow with it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/br&gt;
&lt;h2 id=&#34;airflow-features-for-business-operations&#34;&gt;Airflow features for Business Operations&lt;/h2&gt;
&lt;p&gt;Airflow has several key features that make it a great option for orchestrating business operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html&#34;&gt;&lt;strong&gt;Dynamic task mapping&lt;/strong&gt;&lt;/a&gt;: Oftentimes business operations are not static. You may design your pipelines to have one task per customer or report, and those lists will always be changing. Dynamic task mapping allows you to build flexibility into your pipelines, so they can adjust at runtime based on changing input.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/datasets.html&#34;&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/a&gt;: It is unlikely that you will have one team, much less one pipeline, responsible for all of the data that powers your business. Datasets allow you to make your pipelines event-based, scheduling them for when all data prerequisites are available rather than a specific time. With this type of scheduling, you can create smaller, more modular pipelines, that can be managed by the team responsible for that data, making your operations more efficient and easier to manage.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow-providers/core-extensions/notifications.html&#34;&gt;&lt;strong&gt;Notifications&lt;/strong&gt;&lt;/a&gt;: When relying on an orchestrator to power your business applications, it&amp;rsquo;s critical that you know promptly when something goes wrong. Airflow has a suite of notifications available so you can send alerts to your system of preference.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Use-Cases: ETL/ELT</title>
      <link>/use-cases/etl_analytics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/etl_analytics/</guid>
      <description>
        
        
        &lt;div style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;h1 id=&#34;use-airflow-for-etlelt-pipelines&#34;&gt;Use Airflow for ETL/ELT pipelines&lt;/h1&gt;
&lt;/div&gt;
&lt;p&gt;Extract-Transform-Load (ETL) and Extract-Load-Transform (ELT) data pipelines are the most common use case for Apache Airflow. 90% of respondents in the 2023 Apache Airflow survey are using Airflow for ETL/ELT to power analytics use cases.&lt;/p&gt;
&lt;p&gt;The video below shows a simple ETL/ELT pipeline in Airflow that extracts climate data from a CSV file, as well as weather data from an API, runs transformations and then loads the results into a database to power a dashboard. You can find the code for this example &lt;a href=&#34;https://github.com/astronomer/airflow-quickstart&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;videoContainer&#34; style=&#34;display: flex; justify-content: center; align-items: center; border: 2px solid #ccc; width: 75%; margin: auto; padding: 20px;&#34;&gt;
    &lt;a href=&#34;https://www.youtube.com/embed/ljBU_VyihVQ?autoplay=1&#34;&gt;
        &lt;img id=&#34;videoPlaceholder&#34; src=&#34;/usecase-video-placeholders/placeholder_etl_video.png&#34; style=&#34;cursor: pointer; width: 100%; max-width: 560px;&#34; alt=&#34;Click to play a one minute video showing the use case&#34; title=&#34;Click to play video&#34;/&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;/br&gt;
&lt;h2 id=&#34;why-use-airflow-for-etlelt-pipelines&#34;&gt;Why use Airflow for ETL/ELT pipelines?&lt;/h2&gt;
&lt;p&gt;Airflow is the de-facto standard for defining ETL/ELT pipelines as Python code. Airflow is popular for this use case because it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tool agnostic&lt;/strong&gt;: Airflow can be used to orchestrate ETL/ELT pipelines for any data source or destination.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: There are many Airflow modules available to connect to any data source or destination, and you can write your own custom operators and hooks for specific use cases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: In Airflow you can define &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html&#34;&gt;dynamic tasks&lt;/a&gt;, which serve as placeholders to adapt at runtime based on changing input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Airflow can be scaled to handle infinite numbers of tasks and workflows, given enough computing power.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;airflow-features-for-etlelt-pipelines&#34;&gt;Airflow features for ETL/ELT pipelines&lt;/h2&gt;
&lt;p&gt;Airflow has several key features that make it a great option for ETL/ELT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/datasets.html&#34;&gt;Datasets&lt;/a&gt;&lt;/strong&gt;: In Airflow you can schedule your DAGs in a data-driven way, based on updates to Datasets from any other task in your Airflow instance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/objectstorage.html&#34;&gt;Object Storage&lt;/a&gt;&lt;/strong&gt;: The Airflow Object Storage is an abstraction over the &lt;a href=&#34;https://docs.python.org/3/library/pathlib.html&#34;&gt;Path API&lt;/a&gt; that simplifies interaction with object storage systems such as Amazon S3, Google Cloud Storage, and Azure Blob Storage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow-providers/index.html&#34;&gt;Airflow providers&lt;/a&gt;&lt;/strong&gt;: Airflow providers extend core Airflow functionality with additional modules to simplify integration with popular data tools. You can find a list of active providers &lt;a href=&#34;https://airflow.apache.org/docs/#active-providers&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Use-Cases: Infrastructure Management</title>
      <link>/use-cases/infrastructure-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/infrastructure-management/</guid>
      <description>
        
        
        &lt;div style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;h1 id=&#34;use-airflow-for-infrastructure-management&#34;&gt;Use Airflow for Infrastructure Management&lt;/h1&gt;
&lt;/div&gt;
&lt;p&gt;Airflow can interact with any API, which makes it a great tool to manage your infrastructure, such as Kubernetes or Spark clusters running in any cloud. As of Airflow 2.7, the setup/teardown feature is available, a special type of task with intelligent behavior to spin up and tear down infrastructure at the exact time you need it.&lt;/p&gt;
&lt;p&gt;Infrastructure management is often needed within the context of other use cases, such as MLOps, or implementing data quality checks. This video shows an example of how it might be used for an MLOps pipeline. You can find the code shown in this example &lt;a href=&#34;https://github.com/astronomer/use-case-setup-teardown-data-quality&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;videoContainer&#34; style=&#34;display: flex; justify-content: center; align-items: center; border: 2px solid #ccc; width: 75%; margin: auto; padding: 20px;&#34;&gt;
    &lt;a href=&#34;https://www.youtube.com/embed/JkURWnl76GQ?autoplay=1&#34;&gt;
        &lt;img id=&#34;videoPlaceholder&#34; src=&#34;/usecase-video-placeholders/placeholder_infra_video.png&#34; style=&#34;cursor: pointer; width: 100%; max-width: 560px;&#34; alt=&#34;Click to play a one minute video showing the use case&#34; title=&#34;Click to play video&#34;/&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;/br&gt;
&lt;h2 id=&#34;why-use-airflow-for-infrastructure-management&#34;&gt;Why use Airflow for Infrastructure Management&lt;/h2&gt;
&lt;p&gt;Airflow is a popular choice for pipelines that require managing infrastructure because it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python native&lt;/strong&gt;: Pipelines as Python code make it easy to turn custom functions into tasks. Any logic you need to manage your infrastructure, you can implement in Airflow with Python.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: Infrastructure management is needed for many use cases, including MLOps, data quality checks, and more. Airflow&amp;rsquo;s flexibility and wide array of providers makes it suitable for any use case that you may need to implement.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Airflow can be scaled to handle infinite numbers of tasks and workflows, given enough computing power. If you choose Airflow, your business will be able to grow with it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;airflow-features-for-infrastructure-management&#34;&gt;Airflow features for Infrastructure Management&lt;/h2&gt;
&lt;p&gt;Airflow 2.7 implemented a new key feature that makes it an even greater option for managing infrastructure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/howto/setup-and-teardown.html&#34;&gt;&lt;strong&gt;Setup/teardown tasks&lt;/strong&gt;&lt;/a&gt;: Setup/teardown tasks are a special type of task that can be used to manage the infrastructure needed to run other tasks. They have special behavior to support the pattern of setting up resources and configuration (e.g. a Spark cluster or other compute resources) before a task runs, and then tearing down that infrastructure after the task has completed, even if the task fails.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Use-Cases: MLOps</title>
      <link>/use-cases/mlops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/use-cases/mlops/</guid>
      <description>
        
        
        &lt;div style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;h1 id=&#34;use-airflow-for-machine-learning-operations-mlops&#34;&gt;Use Airflow for Machine Learning Operations (MLOps)&lt;/h1&gt;
&lt;/div&gt;
&lt;p&gt;Machine Learning Operations (MLOps) is a broad term encompassing everything needed to run machine learning models in production. MLOps is a rapidly evolving field with many different best practices and behavioral patterns, with Apache Airflow providing tool agnostic orchestration capabilities for all steps. An emerging subset of MLOps is Large Language Model Operations (LLMOps), which focuses on developing pipelines around applications of large language models like GPT-4 or Command.&lt;/p&gt;
&lt;p&gt;The following video shows an example of using Airflow and Weaviate to create an automatic RAG pipeline that ingests and embeds data from news articles and provides trading advice. You can find the code shown in this example &lt;a href=&#34;https://github.com/astronomer/use-case-airflow-llm-rag-finance&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;videoContainer&#34; style=&#34;display: flex; justify-content: center; align-items: center; border: 2px solid #ccc; width: 75%; margin: auto; padding: 20px;&#34;&gt;
    &lt;a href=&#34;https://www.youtube.com/embed/QcBdh_n4es4?autoplay=1&#34;&gt;
        &lt;img id=&#34;videoPlaceholder&#34; src=&#34;/usecase-video-placeholders/placeholder_mlops_video.png&#34; style=&#34;cursor: pointer; width: 100%; max-width: 560px;&#34; alt=&#34;Click to play a one minute video showing the use case&#34; title=&#34;Click to play video&#34;/&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;/br&gt;
&lt;h2 id=&#34;why-use-airflow-for-mlops&#34;&gt;Why use Airflow for MLOps?&lt;/h2&gt;
&lt;p&gt;Airflow is a popular choice for orchestrating MLOps workflows because it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python native&lt;/strong&gt;: You use Python code to define Airflow pipelines, which makes it easy to integrate the most popular machine learning tools and embed your ML operations in a best practice CI/CD workflow. By using the decorators of the TaskFlow API you can turn existing scripts into Airflow tasks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: Airflow itself is written in Python, which makes it extensible with &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html&#34;&gt;custom modules&lt;/a&gt; and &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/plugins.html&#34;&gt;Airflow plugins&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data agnostic&lt;/strong&gt;: Airflow is data agnostic, which means it can be used to orchestrate any data pipeline, regardless of the data format or storage solution. You can plug in any new data storage, such as the latest vector database or your favorite RDBMS, with minimal effort.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;airflow-features-for-mlops&#34;&gt;Airflow features for MLOps&lt;/h2&gt;
&lt;p&gt;Airflow has several key features that make it a great option for orchestrating MLOps workflows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Monitoring and alerting&lt;/strong&gt;: Airflow comes with production-ready monitoring and alerting modules like Airflow notifiers, extensive logging features, and Airflow listeners. They enable you to have fine-grained control over how you monitor your ML operations and how Airflow alerts you if something goes wrong.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Features for day 2 ops&lt;/strong&gt;: Simple features like automatic retries, complex dependencies and branching logic, as well as the option to make pipelines dynamic make a big difference when orchestrating MLOps pipelines. Airflow has all of these built-in.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow-providers/index.html&#34;&gt;Airflow providers&lt;/a&gt;&lt;/strong&gt;: Airflow providers extend core Airflow functionality with additional modules to simplify integration with popular data tools, including many popular MLOps tools. You can find a list of active providers &lt;a href=&#34;https://airflow.apache.org/docs/#active-providers&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
