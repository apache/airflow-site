<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Airflow ‚Äì release</title>
    <link>/blog/tags/release/</link>
    <description>Recent content in release on Apache Airflow</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 08 Aug 2024 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="/blog/tags/release/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Blog: Apache Airflow 2.10.0 is here</title>
      <link>/blog/airflow-2.10.0/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.10.0/</guid>
      <description>
        
        
        &lt;p&gt;I&amp;rsquo;m happy to announce that Apache Airflow 2.10.0 is now available, bringing an array of noteworthy enhancements and new features that will greatly serve our community.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.10.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.10.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.10.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.10.0/&lt;/a&gt; &lt;br&gt;
üõ† Release Notes: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.10.0/release_notes.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.10.0/release_notes.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: &amp;ldquo;docker pull apache/airflow:2.10.0&amp;rdquo; &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.10.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.10.0&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;airflow-now-collects-telemetry-data-by-default&#34;&gt;Airflow now collects Telemetry data by default&lt;/h2&gt;
&lt;p&gt;With the release of Airflow 2.10.0, we‚Äôve introduced the collection of basic telemetry data, as outlined &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.10.0/faq.html#does-airflow-collect-any-telemetry-data&#34;&gt;here&lt;/a&gt;. This data will play a crucial role in helping Airflow maintainers gain a deeper understanding of how Airflow is utilized across various deployments. The insights derived from this information are invaluable in guiding the prioritization of patches, minor releases, and security fixes. Moreover, this data will inform key decisions regarding the development roadmap, ensuring that Airflow continues to evolve in line with community needs.&lt;/p&gt;
&lt;p&gt;For those who prefer not to participate in data collection, deployments can easily opt-out by setting the &lt;code&gt;[usage_data_collection] enabled&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt; or by using the &lt;code&gt;SCARF_ANALYTICS=false&lt;/code&gt; environment variable.&lt;/p&gt;
&lt;h2 id=&#34;multiple-executor-configuration-formerly-hybrid-execution&#34;&gt;Multiple Executor Configuration (formerly &amp;ldquo;Hybrid Execution&amp;rdquo;)&lt;/h2&gt;
&lt;p&gt;Each executor comes with its unique set of strengths and weaknesses, typically balancing latency, isolation, and compute efficiency. Traditionally, an Airflow environment is limited to a single executor, requiring users to make trade-offs, as no single executor is perfectly suited for all types of tasks.&lt;/p&gt;
&lt;p&gt;We are introducing a new feature that allows for the concurrent use of multiple executors within a single Airflow environment. This flexibility enables users to take advantage of the specific strengths of different executors for various tasks, improving overall efficiency and mitigating weaknesses. Users can set a default executor for the entire environment and, if necessary, assign particular executors to individual DAGs or tasks.&lt;/p&gt;
&lt;p&gt;To configure multiple executors we can pass comma separated list in airflow configuration. The first executor in the list will be the default executor for the environment.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[core]
executor = &#39;LocalExecutor,CeleryExecutor&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To make it easier for dag authors, we can also specify aliases for executors that can be specified in the executor configuration&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-commandline&#34; data-lang=&#34;commandline&#34;&gt;[core]
executor = &#39;LocalExecutor,KubernetesExecutor,my.custom.module.ExecutorClass:ShortName&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;DAG authors can specify executors to use at the task&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;BashOperator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;hello_world&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;executor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ShortName&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;bash_command&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;echo &amp;#39;hello world!&amp;#39;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;executor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;KubernetesExecutor&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;hello_world&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;hello world!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can also specify executors on the DAG level&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;hello_world&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;hello world!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;hello_world_again&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;hello world again!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;hello_worlds&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;default_args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;executor&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ShortName&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Applies to all tasks in the DAG&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# All tasks will use the executor from default args automatically&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;hw&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hello_world&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;hw_again&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hello_world_again&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;dynamic-dataset-scheduling-through-datasetalias&#34;&gt;Dynamic Dataset scheduling through DatasetAlias&lt;/h2&gt;
&lt;p&gt;Airflow 2.10 comes with &lt;code&gt;DatasetAlias&lt;/code&gt; class which can be passed as a value in the &lt;code&gt;outlets&lt;/code&gt;, &lt;code&gt;inlets&lt;/code&gt; on a task, and &lt;code&gt;schedule&lt;/code&gt; on a DAG. An instance of &lt;code&gt;DatasetAlias&lt;/code&gt; is resolved dynamically to a real dataset. Downstream can depend on either the resolved dataset or on an alias itself.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DatasetAlias&lt;/code&gt; has one argument &lt;code&gt;name&lt;/code&gt; that uniquely identifies the dataset. The task must first declare the alias as an outlet, and use &lt;code&gt;outlet_events&lt;/code&gt; or &lt;code&gt;yield Metadata&lt;/code&gt; to add events to it.&lt;/p&gt;
&lt;h3 id=&#34;emit-a-dataset-event-during-task-execution-through-outlet_events&#34;&gt;Emit a dataset event during task execution through outlet_events&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.datasets&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DatasetAlias&lt;/span&gt;

&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;outlets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DatasetAlias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;my-task-outputs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;my_task_with_outlet_events&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;outlet_events&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;outlet_events&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;my-task-outputs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;s3://bucket/my-task&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;emit-a-dataset-event-during-task-execution-by-yielding-metadata&#34;&gt;Emit a dataset event during task execution by yielding Metadata&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.datasets.metadata&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Metadata&lt;/span&gt;

&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;outlets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DatasetAlias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;my-task-outputs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;my_task_with_metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;s3_dataset&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;s3://bucket/my-task}&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;yield&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s3_dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;my-task-outputs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are two options for scheduling based on dataset aliases. Schedule based on &lt;code&gt;DatasetAlias&lt;/code&gt; or real datasets.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dataset-alias-producer&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;outlets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DatasetAlias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;example-alias&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;produce_dataset_events&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;outlet_events&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;outlet_events&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;example-alias&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;s3://bucket/my-task&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dataset-consumer&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schedule&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;s3://bucket/my-task&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dataset-alias-consumer&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schedule&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DatasetAlias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;example-alias&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;dataset-aliases-ui-enhancements&#34;&gt;Dataset Aliases UI Enhancements&lt;/h3&gt;
&lt;p&gt;Now users can see Dataset Aliases in legend of each cross-dag dependency graph with a corresponded icon/color.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dag_dependencies_legend.png&#34; alt=&#34;DAG Dependencies graph&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;dark-mode-for-airflow-ui&#34;&gt;Dark Mode for Airflow UI&lt;/h2&gt;
&lt;p&gt;Airflow 2.10 comes with new Dark Mode feature which is designed to enhance user experience by offering an alternative visual theme that is easier on the eyes, especially in low-light conditions. You can toggle the crescent icon on the right side of the navigation bar to switch between light and dark mode.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;airflow_dark_mode.png&#34; alt=&#34;Airflow Dark mode&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;airflow_light_mode.png&#34; alt=&#34;Airflow Light mode&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;task-instance-history&#34;&gt;Task Instance History&lt;/h2&gt;
&lt;p&gt;In Apache Airflow 2.10.0, when a task instance is retried or cleared, its execution history is maintained. You can view this history by clicking on the task instance in the Grid view, allowing you to access information about each attempt, such as logs, execution durations, and any failures. This feature improves transparency into the task&amp;rsquo;s execution process, making it easier to troubleshoot and analyze your DAGs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;task_instance_history.png&#34; alt=&#34;Task instance history&#34;&gt;&lt;/p&gt;
&lt;p&gt;The history displays the final values of the task instance attributes for each specific run. On the log page, you can also access the logs for each attempt of the task instance. This information is valuable for debugging purposes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;task_instance_history_log.png&#34; alt=&#34;Task instance history&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;dataset-ui-enhancements&#34;&gt;Dataset UI Enhancements&lt;/h2&gt;
&lt;p&gt;The dataset page has been revamped to include a focused dataset events section with additional details such as extras, consuming DAGs, and producing tasks.
&lt;img src=&#34;dataset_list.png&#34; alt=&#34;Dataset list&#34;&gt;&lt;/p&gt;
&lt;p&gt;We now have separate dependency graph and dataset list pages in new tabs, enhancing the user experience.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dependency_graph.png&#34; alt=&#34;Dataset dependency graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;Dataset events are now displayed in both the Details tab of each DAG run and within the DAG graph.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dataset_details.png&#34; alt=&#34;Dataset list&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;toggle-datasets-in-graph&#34;&gt;Toggle datasets in Graph&lt;/h3&gt;
&lt;p&gt;We can now toggle the datasets in the DAG graph&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dataset_toggle_on.png&#34; alt=&#34;Dataset toggle button on&#34;&gt;
&lt;img src=&#34;dataset_toggle_off.png&#34; alt=&#34;Dataset toggle button off&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;dataset-conditions-in-dag-graph-view&#34;&gt;Dataset Conditions in DAG Graph view&lt;/h3&gt;
&lt;p&gt;We now display the graph view with logical gates. Datasets with actual events are highlighted with a different border, making it easier to see what triggered the selected run.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;render_dataset_conditions.png&#34; alt=&#34;Render dataset conditions in graph view&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;dataset-event-info-in-dag-graph&#34;&gt;Dataset event info in DAG Graph&lt;/h3&gt;
&lt;p&gt;For a DAG run, users can now view the dataset events connected to it directly in the graph view.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dataset_info.png&#34; alt=&#34;Dataset event info&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;on-demand-dag-re-parsing&#34;&gt;On-demand DAG Re-parsing&lt;/h2&gt;
&lt;p&gt;In 2.10 users can now reparse the DAGs on demand using below button on DAG list and DAG detail pages&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;DAG_reparsing_button_list.png&#34; alt=&#34;DAG Reparsing button on DAG list page&#34;&gt;
&lt;img src=&#34;DAG_reparse_button_detail.png&#34; alt=&#34;DAG Reparsing button on DAG detail page&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;additional-new-features&#34;&gt;Additional new features&lt;/h2&gt;
&lt;p&gt;Here are just a few interesting new features since there are too many to list in full:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deferrable operators can now execute directly from the triggerer without needing to go through the worker. This is especially efficient for certain operators, like sensors, and can help teams save both time and money.&lt;/li&gt;
&lt;li&gt;Crucial executor logs are now integrated into the task logs. If the executor fails to start a task, the relevant error messages will be available in the task logs, simplifying the debugging process.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contributors&#34;&gt;Contributors&lt;/h2&gt;
&lt;p&gt;Thanks to everyone who contributed to this release, including Andrey Anshin, Brent Bovenzi, Daniel Standish, Ephraim Anierobi, Hussein Awala, Jarek Potiuk, Jed Cunningham, Jens Scheffler, Tzu-ping Chung, Vincent, and over 63 others!&lt;/p&gt;
&lt;p&gt;I hope you enjoy using Apache Airflow 2.10.0!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 2.9.0: Dataset and UI Improvements</title>
      <link>/blog/airflow-2.9.0/</link>
      <pubDate>Mon, 08 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.9.0/</guid>
      <description>
        
        
        &lt;p&gt;I‚Äôm happy to announce that Apache Airflow 2.9.0 has been released! This time around we have new features for data-aware scheduling and a bunch of UI-related improvements.&lt;/p&gt;
&lt;p&gt;Apache Airflow 2.9.0 contains over 550 commits, which include 38 new features, 70 improvements, 31 bug fixes, and 18 documentation changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.9.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.9.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.9.0/&lt;/a&gt; &lt;br&gt;
üõ† Release Notes: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/release_notes.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.9.0/release_notes.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: &amp;ldquo;docker pull apache/airflow:2.9.0&amp;rdquo; &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.9.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.9.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Airflow 2.9.0 is also the first release that supports Python 3.12. However, Pendulum 2 does not support Python 3.12, so you‚Äôll need to use &lt;a href=&#34;https://pendulum.eustace.io/blog/announcing-pendulum-3-0-0.html&#34;&gt;Pendulum 3&lt;/a&gt; if you upgrade to Python 3.12.&lt;/p&gt;
&lt;h2 id=&#34;new-data-aware-scheduling-options&#34;&gt;New data-aware scheduling options&lt;/h2&gt;
&lt;h3 id=&#34;logical-operators-and-conditional-expressions-for-dag-scheduling&#34;&gt;Logical operators and conditional expressions for DAG scheduling&lt;/h3&gt;
&lt;p&gt;When Datasets were added in Airflow 2.4, DAGs only had scheduling support for logical AND combinations of Datasets. Simply, you could schedule against more than one Dataset, but a DAG run would only be created once all the Datasets were updated after the last run. Now in Airflow 2.9, we support logical OR and even arbitrary combinations of AND and OR.&lt;/p&gt;
&lt;p&gt;As an example, you can schedule a DAG whenever &lt;code&gt;dataset_1&lt;/code&gt; or &lt;code&gt;dataset_2&lt;/code&gt; are updated :&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;schedule&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataset_2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can have arbitrary combinations:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;schedule&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataset_2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataset_3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can read more about this new functionality in the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/authoring-and-scheduling/datasets.html#advanced-dataset-scheduling-with-conditional-expressions&#34;&gt;data-aware scheduling docs&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;combining-dataset-and-time-based-schedules&#34;&gt;Combining Dataset and Time-Based Schedules&lt;/h3&gt;
&lt;p&gt;Airflow 2.9 comes with a new timetable, &lt;code&gt;DatasetOrTimeSchedule&lt;/code&gt;, that allows you to schedule DAGs based on both dataset events and a timetable. Now you have the best of both worlds.&lt;/p&gt;
&lt;p&gt;For example, to run whenever &lt;code&gt;dataset_1&lt;/code&gt; updates and at midnight UTC:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;schedule&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DatasetOrTimeSchedule&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;timetable&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CronTriggerTimetable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;0 0 * * *&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timezone&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;UTC&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;datasets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag1_dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;dataset-event-rest-api-endpoints&#34;&gt;Dataset Event REST API endpoints&lt;/h3&gt;
&lt;p&gt;New REST API endpoints have been introduced for creating, listing, and deleting dataset events. This makes it possible for external systems to notify Airflow about dataset updates and unlocks management of event queues for more sophisticated use cases.&lt;/p&gt;
&lt;p&gt;See the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/stable-rest-api-ref.html#tag/Dataset&#34;&gt;Dataset API docs&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3 id=&#34;dataset-ui-enhancements&#34;&gt;Dataset UI Enhancements&lt;/h3&gt;
&lt;p&gt;The DAG&amp;rsquo;s graph view has been enhanced to display both the datasets it is scheduled on and those in the task outlets, providing a comprehensive overview of the datasets consumed and produced by the DAG.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;datasets-in-graph.png&#34; alt=&#34;Datasets in the graph view&#34;&gt;&lt;/p&gt;
&lt;p&gt;The main datasets view now allows you to filter for both DAGs and datasets:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dataset-view-filtering.png&#34; alt=&#34;Dataset view filtering&#34;&gt;&lt;/p&gt;
&lt;p&gt;When viewing a Dataset, you can now create a manual dataset event through the UI by clicking the play button shown in the top right here:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;create-manual-dataset-event.png&#34; alt=&#34;Creating manual Dataset event&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;custom-names-for-dynamic-task-mapping&#34;&gt;Custom names for Dynamic Task Mapping&lt;/h2&gt;
&lt;p&gt;Gone are the days of clicking into index numbers and hunting for the dynamically mapped task you wanted to see! This has been a requested feature ever since task mapping was added in Airflow 2.3, and we are happy it‚Äôs finally here.&lt;/p&gt;
&lt;p&gt;You can provide a &lt;code&gt;map_index_template&lt;/code&gt; to mapped operators:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;BashOperator&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;partial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;hello&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;bash_command&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;echo Hello $NAME&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;map_index_template&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{{ task.env[&amp;#39;NAME&amp;#39;] }}&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;env&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;NAME&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;John&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;NAME&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bob&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;NAME&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Fred&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}],&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That template will be rendered after each task finishes running and will populate the name in the UI:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dynamic-task-mapping-custom-names.png&#34; alt=&#34;Dynamic Task Mapping custom names&#34;&gt;&lt;/p&gt;
&lt;p&gt;More details on this, including a taskflow example, is available in the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/authoring-and-scheduling/dynamic-task-mapping.html#named-mapping&#34;&gt;dynamic task mapping docs&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;object-storage-as-xcom-backend&#34;&gt;Object Storage as XCom Backend&lt;/h2&gt;
&lt;p&gt;You can now configure Object Storage to be used as an XCom backend, making it much easier to get XCom results into an object store. Deployment managers can configure the object store of their choice, a size threshold to route some results to the Airflow metadata database and some to the object store, and even a compression method to apply before the data is stored.&lt;/p&gt;
&lt;p&gt;The following configuration will store anything above 1MB in S3 and will compress it using gzip:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[core]
xcom_backend = airflow.providers.common.io.xcom.backend.XComObjectStoreBackend

[common.io]
xcom_objectstorage_path = s3://conn_id@mybucket/key
xcom_objectstorage_threshold = 1048576
xcom_objectstorage_compression = gzip
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;See the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/core-concepts/xcoms.html#object-storage-xcom-backend&#34;&gt;docs on the object storage xcom backend&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2 id=&#34;display-names-for-dags-and-tasks&#34;&gt;Display names for DAGs and Tasks&lt;/h2&gt;
&lt;p&gt;Get your emojis ready! You can now set a display name for dags and tasks, separate from the &lt;code&gt;dag_id&lt;/code&gt; and &lt;code&gt;task_id&lt;/code&gt;. This allows you to have localized display names in the UI, or just use a bunch of emojis.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;dag_display_name&lt;/code&gt; and &lt;code&gt;task_display_name&lt;/code&gt;, you can break away from the ascii handcuffs:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;not_a_fun_dag_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag_display_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;üì£ Best DAG ever üéâ&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;BashOperator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;some_task&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_display_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ü•≥ Fun task!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;display-names.png&#34; alt=&#34;Display names for DAGs and tasks&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;task-log-grouping&#34;&gt;Task log grouping&lt;/h2&gt;
&lt;p&gt;Airflow now has support for arbitrary grouping of task logs.&lt;/p&gt;
&lt;p&gt;By default, pre-execute and post-execute logs are grouped and collapsed, making it easier to see your task logs:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pre-post-logs-grouped.png&#34; alt=&#34;Pre and post execute logs are grouped&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can also use this feature in your task code to make your logs easier to follow:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;big_hello&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;::group::Setup our big Hello&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;greeting&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Hello Airflow 2.9&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;greeting&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Adding &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; to our greeting. Current greeting: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;greeting&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;::endgroup::&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;greeting&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That custom group is collapsed by default:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;custom-log-grouping.png&#34; alt=&#34;Custom log grouping collapsed by default&#34;&gt;&lt;/p&gt;
&lt;p&gt;And it can be expanded if you want to dig into the details:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;custom-log-grouping-expanded.png&#34; alt=&#34;Custom log grouping expanded&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ui-modernization&#34;&gt;UI Modernization&lt;/h2&gt;
&lt;p&gt;In addition to all the UI improvements mentioned above, we have a bunch more improvements in Airflow 2.9!&lt;/p&gt;
&lt;p&gt;The rest of the DAG level views have been moved into React and the grid view interface, allowing for a more cohesive experience. This includes the calendar, task duration, run duration (which replaces landing times), and the audit log. These weren‚Äôt just ‚Äúmoved‚Äù, they each were improved upon as well.&lt;/p&gt;
&lt;p&gt;Here is the new run duration view, which replaces landing times. Users can toggle between landing times and simple run duration:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;run-duration.png&#34; alt=&#34;Run duration&#34;&gt;&lt;/p&gt;
&lt;p&gt;And the new task duration view. Users can toggle queued time on/off and see the median value across the displayed runs:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;task-duration.png&#34; alt=&#34;Task duration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;additional-new-features&#34;&gt;Additional new features&lt;/h2&gt;
&lt;p&gt;Here are just a few interesting new features since there are too many to list in full:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All create/update/delete actions in the REST API are now recorded in the audit log&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/administration-and-deployment/logging-monitoring/callbacks.html#callback-types&#34;&gt;New &lt;code&gt;on_skipped_callback&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/core-concepts/dags.html#dag-auto-pausing-experimental&#34;&gt;Auto pause DAGs after n consecutive failures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for &lt;a href=&#34;https://matomo.org/&#34;&gt;Matomo&lt;/a&gt; as an &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/administration-and-deployment/logging-monitoring/tracking-user-activity.html&#34;&gt;analytics tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.9.0/howto/operator/bash.html&#34;&gt;New &lt;code&gt;@task.bash&lt;/code&gt; TaskFlow decorator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support regex in dag_id for the DAG pause and resume CLI commands&lt;/li&gt;
&lt;li&gt;&lt;code&gt;airflow tasks test&lt;/code&gt; now works with deferrable operators&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contributors&#34;&gt;Contributors&lt;/h2&gt;
&lt;p&gt;Thanks to everyone who contributed to this release, including Amogh Desai, Andrey Anshin, Brent Bovenzi, Daniel Standish, Ephraim Anierobi, Hussein Awala, Jarek Potiuk, Jed Cunningham, Jens Scheffler, Tzu-ping Chung, Vincent Beck, Wei Lee, and over 120 others!&lt;/p&gt;
&lt;p&gt;I‚Äôd especially like to thank our release manager, Ephraim, for getting this release out the door.&lt;/p&gt;
&lt;p&gt;I hope you enjoy using Apache Airflow 2.9.0!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 2.8.0 is here</title>
      <link>/blog/airflow-2.8.0/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.8.0/</guid>
      <description>
        
        
        &lt;p&gt;I am thrilled to announce the release of Apache Airflow 2.8.0, featuring a host of significant enhancements and new features that will greatly benefit our community.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.8.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.8.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.8.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.8.0/&lt;/a&gt; &lt;br&gt;
üõ† Release Notes: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.8.0/release_notes.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.8.0/release_notes.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: &amp;ldquo;docker pull apache/airflow:2.8.0&amp;rdquo; &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.8.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.8.0&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;airflow-object-storage-aip-58&#34;&gt;Airflow Object Storage (AIP-58)&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;This feature is experimental and subject to change.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Airflow now offers a generic abstraction layer over various object stores like S3, GCS, and Azure Blob Storage, enabling the use of different storage systems in DAGs without code modification.&lt;/p&gt;
&lt;p&gt;In addition, it allows you to use most of the standard Python modules, like shutil, that can work with file-like objects.&lt;/p&gt;
&lt;p&gt;Here is an example of how to use the new feature to open a file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.io.path&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ObjectStoragePath&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;base&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ObjectStoragePath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;s3://my-bucket/&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;aws_default&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# conn_id is optional&lt;/span&gt;

&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;read_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ObjectStoragePath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The above example is just the tip of the iceberg. The new feature allows you to configure an alternative backend for a scheme or protocol.&lt;/p&gt;
&lt;p&gt;Here is an example of how to configure a custom backend for the &lt;code&gt;dbfs&lt;/code&gt; scheme:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.io.path&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ObjectStoragePath&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.io.store&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attach&lt;/span&gt;

&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;fsspec.implementations.dbfs&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DBFSFileSystem&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;attach&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;protocol&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dbfs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DBFSFileSystem&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;instance&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;myinstance&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;token&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;mytoken&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;base&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ObjectStoragePath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dbfs://my-location/&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For more information: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/objectstorage.html&#34;&gt;Airflow Object Storage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The support for a specific object storage system depends on the installed providers,
with out-of-the-box support for the file scheme.&lt;/p&gt;
&lt;h2 id=&#34;ship-logs-from-other-components-to-task-logs&#34;&gt;Ship logs from other components to Task logs&lt;/h2&gt;
&lt;p&gt;This feature seamlessly integrates task-related messages from various Airflow components, including the Scheduler and
Executors, into the task logs. This integration allows users to easily track error messages and other relevant
information within a single log view.&lt;/p&gt;
&lt;p&gt;Presently, suppose a task is terminated by the scheduler before initiation, times out due to prolonged queuing, or transitions into a zombie state. In that case, it is not recorded in the task log. With this enhancement, in such situations,
it becomes feasible to dispatch an error message to the task log for convenient visibility on the UI.&lt;/p&gt;
&lt;p&gt;This feature can be toggled, for more information &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#enable-task-context-logger&#34;&gt;see ‚Äúenable_task_context_logger‚Äù in the logging configuration documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;listener-hooks-for-datasets&#34;&gt;Listener hooks for Datasets&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Please note that listeners are still experimental and subject to change.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This feature enables users to subscribe to Dataset creation and update events using listener hooks.
It‚Äôs particularly useful to trigger external processes based on a Dataset being created or updated.&lt;/p&gt;
&lt;h2 id=&#34;using-extra-index-urls-with-pythonvirtualenvoperator-and-caching&#34;&gt;Using Extra Index URLs with PythonVirtualEnvOperator and Caching&lt;/h2&gt;
&lt;p&gt;This feature allows you to specify extra index URLs to PythonVirtualEnvOperator (+corresponding decorator) to be able to install virtualenvs with (private) additional Python package repositories.&lt;/p&gt;
&lt;p&gt;You can also reuse the virtualenvs by caching them in a specified directory and reusing them in subsequent runs. This
can be achieved by setting the &lt;code&gt;venv_cache_path&lt;/code&gt; to a file system folder on your worker&lt;/p&gt;
&lt;p&gt;For more information: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonvirtualenvoperator&#34;&gt;PythonVirtualenvOperator&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;web-ui-improvements&#34;&gt;Web UI improvements&lt;/h1&gt;
&lt;p&gt;There are a number of improvements to the Web UI in this release, including:&lt;/p&gt;
&lt;h2 id=&#34;add-multiselect-to-run-state-in-grid-view&#34;&gt;Add multiselect to run state in grid view:&lt;/h2&gt;
&lt;p&gt;The grid view now supports multiselect for run states. This allows you to select multiple states to filter the dag runs shown in the grid view.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;multiselect-states.png&#34; alt=&#34;Multiselect on the run state&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;improved-visibility-of-task-status-in-the-graph-view&#34;&gt;Improved visibility of task status in the Graph view&lt;/h2&gt;
&lt;p&gt;You can now see the status of a task in the graph view through the border color of the task. This makes it easier to see the status of a task at a glance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;task_status_visibility.png&#34; alt=&#34;Task status visibility&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;raw-html-code-in-dag-docs-and-dag-params-descriptions-is-disabled-by-default&#34;&gt;Raw HTML code in DAG docs and DAG params descriptions is disabled by default&lt;/h2&gt;
&lt;p&gt;As part of our continuous quest to make airflow more secure by default, we have disabled raw HTML code in DAG docs and DAG params descriptions by default.
We care for your security, and &amp;ldquo;secure by default&amp;rdquo; is one of the things we follow strongly.&lt;/p&gt;
&lt;p&gt;Other notable UI improvements include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simplify DAG trigger UI&lt;/li&gt;
&lt;li&gt;Hide logical date and run id in trigger UI form&lt;/li&gt;
&lt;li&gt;Move external logs links to top of react logs page&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional new features and improvements can be found in the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.8.0/release_notes.html#airflow-2-8-0-2023-12-14&#34;&gt;Airflow 2.8.0 release notes&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;contributors&#34;&gt;Contributors&lt;/h1&gt;
&lt;p&gt;Thanks to everyone who contributed to this release, including Amogh Desai, Andrey Anshin, Bolke de Bruin, Daniel DylƒÖg, Daniel Standish, Ephraim Anierobi, Hussein Awala, Jarek Potiuk, Jed Cunningham, Jens Scheffler, mhenc, Miroslav ≈†ediv√Ω, Pankaj Koti, Tzu-ping Chung, Vincent, and everyone else who committed, all 110 of you! You are what makes Airflow the successful project that it is!&lt;/p&gt;
&lt;p&gt;I hope you enjoy using Apache Airflow 2.8.0!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 2.7.0 is here</title>
      <link>/blog/airflow-2.7.0/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.7.0/</guid>
      <description>
        
        
        &lt;p&gt;I‚Äôm happy to announce that Apache Airflow 2.7.0 has been released! Some notable features have been added that we are excited for the community to use.&lt;/p&gt;
&lt;p&gt;Apache Airflow 2.7.0 contains over 500 commits, which include 40 new features, 49 improvements, 53 bug fixes, and 15 documentation changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.7.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.7.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.7.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.7.0/&lt;/a&gt; &lt;br&gt;
üõ† Release Notes: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.7.0/release_notes.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.7.0/release_notes.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: &amp;ldquo;docker pull apache/airflow:2.7.0&amp;rdquo; &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.7.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.7.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Airflow 2.7.0 is a release that focuses on security. The Airflow security team, working together with security researchers, identified a number of areas that required strengthening of security. This resulted in, among others things, an improved description of the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/security/security_model/&#34;&gt;Airflow security model&lt;/a&gt;, a better explanation of our &lt;a href=&#34;https://github.com/apache/airflow/security/policy&#34;&gt;security policy&lt;/a&gt; and the disabling of certain, potentially dangerous, features by default - like, for example, connection testing (#32052).&lt;/p&gt;
&lt;p&gt;Airflow 2.7.0 is also the first release that drops support for end-of-life Python 3.7. This allows Airflow users and maintainers to make use of features and improvements in Python 3.8, and unlocks newer versions of our dependencies.&lt;/p&gt;
&lt;h2 id=&#34;setup-and-teardown-aip-52&#34;&gt;Setup and Teardown (AIP-52)&lt;/h2&gt;
&lt;p&gt;Airflow now has first class support for the concept of setup and teardown tasks. These tasks have special behavior in that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Teardown tasks will still run, no matter what state the upstream tasks end up in&lt;/li&gt;
&lt;li&gt;Teardown tasks failing won‚Äôt, by default, cause the DAG run to fail&lt;/li&gt;
&lt;li&gt;Automatically clear setup/teardown tasks when clearing a dependent task&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can read more about setup and teardown in the &lt;a href=&#34;/blog/introducing_setup_teardown/&#34;&gt;Introducing Setup and Teardown tasks blog post&lt;/a&gt;, or in the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.7.0/howto/setup-and-teardown.html&#34;&gt;setup and teardown docs&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;cluster-activity-ui&#34;&gt;Cluster Activity UI&lt;/h2&gt;
&lt;p&gt;There is a new top level page in Airflow, the Cluster Activity page. This gives an overview of the cluster, including component health, dag and task state counts, and more!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cluster_activity.png&#34; alt=&#34;New cluster activity page&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;graph-and-gantt-views-moved-into-the-grid-view-ui&#34;&gt;Graph and gantt views moved into the Grid view UI&lt;/h2&gt;
&lt;p&gt;The graph and gantt views have been rewritten and moved into the now familiar grid view. This makes it easier to jump between task details, logs, graph, and gantt views without losing your place in a complicated DAG.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph_in_grid.png&#34; alt=&#34;Graph in grid view&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;enable-deferrable-mode-for-all-deferable-tasks-with-1-config-setting&#34;&gt;Enable deferrable mode for all deferable tasks with 1 config setting&lt;/h2&gt;
&lt;p&gt;Airflow 2.7.0 comes with a new config option, &lt;code&gt;default_deferrable&lt;/code&gt;, which allows admins to enable deferrable mode for all deferrable tasks without requiring any DAG modifications. Simply set it in your config and enjoy async tasks!&lt;/p&gt;
&lt;h2 id=&#34;openlineage-built-in-integration&#34;&gt;OpenLineage built-in integration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://openlineage.io/&#34;&gt;OpenLineage&lt;/a&gt; provides a spec standardizing operational lineage collection and distribution across the data ecosystem that projects ‚Äì open source or proprietary ‚Äì implement.&lt;/p&gt;
&lt;p&gt;With 2.7.0, OpenLineage changes from a plugin implementation maintained in the OpenLineage project to a built-in feature of Airflow. As a plugin, OpenLineage depended on Airflow and operators‚Äô internals, making it brittle. Built-in OpenLineage support in Airflow makes publishing operational lineage through the OpenLineage ecosystem easier and more reliable. It has been implemented by moving the &lt;a href=&#34;https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow&#34;&gt;openlineage-airflow&lt;/a&gt; package from the OpenLineage project to an &lt;code&gt;apache-airflow-providers-openlineage&lt;/code&gt; provider in the base Airflow Docker image, where it can be easily enabled by configuration. Also, lineage extraction logic that was included in &lt;a href=&#34;https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/openlineage/airflow/extractors&#34;&gt;Extractors&lt;/a&gt; in that package has been moved into each corresponding provider package along with unit tests, eliminating the need for Extractors in most cases. For this purpose, a new optional API for Operators (&lt;code&gt;get_openlineage_facets_on_{start(), complete(ti), failure(ti)}&lt;/code&gt;, documented &lt;a href=&#34;https://openlineage.io/docs/integrations/airflow/default-extractors&#34;&gt;here&lt;/a&gt;) can be used. Having the extraction logic in each provider ensures the stability of the lineage contract in each operator and makes adding lineage coverage to custom operators easier.&lt;/p&gt;
&lt;h2 id=&#34;some-executors-moved-into-providers&#34;&gt;Some executors moved into providers&lt;/h2&gt;
&lt;p&gt;Some of the executors that were shipped in core Airflow have moved into their respective providers for Airflow 2.7.0. The great benefit of this is to allow faster bug-fix releases as providers are released independently from core.
The following providers have been moved and require certain minimum providers versions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In order to use Celery executors, install the &lt;a href=&#34;https://pypi.org/project/apache-airflow-providers-celery/&#34;&gt;celery provider version 3.3.0+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In order to use the Kubernetes executor, install the &lt;a href=&#34;https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/&#34;&gt;kubernetes provider version 7.4.0+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In order to use the Dask executor, install any version of the &lt;a href=&#34;https://pypi.org/project/apache-airflow-providers-daskexecutor/&#34;&gt;daskexecutor provider&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you use the official docker images, all of these providers come preinstalled.&lt;/p&gt;
&lt;h2 id=&#34;additional-new-features&#34;&gt;Additional new features&lt;/h2&gt;
&lt;p&gt;Here are just a few interesting new features, since there are too many to list in full:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pools can now consider tasks in the deferred state as running (#32709)&lt;/li&gt;
&lt;li&gt;chain_linear, like chain but allowing sequential tasks (#31927)&lt;/li&gt;
&lt;li&gt;Grid view now supports keyboard shortcuts! (#30950)&lt;/li&gt;
&lt;li&gt;Mark task groups as success or failed (#30478)&lt;/li&gt;
&lt;li&gt;Fail_stop, allowing all remaining and running tasks to be failed on the first failure in a DAG (#29406)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contributors&#34;&gt;Contributors&lt;/h2&gt;
&lt;p&gt;Thanks to everyone who contributed to this release, including Akash Sharma, Amogh Desai, Brent Bovenzi, D. Ferruzzi, Daniel Standish, Ephraim Anierobi, Hussein Awala, Jarek Potiuk, Jed Cunningham, Karthikeyan Singaravelan, Maciej Obuchowski, Niko Oliveira, Pankaj Koti, Pankaj Singh, Pierre Jeambrun, Tzu-ping Chung, Utkarsh Sharma, Vincent Beck, and over 74 others!&lt;/p&gt;
&lt;p&gt;I‚Äôd especially like to thank our release manager, Ephraim, for getting this release out the door.&lt;/p&gt;
&lt;p&gt;I hope you enjoy using Apache Airflow 2.7.0!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: what&#39;s new in Apache Airflow 2.6.0</title>
      <link>/blog/airflow-2.6.0/</link>
      <pubDate>Sun, 30 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.6.0/</guid>
      <description>
        
        
        &lt;p&gt;I am excited to announce that Apache Airflow 2.6.0 has been released, bringing many minor features and improvements to the community.&lt;/p&gt;
&lt;p&gt;Apache Airflow 2.6.0 contains over 500 commits, which include 42 new features, 58 improvements, 38 bug fixes, and 17 documentation changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.6.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.6.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.6.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.6.0/&lt;/a&gt; &lt;br&gt;
üõ† Release Notes: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.6.0/release_notes.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.6.0/release_notes.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: &amp;ldquo;docker pull apache/airflow:2.6.0&amp;rdquo; &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.6.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.6.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As the changelog is quite large, the following are some notable new features that shipped in this release.&lt;/p&gt;
&lt;h2 id=&#34;trigger-logs-can-now-be-viewed-in-webserver&#34;&gt;Trigger logs can now be viewed in webserver&lt;/h2&gt;
&lt;p&gt;Trigger logs have now been added to task logs. They appear right alongside the rest of the logs from your task.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;trigger_logging.png&#34; alt=&#34;Trigger logs shown in task log&#34;&gt;&lt;/p&gt;
&lt;p&gt;Adding this feature required changes across the entire Airflow logging stack, so be sure to update your providers if you are using remote logging.&lt;/p&gt;
&lt;h2 id=&#34;grid-view-improvements&#34;&gt;Grid view improvements&lt;/h2&gt;
&lt;p&gt;The grid view has received a number of minor improvements in this release.&lt;/p&gt;
&lt;p&gt;Most notably, there is now a graph tab in the grid view. This offers a more integrated graph representation of the DAG, where choosing a task in either the grid or graph will highlight the same task in both views.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph.png&#34; alt=&#34;The new graph view&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can also filter upstream and downstream from a single task. For example, in the screenshot above, &lt;code&gt;describe_integrity&lt;/code&gt; is the selected task. If you choose to filter downstream, this is the result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;filter_downstream.png&#34; alt=&#34;The new graph view can be filtered to show downstream tasks only&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;trigger-ui-based-on-dag-level-params&#34;&gt;Trigger UI based on DAG level params&lt;/h2&gt;
&lt;p&gt;A user-friendly form is now shown to users triggering runs for DAGs with DAG level params.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;trigger_dag_form.png&#34; alt=&#34;Form shown for params in UI when triggering a DAG&#34;&gt;&lt;/p&gt;
&lt;p&gt;See the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.6.0/core-concepts/params.html#use-params-to-provide-a-trigger-ui-form&#34;&gt;Params docs&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2 id=&#34;consolidation-of-handling-stuck-queued-tasks&#34;&gt;Consolidation of handling stuck queued tasks&lt;/h2&gt;
&lt;p&gt;Airflow now has a single configuration, &lt;code&gt;[scheduler] task_queued_timeout&lt;/code&gt;, to handle tasks that get stuck in queued for too long. With a simpler implementation than the outgoing code handling these tasks, tasks stuck in queued will no longer slip through the cracks and stay stuck.&lt;/p&gt;
&lt;p&gt;For more details, see the &lt;a href=&#34;https://medium.com/apache-airflow/unsticking-airflow-stuck-queued-tasks-are-no-more-in-2-6-0-6f40a1a22835&#34;&gt;Unsticking Airflow: Stuck Queued Tasks are No More in 2.6.0&lt;/a&gt; Medium post.&lt;/p&gt;
&lt;h2 id=&#34;cluster-policy-hooks-can-come-from-plugins&#34;&gt;Cluster Policy hooks can come from plugins&lt;/h2&gt;
&lt;p&gt;Cluster policy hooks (e.g. &lt;code&gt;dag_policy&lt;/code&gt;), can now come from Airflow plugins in addition to Airflow local settings. By allowing multiple hooks to be defined, it makes it easier for more than one team to run hooks in a single Airflow instance.&lt;/p&gt;
&lt;p&gt;See the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.6.0/administration-and-deployment/cluster-policies.html&#34;&gt;cluster policy docs&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2 id=&#34;notification-support-added&#34;&gt;Notification support added&lt;/h2&gt;
&lt;p&gt;The notifications framework allows you to send messages to external systems when a task instance/DAG run changes state. For example, you can easily post a message to Slack&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;err&#34;&gt;‚Äú&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;slack_notifier_example&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;‚Äù&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;start_date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2023&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;on_success_callback&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;send_slack_notification&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;The DAG {{ dag.dag_id }} succeeded&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;channel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;#general&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;username&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Airflow&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As of today, Slack is the only system supported out of the box. However, watch this space as more integrations will be added soon.&lt;/p&gt;
&lt;p&gt;You can also create notifiers for your own use, refer to the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.6.0/howto/notifications.html&#34;&gt;notifier how-to docs&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2 id=&#34;thanks-to-the-contributors&#34;&gt;Thanks to the contributors&lt;/h2&gt;
&lt;p&gt;Thanks to everyone who contributed to this release, including Andrey Anshin, Ash Berlin-Taylor, Brent Bovenzi, Daniel Standish, Ephraim Anierobi, Hussein Awala, Jarek Potiuk, Jed Cunningham, Josh Fell, Michael Petro, Niko Oliveira, Pierre Jeambrun, Tzu-ping Chung, Victor Chiapaikeo, and over 120 others!&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d especially like to thank our release manager, Ephraim, for getting this release out the door.&lt;/p&gt;
&lt;p&gt;I hope you enjoy using Apache Airflow 2.6.0!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 2.5.0: Tick-Tock</title>
      <link>/blog/airflow-2.5.0/</link>
      <pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.5.0/</guid>
      <description>
        
        
        &lt;p&gt;Apache Airfow 2.5 has just been released, barely two and a half months after 2.4!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.5.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.5.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.5.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.5.0/&lt;/a&gt; &lt;br&gt;
üõ†Ô∏è Release Notes: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.5.0/release_notes.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.5.0/release_notes.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: docker pull apache/airflow:2.5.0 &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.5.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.5.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This quicker release cadence is a departure from our previous habit of releasing every five-to-seven months and was a deliberate effort to listen to you, our users, and get the changes and improvements into your workflows earlier.&lt;/p&gt;
&lt;h2 id=&#34;usability-improvements-to-the-datasets-ui&#34;&gt;Usability improvements to the Datasets UI&lt;/h2&gt;
&lt;p&gt;When we released Dataset aware scheduling in September we knew that the tools we gave to manage the Datasets were very much a Minimum Viable Product, and in the last two months the committers and contributors have been hard at work at making the UI much more usable when it comes to Datasets.&lt;/p&gt;
&lt;p&gt;But we we aren&amp;rsquo;t done yet - keep an eye out for more improvements coming over the next couple of releases too.&lt;/p&gt;
&lt;h2 id=&#34;greatly-improved-airflow-dags-test-command&#34;&gt;Greatly improved &lt;code&gt;airflow dags test&lt;/code&gt; command&lt;/h2&gt;
&lt;p&gt;This airflow subcommand has been rethought and re-optimized to make it much easier to test your DAGs locally - the major changes are:&lt;/p&gt;
&lt;p&gt;a. Task logs are visible right there in the console, instead of hidden away inside the task log files
b. It is about an order of magnitude quicker to run the tasks than before (i.e. it gets to running the task code so much quicker)
c. Everything runs in one process, so you can put a breakpoint in your IDE, and configure it to run &lt;code&gt;airflow dags test &amp;lt;mydag&amp;gt;&lt;/code&gt; then debug code!&lt;/p&gt;
&lt;h2 id=&#34;auto-tailing-task-logs-in-the-grid-view&#34;&gt;Auto tailing task logs in the Grid view&lt;/h2&gt;
&lt;p&gt;Hopefully the headline says enough. It&amp;rsquo;s lovely, go check it out.&lt;/p&gt;
&lt;h2 id=&#34;more-improvments-to-dynamic-task-mapping&#34;&gt;More improvments to Dynamic-Task mapping&lt;/h2&gt;
&lt;p&gt;In a similar vein to the improvements to the Dataset (UI), we have continued to iterate on and improve the feature we first added in Airflow 2.3, Dynamic Task Mapping, and 2.5 includes &lt;a href=&#34;https://github.com/apache/airflow/pulls?q=is%3Apr+author%3Auranusjr+is%3Aclosed+milestone%3A%22Airflow+2.5.0%22&#34;&gt;dozens of improvements&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;thanks-to-the-contributors&#34;&gt;Thanks to the contributors&lt;/h2&gt;
&lt;p&gt;Andrey Anshin, Ash Berlin-Taylor, blag, Bolke de Bruin, Brent Bovenzi, Chenglong Yan, Daniel Standish, Dov Benyomin Sohacheski, Elad Kalif, Ephraim Anierobi, Jarek Potiuk, Jed Cunningham, Jorrick Sleijster, Michael Petro, Niko, Pierre Jeambrun, Tzu-ping Chung and many more, over 75 of you. Thank you!&lt;/p&gt;
&lt;p&gt;And a special thank you to Ephraim who tirelessly worked behind the scenes as release manager!&lt;/p&gt;
&lt;p&gt;A much shorter change log than 2.4, but I think you&amp;rsquo;ll agree, some great changes.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 2.4.0: That Data Aware Release</title>
      <link>/blog/airflow-2.4.0/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.4.0/</guid>
      <description>
        
        
        &lt;p&gt;Apache Airflow 2.4.0 contains over 650 &amp;ldquo;user-facing&amp;rdquo; commits (excluding commits to providers or chart) and over 870 total. That includes 46 new features, 39 improvements, 52 bug fixes, and several documentation changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.4.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.4.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.4.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.4.0/&lt;/a&gt; &lt;br&gt;
üõ†Ô∏è Release Notes: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.4.0/release_notes.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.4.0/release_notes.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: docker pull apache/airflow:2.4.0 &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.4.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.4.0&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;data-aware-scheduling-aip-48&#34;&gt;Data-aware scheduling (AIP-48)&lt;/h2&gt;
&lt;p&gt;This one is big. Airflow now has the ability to schedule DAGs based on other tasks updating datasets.&lt;/p&gt;
&lt;p&gt;What does this mean, exactly? This is a great new feature that lets DAG authors create smaller, more self-contained DAGs, which chain together into a larger data-based workflow. If you are currently using &lt;code&gt;ExternalTaskSensor&lt;/code&gt; or &lt;code&gt;TriggerDagRunOperator&lt;/code&gt; you should take a look at datasets &amp;ndash; in most cases you can replace them with something that will speed up the scheduling!&lt;/p&gt;
&lt;p&gt;But enough talking, lets have a short example. First lets write a simple DAG with a task called &lt;code&gt;my_task&lt;/code&gt; that produces a dataset called &lt;code&gt;my-dataset&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uri&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;my-dataset&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;producer&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;outlets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;my_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
        &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Datasets are defined by a URI. Now, we can create a second DAG (&lt;code&gt;consumer&lt;/code&gt;) that gets scheduled whenever this dataset changes:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uri&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;my-dataset&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;dataset-consumer&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schedule&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]):&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With these two DAGs, the instant &lt;code&gt;my_task&lt;/code&gt; finishes, Airflow will create the DAG run for the &lt;code&gt;dataset-consumer&lt;/code&gt; workflow.&lt;/p&gt;
&lt;p&gt;We know that what exists right now won&amp;rsquo;t fit all use cases that people might wish for datasets, and in the coming minor releases (2.5, 2.6, etc.) we will expand and improve upon this foundation.&lt;/p&gt;
&lt;p&gt;Datasets represent the abstract concept of a dataset, and (for now) do not have any direct read or write capability - in this release we are adding the foundational feature that we will build upon in the future - and it&amp;rsquo;s part of our goal to have smaller releases to get new features in your hands sooner!&lt;/p&gt;
&lt;p&gt;For more information on datasets, see the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.4.0/concepts/datasets.html&#34;&gt;documentation on Data-aware scheduling&lt;/a&gt;. That includes details on how datasets are identified (URIs), how you can depend on multiple datasets, and how to think about what a dataset is (hint: don&amp;rsquo;t include &amp;ldquo;date partitions&amp;rdquo; in a dataset, it&amp;rsquo;s higher level than that).&lt;/p&gt;
&lt;h2 id=&#34;easier-management-of-conflicting-python-dependencies-using-the-new-externalpythonoperator&#34;&gt;Easier management of conflicting python dependencies using the new ExternalPythonOperator&lt;/h2&gt;
&lt;p&gt;As much as we wish all python libraries could be used happily together that sadly isn&amp;rsquo;t the world we live in, and sometimes there are conflicts when trying to install multiple python libraries in an Airflow install &amp;ndash; right now we hear this a lot with &lt;code&gt;dbt-core&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To make this easier we have introduced &lt;code&gt;@task.external_python&lt;/code&gt; (and the matching &lt;code&gt;ExternalPythonOperator&lt;/code&gt;) that lets you run an python function as an Airflow task in a pre-configured virtual env, or even a whole different python version. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;external_python&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;python&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;/opt/venvs/task_deps/bin/python&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;my_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_interval_start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_interval_env&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Looking at data between &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_interval_start&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; and &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_interval_end&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are a few subtlties as to what you need installed in the virtual env depending on which context variables you access, so be sure to read the &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/2.4.0/howto/operator/python.html#externalpythonoperator&#34;&gt;how-to on using the ExternalPythonOperator&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;more-improvements-to-dynamic-task-mapping-aip-42&#34;&gt;More improvements to Dynamic Task Mapping (AIP-42)&lt;/h2&gt;
&lt;p&gt;You asked, we listened. Dynamic task mapping now includes support for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;expand_kwargs&lt;/code&gt;: To assign multiple parameters to a non-TaskFlow operator.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;zip&lt;/code&gt;: To combine multiple things without cross-product.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;map&lt;/code&gt;: To transform the parameters just before the task is run.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on dynamic task mapping, see the new sections of the doc on &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.4.0/concepts/dynamic-task-mapping.html#transforming-mapped-data&#34;&gt;Transforming Mapped Data&lt;/a&gt;, &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.4.0/concepts/dynamic-task-mapping.html#combining-upstream-data-aka-zipping&#34;&gt;Combining upstream data (aka &amp;ldquo;zipping&amp;rdquo;)&lt;/a&gt;, and &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.4.0/concepts/dynamic-task-mapping.html#assigning-multiple-parameters-to-a-non-taskflow-operator&#34;&gt;Assigning multiple parameters to a non-TaskFlow operator&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;auto-register-dags-used-in-a-context-manager-no-more-as-dag-needed&#34;&gt;Auto-register DAGs used in a context manager (no more &lt;code&gt;as dag:&lt;/code&gt; needed)&lt;/h2&gt;
&lt;p&gt;This one is a small quality of life improvement, and I don&amp;rsquo;t want to admit how many times I forgot the &lt;code&gt;as dag:&lt;/code&gt;, or worse, had &lt;code&gt;as dag:&lt;/code&gt; repeated.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;example&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
  &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;


&lt;span class=&#34;nd&#34;&gt;@dag&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;dag_maker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;dag2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag_maker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;can become&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;example&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;


&lt;span class=&#34;nd&#34;&gt;@dag&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;my_dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;my_dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you want to disable the behaviour for any reason, set &lt;code&gt;auto_register=False&lt;/code&gt; on the DAG:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# This dag will not be picked up by Airflow as it&amp;#39;s not assigned to a variable&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;example&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;auto_register&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;additional-improvements&#34;&gt;Additional improvements&lt;/h2&gt;
&lt;p&gt;With over 650 commits the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.4.0/release_notes.html#airflow-2-4-0-2022-09-19&#34;&gt;full list of features, fixes and changes&lt;/a&gt; is too big to go in to here (check out the release notes for a full list), but some noteworthy or interesting small features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Auto-refresh on the home page&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;@task.short_circuit&lt;/code&gt; TaskFlow decorator&lt;/li&gt;
&lt;li&gt;Add roles delete command to cli&lt;/li&gt;
&lt;li&gt;Add support for &lt;code&gt;TaskGroup&lt;/code&gt; in &lt;code&gt;ExternalTaskSensor&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;@task.kubernetes&lt;/code&gt; taskflow decorator&lt;/li&gt;
&lt;li&gt;Add experimental &lt;code&gt;parsing_context&lt;/code&gt; to enable optimization of Dynamic DAG handling in workers&lt;/li&gt;
&lt;li&gt;Consolidate to one &lt;code&gt;schedule&lt;/code&gt; param&lt;/li&gt;
&lt;li&gt;Allow showing non-sensitive config values in Admin -&amp;gt; Configuration (rather than all or nothing)&lt;/li&gt;
&lt;li&gt;Operator name separate from class (no more &lt;code&gt;_PythonDecoratedOperator&lt;/code&gt; when using TaskFlow)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contributors&#34;&gt;Contributors&lt;/h2&gt;
&lt;p&gt;Thanks to everyone who contributed to this release, including Andrey Anshin, Ash Berlin-Taylor, Bart≈Çomiej Hirsz, Brent Bovenzi, Chenglong Yan, D. Ferruzzi, Daniel Standish, Drew Hubl, Elad Kalif, Ephraim Anierobi, Jarek Potiuk, Jed Cunningham, Josh Fell, Mark Norman Francis, Niko, Tzu-ping Chung, Vincent, Wojciech Januszek, chethanuk-plutoflume, pierrejeambrun, and everyone else who committed, all 152 of you! You are what makes Airflow the successful project that it is!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 2.3.0 is here</title>
      <link>/blog/airflow-2.3.0/</link>
      <pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.3.0/</guid>
      <description>
        
        
        &lt;p&gt;Apache Airflow 2.3.0 contains over 700 commits since 2.2.0 and includes 50 new features, 99 improvements, 85 bug fixes, and several doc changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.3.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.3.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.3.0/&lt;/a&gt; &lt;br&gt;
üõ†Ô∏è Release Notes: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/release_notes.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.3.0/release_notes.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: docker pull apache/airflow:2.3.0 &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.3.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.3.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As the changelog is quite large, the following are some notable new features that shipped in this release.&lt;/p&gt;
&lt;h2 id=&#34;dynamic-task-mappingaip-42&#34;&gt;Dynamic Task Mapping(AIP-42)&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s now first-class support for dynamic tasks in Airflow. What this means is that you can generate tasks dynamically at runtime. Much like using a &lt;code&gt;for&lt;/code&gt; loop
to create a list of tasks, here you can create the same tasks without having to know the exact number of tasks ahead of time.&lt;/p&gt;
&lt;p&gt;You can have a &lt;code&gt;task&lt;/code&gt; generate the list to iterate over, which is not possible with a &lt;code&gt;for&lt;/code&gt; loop.&lt;/p&gt;
&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;make_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# This can also be from an API call, checking a database, -- almost anything you like, as long as the&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# resulting list/dictionary can be stored in the current XCom backend.&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;b&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;str&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;


&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;consumer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dynamic-map&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start_date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2022&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;consumer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;More information can be found here: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html&#34;&gt;Dynamic Task Mapping&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;grid-view-replaces-tree-view&#34;&gt;Grid View replaces Tree View&lt;/h2&gt;
&lt;p&gt;Grid view replaces tree view in Airflow 2.3.0.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Screenshots&lt;/strong&gt;:
&lt;img src=&#34;grid-view.png&#34; alt=&#34;The new grid view&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;purge-history-from-metadata-database&#34;&gt;Purge history from metadata database&lt;/h2&gt;
&lt;p&gt;Airflow 2.3.0 introduces a new &lt;code&gt;airflow db clean&lt;/code&gt; command that can be used to purge old data from the metadata database.&lt;/p&gt;
&lt;p&gt;You would want to use this command if you want to reduce the size of the metadata database.&lt;/p&gt;
&lt;p&gt;More information can be found here: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#purge-history-from-metadata-database&#34;&gt;Purge history from metadata database&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;localkubernetesexecutor&#34;&gt;LocalKubernetesExecutor&lt;/h2&gt;
&lt;p&gt;There is a new executor named LocalKubernetesExecutor. This executor helps you run some tasks using LocalExecutor and run another set of tasks using the KubernetesExecutor in the same deployment based on the task&amp;rsquo;s queue.&lt;/p&gt;
&lt;p&gt;More information can be found here: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/executor/local_kubernetes.html&#34;&gt;LocalKubernetesExecutor&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;dagprocessormanager-as-standalone-process-aip-43&#34;&gt;DagProcessorManager as standalone process (AIP-43)&lt;/h2&gt;
&lt;p&gt;As of 2.3.0, you can run the DagProcessorManager as a standalone process. Because DagProcessorManager runs user code, separating it from the scheduler process and running it as an independent process in a different host is a good idea.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;airflow dag-processor&lt;/code&gt; cli command will start a new process that will run the DagProcessorManager in a separate process. Before you can run the DagProcessorManager as a standalone process, you need to set the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#standalone_dag_processor&#34;&gt;[scheduler] standalone_dag_processor&lt;/a&gt; to &lt;code&gt;True&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;More information can be found here: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/cli-and-env-variables-ref.html#dag-processor&#34;&gt;dag-processor CLI command&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;json-serialization-for-connections&#34;&gt;JSON serialization for connections&lt;/h2&gt;
&lt;p&gt;You can now create connections using the &lt;code&gt;json&lt;/code&gt; serialization format.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;airflow connections add &lt;span class=&#34;s1&#34;&gt;&amp;#39;my_prod_db&amp;#39;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --conn-json &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;conn_type&amp;#34;: &amp;#34;my-conn-type&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;login&amp;#34;: &amp;#34;my-login&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;password&amp;#34;: &amp;#34;my-password&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;host&amp;#34;: &amp;#34;my-host&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;port&amp;#34;: 1234,
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;schema&amp;#34;: &amp;#34;my-schema&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;extra&amp;#34;: {
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;            &amp;#34;param1&amp;#34;: &amp;#34;val1&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;            &amp;#34;param2&amp;#34;: &amp;#34;val2&amp;#34;
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        }
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;    }&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can also use &lt;code&gt;json&lt;/code&gt; serialization format when setting the connection in environment variables.&lt;/p&gt;
&lt;p&gt;More information can be found here: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/howto/connection.html&#34;&gt;JSON serialization for connections&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;airflow-db-downgrade-and-offline-generation-of-sql-scripts&#34;&gt;Airflow &lt;code&gt;db downgrade&lt;/code&gt; and Offline generation of SQL scripts&lt;/h2&gt;
&lt;p&gt;Airflow 2.3.0 introduced a new command &lt;code&gt;airflow db downgrade&lt;/code&gt; that will downgrade the database to your chosen version.&lt;/p&gt;
&lt;p&gt;You can also generate the downgrade/upgrade SQL scripts for your database and manually run it against your database or just view the SQL queries that would be run by the downgrade/upgrade command.&lt;/p&gt;
&lt;p&gt;More information can be found here: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow&#34;&gt;Airflow &lt;code&gt;db downgrade&lt;/code&gt; and Offline generation of SQL scripts&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reuse-of-decorated-tasks&#34;&gt;Reuse of decorated tasks&lt;/h2&gt;
&lt;p&gt;You can now reuse decorated tasks across your dag files. A decorated task has an &lt;code&gt;override&lt;/code&gt; method that allows you to override it&amp;rsquo;s arguments.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;add_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Task args: x=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, y=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;


&lt;span class=&#34;nd&#34;&gt;@dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start_date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2022&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;mydag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;add_task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;override&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;start&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;add_task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;override&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;add_start_&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;More information can be found here: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.3.0/tutorial_taskflow_api.html#reusing-a-decorated-task&#34;&gt;Reuse of decorated DAGs&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;other-small-features&#34;&gt;Other small features&lt;/h2&gt;
&lt;p&gt;This isn‚Äôt a comprehensive list, but some noteworthy or interesting small features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support different timeout value for dag file parsing&lt;/li&gt;
&lt;li&gt;&lt;code&gt;airflow dags reserialize&lt;/code&gt; command to reserialize dags&lt;/li&gt;
&lt;li&gt;Events Timetable&lt;/li&gt;
&lt;li&gt;SmoothOperator - Operator that does literally nothing except logging a YouTube link to
Sade&amp;rsquo;s &amp;ldquo;Smooth Operator&amp;rdquo;. Enjoy!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contributors&#34;&gt;Contributors&lt;/h2&gt;
&lt;p&gt;Thanks to everyone who contributed to this release: Ash Berlin-Taylor, Brent Bovenzi, Daniel Standish, Elad, Ephraim Anierobi, Jarek Potiuk, Jed Cunningham, Josh Fell, Kamil Bregu≈Ça, Kanthi, Kaxil Naik, Khalid Mammadov, Malthe Borch, Ping Zhang, Tzu-ping Chung and many others who keep making Airflow better for everyone.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: What&#39;s new in Apache Airflow 2.2.0</title>
      <link>/blog/airflow-2.2.0/</link>
      <pubDate>Mon, 11 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-2.2.0/</guid>
      <description>
        
        
        &lt;p&gt;I‚Äôm proud to announce that Apache Airflow 2.2.0 has been released. It contains over 600 commits since 2.1.4 and includes 30 new features, 84 improvements, 85 bug fixes, and many internal and doc changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;üì¶ PyPI: &lt;a href=&#34;https://pypi.org/project/apache-airflow/2.2.0/&#34;&gt;https://pypi.org/project/apache-airflow/2.2.0/&lt;/a&gt; &lt;br&gt;
üìö Docs: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.2.0/&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.2.0/&lt;/a&gt; &lt;br&gt;
üõ†Ô∏è Changelog: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/2.2.0/changelog.html&#34;&gt;https://airflow.apache.org/docs/apache-airflow/2.2.0/changelog.html&lt;/a&gt; &lt;br&gt;
üê≥ Docker Image: docker pull apache/airflow:2.2.0 &lt;br&gt;
üöè Constraints: &lt;a href=&#34;https://github.com/apache/airflow/tree/constraints-2.2.0&#34;&gt;https://github.com/apache/airflow/tree/constraints-2.2.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As the changelog is quite large, the following are some notable new features that shipped in this release.&lt;/p&gt;
&lt;h2 id=&#34;custom-timetables-aip-39&#34;&gt;Custom Timetables (AIP-39)&lt;/h2&gt;
&lt;p&gt;Airflow has historically used cron expressions and timedeltas to represent when a DAG should run. This worked for a lot of use cases, but not all. For example, running daily on Monday-Friday, but not on weekends wasn‚Äôt possible.&lt;/p&gt;
&lt;p&gt;To provide more scheduling flexibility, determining when a DAG should run is now done with Timetables. Of course, backwards compatibility has been maintained - cron expressions and timedeltas are still fully supported, however, timetables are pluggable so you can add your own custom timetable to fit your needs! For example, you could write a timetable to schedule a DagRun&lt;/p&gt;
&lt;p&gt;&lt;code&gt;execution_date&lt;/code&gt; has long been confusing to new Airflowers, so as part of this change a new concept has been added to Airflow to replace it named &lt;code&gt;data_interval&lt;/code&gt;, which is the period of data that a task should operate on. The following are now available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;logical_date&lt;/code&gt; (aka &lt;code&gt;execution_date&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data_interval_start&lt;/code&gt; (same value as &lt;code&gt;execution_date&lt;/code&gt; for cron)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data_interval_end&lt;/code&gt; (aka &lt;code&gt;next_execution_date&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you write your own timetables, keep in mind they should be idempotent and fast as they are used in the scheduler to create DagRuns.&lt;/p&gt;
&lt;p&gt;More information can be found at: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/howto/timetable.html&#34;&gt;Customizing DAG Scheduling with Timetables&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;deferrable-tasks-aip-40&#34;&gt;Deferrable Tasks (AIP-40)&lt;/h2&gt;
&lt;p&gt;Deferrable tasks allows operators or sensors to defer themselves until a light-weight async check passes, at which point they can resume executing. Most importantly, this results in the worker slot, and most notably any resources used by it, to be returned to Airflow. This allows simple things like monitoring a job in an external system or watching for an event to be much cheaper.&lt;/p&gt;
&lt;p&gt;To support this feature, a new component has been added to Airflow, the triggerer, which is the daemon process that runs the asyncio event loop.&lt;/p&gt;
&lt;p&gt;Airflow 2.2.0 ships with 2 deferrable sensors, &lt;code&gt;DateTimeSensorAsync&lt;/code&gt; and &lt;code&gt;TimeDeltaSensorAsync&lt;/code&gt;, both of which are drop-in replacements for the existing corresponding sensor.&lt;/p&gt;
&lt;p&gt;More information can be found at:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/concepts/deferring.html&#34;&gt;Deferrable Operators &amp;amp; Triggers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;custom-task-decorators-and-taskdocker&#34;&gt;Custom &lt;code&gt;@task&lt;/code&gt; decorators and &lt;code&gt;@task.docker&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Airflow 2.2.0 allows providers to create custom &lt;code&gt;@task&lt;/code&gt; decorators in the TaskFlow interface.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;@task.docker&lt;/code&gt; decorator is one such decorator that allows you to run a function in a docker container. Airflow handles getting the code into the container and returning xcom - you just worry about your function. This is particularly useful when you have conflicting dependencies between Airflow itself and tasks you need to run.&lt;/p&gt;
&lt;p&gt;More information on creating custom &lt;code&gt;@task&lt;/code&gt; decorators can be found at: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/howto/create-custom-decorator.html&#34;&gt;Creating Custom @task Decorators&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More information on the &lt;code&gt;@task.docker&lt;/code&gt; decorator can be found at: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html#using-the-taskflow-api-with-docker-or-virtual-environments&#34;&gt;Using the Taskflow API with Docker or Virtual Environments&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;validation-of-dag-params&#34;&gt;Validation of DAG params&lt;/h2&gt;
&lt;p&gt;You can now apply validation on DAG params by passing a &lt;code&gt;Param&lt;/code&gt; object for each param. The &lt;code&gt;Param&lt;/code&gt; object supports the full &lt;a href=&#34;https://json-schema.org/draft/2020-12/json-schema-validation.html&#34;&gt;json-schema validation specifications&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Currently this only functions with manually triggered DAGs, but it does set the stage for future params related functionality.&lt;/p&gt;
&lt;p&gt;More information can be found at: &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/concepts/params.html&#34;&gt;Params&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;other-small-features&#34;&gt;Other small features&lt;/h2&gt;
&lt;p&gt;This isn‚Äôt a comprehensive list, but some noteworthy or interesting small features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Testing Connections from the UI - test the credentials for your Connection actually work&lt;/li&gt;
&lt;li&gt;Duplication Connections from the UI&lt;/li&gt;
&lt;li&gt;DAGs ‚ÄúNext run‚Äù info is shown in the UI, including when the run will actually start&lt;/li&gt;
&lt;li&gt;&lt;code&gt;airflow standalone&lt;/code&gt; command runs all of the Airflow components directly without docker - great for local development&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contributors&#34;&gt;Contributors&lt;/h2&gt;
&lt;p&gt;Thanks to everyone who contributed to this release: Andrew Godwin, Ash Berlin-Taylor, Brent Bovenzi, Elad Kalif, Ephraim Anierobi, James Timmins, Jarek Potiuk, Jed Cunningham, Josh Fell, Kamil Bregu≈Ça, Kaxil Naik, Malthe Borch, Sam Wheating, Sumit Maheshwari, Tzu-ping Chung and many others&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 2.0 is here!</title>
      <link>/blog/airflow-two-point-oh-is-here/</link>
      <pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-two-point-oh-is-here/</guid>
      <description>
        
        
        &lt;p&gt;I am proud to announce that Apache Airflow 2.0.0 has been released.&lt;/p&gt;
&lt;p&gt;The full changelog is about 3,000 lines long (already excluding everything backported to 1.10), so for now I&amp;rsquo;ll simply share some of the major features in 2.0.0 compared to 1.10.14:&lt;/p&gt;
&lt;h2 id=&#34;a-new-way-of-writing-dags-the-taskflow-api-aip-31&#34;&gt;A new way of writing dags: the TaskFlow API (AIP-31)&lt;/h2&gt;
&lt;p&gt;(Known in 2.0.0alphas as Functional DAGs.)&lt;/p&gt;
&lt;p&gt;DAGs are now much much nicer to author especially when using PythonOperator. Dependencies are handled more clearly and XCom is nicer to use&lt;/p&gt;
&lt;p&gt;Read more here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html&#34;&gt;TaskFlow API Tutorial&lt;/a&gt; &lt;br&gt;
&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#decorated-flows&#34;&gt;TaskFlow API Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A quick teaser of what DAGs can now look like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.decorators&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.utils.dates&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;days_ago&lt;/span&gt;

&lt;span class=&#34;nd&#34;&gt;@dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;airflow&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schedule_interval&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start_date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;days_ago&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;tutorial_taskflow_api_etl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
   &lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;
   &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;extract&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
       &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;1001&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;301.27&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;1002&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;433.21&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;1003&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;502.22&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

   &lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;
   &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;order_data_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
       &lt;span class=&#34;n&#34;&gt;total_order_value&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;

       &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;order_data_dict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
           &lt;span class=&#34;n&#34;&gt;total_order_value&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;

       &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;total_order_value&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;total_order_value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

   &lt;span class=&#34;nd&#34;&gt;@task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
   &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;total_order_value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;

       &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Total order value is: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%.2f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;total_order_value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

   &lt;span class=&#34;n&#34;&gt;order_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;extract&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
   &lt;span class=&#34;n&#34;&gt;order_summary&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;order_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
   &lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;order_summary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;total_order_value&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;tutorial_etl_dag&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tutorial_taskflow_api_etl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;fully-specified-rest-api-aip-32&#34;&gt;Fully specified REST API (AIP-32)&lt;/h2&gt;
&lt;p&gt;We now have a fully supported, no-longer-experimental API with a comprehensive OpenAPI specification&lt;/p&gt;
&lt;p&gt;Read more here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html&#34;&gt;REST API Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;massive-scheduler-performance-improvements&#34;&gt;Massive Scheduler performance improvements&lt;/h2&gt;
&lt;p&gt;As part of AIP-15 (Scheduler HA+performance) and other work Kamil did, we significantly improved the performance of the Airflow Scheduler. It now starts tasks much, MUCH quicker.&lt;/p&gt;
&lt;p&gt;Over at Astronomer.io we&amp;rsquo;ve &lt;a href=&#34;https://www.astronomer.io/blog/airflow-2-scheduler&#34;&gt;benchmarked the scheduler‚Äîit&amp;rsquo;s fast&lt;/a&gt; (we had to triple check the numbers as we don&amp;rsquo;t quite believe them at first!)&lt;/p&gt;
&lt;h2 id=&#34;scheduler-is-now-ha-compatible-aip-15&#34;&gt;Scheduler is now HA compatible (AIP-15)&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s now possible and supported to run more than a single scheduler instance. This is super useful for both resiliency (in case a scheduler goes down) and scheduling performance.&lt;/p&gt;
&lt;p&gt;To fully use this feature you need Postgres 9.6+ or MySQL 8+ (MySQL 5, and MariaDB won&amp;rsquo;t work with more than one scheduler I&amp;rsquo;m afraid).&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s no config or other set up required to run more than one scheduler‚Äîjust start up a scheduler somewhere else (ensuring it has access to the DAG files) and it will cooperate with your existing schedulers through the database.&lt;/p&gt;
&lt;p&gt;For more information, read the &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/scheduler.html#running-more-than-one-scheduler&#34;&gt;Scheduler HA documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;task-groups-aip-34&#34;&gt;Task Groups (AIP-34)&lt;/h2&gt;
&lt;p&gt;SubDAGs were commonly used for grouping tasks in the UI, but they had many drawbacks in their execution behaviour (primarily that they only executed a single task in parallel!) To improve this experience, we‚Äôve introduced &amp;ldquo;Task Groups&amp;rdquo;: a method for organizing tasks which provides the same grouping behaviour as a subdag without any of the execution-time drawbacks.&lt;/p&gt;
&lt;p&gt;SubDAGs will still work for now, but we think that any previous use of SubDAGs can now be replaced with task groups. If you find an example where this isn&amp;rsquo;t the case, please let us know by opening an issue on GitHub&lt;/p&gt;
&lt;p&gt;For more information, check out the &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/concepts.html#taskgroup&#34;&gt;Task Group documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;refreshed-ui&#34;&gt;Refreshed UI&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ve given the Airflow UI &lt;a href=&#34;https://github.com/apache/airflow/pull/11195&#34;&gt;a visual refresh&lt;/a&gt; and updated some of the styling.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;airflow-2.0-ui.gif&#34; alt=&#34;Airflow 2.0&amp;rsquo;s new UI&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have also added an option to auto-refresh task states in Graph View so you no longer need to continuously press the refresh button :).&lt;/p&gt;
&lt;p&gt;Check out &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/ui.html&#34;&gt;the screenshots in the docs&lt;/a&gt; for more.&lt;/p&gt;
&lt;h2 id=&#34;smart-sensors-for-reduced-load-from-sensors-aip-17&#34;&gt;Smart Sensors for reduced load from sensors (AIP-17)&lt;/h2&gt;
&lt;p&gt;If you make heavy use of sensors in your Airflow cluster, you might find that sensor execution takes up a significant proportion of your cluster even with &amp;ldquo;reschedule&amp;rdquo; mode. To improve this, we&amp;rsquo;ve added a new mode called &amp;ldquo;Smart Sensors&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This feature is in &amp;ldquo;early-access&amp;rdquo;: it&amp;rsquo;s been well-tested by Airbnb and is &amp;ldquo;stable&amp;rdquo;/usable, but we reserve the right to make backwards incompatible changes to it in a future release (if we have to. We&amp;rsquo;ll try very hard not to!)&lt;/p&gt;
&lt;p&gt;Read more about it in the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/smart-sensor.html&#34;&gt;Smart Sensors documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;simplified-kubernetesexecutor&#34;&gt;Simplified KubernetesExecutor&lt;/h2&gt;
&lt;p&gt;For Airflow 2.0, we have re-architected the KubernetesExecutor in a fashion that is simultaneously faster, easier to understand, and more flexible for Airflow users. Users will now be able to access the full Kubernetes API to create a .yaml &lt;code&gt;pod_template_file&lt;/code&gt; instead of specifying parameters in their airflow.cfg.&lt;/p&gt;
&lt;p&gt;We have also replaced the &lt;code&gt;executor_config&lt;/code&gt; dictionary with the &lt;code&gt;pod_override&lt;/code&gt; parameter, which takes a Kubernetes V1Pod object for a1:1 setting override. These changes have removed over three thousand lines of code from the KubernetesExecutor, which makes it run faster and creates fewer potential errors.&lt;/p&gt;
&lt;p&gt;Read more here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/executor/kubernetes.html?highlight=pod_override#pod-template-file&#34;&gt;Docs on pod_template_file&lt;/a&gt; &lt;br&gt;
&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/executor/kubernetes.html?highlight=pod_override#pod-override&#34;&gt;Docs on pod_override&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;airflow-core-and-providers-splitting-airflow-into-60-packages&#34;&gt;Airflow core and providers: Splitting Airflow into 60+ packages:&lt;/h2&gt;
&lt;p&gt;Airflow 2.0 is not a monolithic &amp;ldquo;one to rule them all&amp;rdquo; package. We‚Äôve split Airflow into core and 61 (for now) provider packages. Each provider package is for either a particular external service (Google, Amazon, Microsoft, Snowflake), a database (Postgres, MySQL), or a protocol (HTTP/FTP). Now you can create a custom Airflow installation from &amp;ldquo;building&amp;rdquo; blocks and choose only what you need, plus add whatever other requirements you might have. Some of the common providers are installed automatically (ftp, http, imap, sqlite) as they are commonly used. Other providers are automatically installed when you choose appropriate extras when installing Airflow.&lt;/p&gt;
&lt;p&gt;The provider architecture should make it much easier to get a fully customized, yet consistent runtime with the right set of Python dependencies.&lt;/p&gt;
&lt;p&gt;But that‚Äôs not all: you can write your own custom providers and add things like custom connection types, customizations of the Connection Forms, and extra links to your operators in a manageable way. You can build your own provider and install it as a Python package and have your customizations visible right in the Airflow UI.&lt;/p&gt;
&lt;p&gt;Our very own Jarek Potiuk has written about &lt;a href=&#34;https://higrys.medium.com/airflow-2-0-providers-1bd21ba3bd93&#34;&gt;providers in much more detail&lt;/a&gt; on Jarek&amp;rsquo;s blog.&lt;/p&gt;
&lt;p&gt;Docs on the &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow-providers/&#34;&gt;providers concept and writing custom providers&lt;/a&gt; &lt;br&gt;
Docs on &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html&#34;&gt;all providers packages available&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;security&#34;&gt;Security&lt;/h2&gt;
&lt;p&gt;As part of Airflow 2.0 effort, there has been a conscious focus on Security and reducing areas of exposure. This is represented across different functional areas in different forms. For example, in the new REST API, all operations now require authorization. Similarly, in the configuration settings, the Fernet key is now required to be specified.&lt;/p&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;Configuration in the form of the airflow.cfg file has been rationalized further in distinct sections, specifically around &amp;ldquo;core&amp;rdquo;. Additionally, a significant amount of configuration options have been deprecated or moved to individual component-specific configuration files, such as the pod-template-file for Kubernetes execution-related configuration.&lt;/p&gt;
&lt;h2 id=&#34;thanks-to-all-of-you&#34;&gt;Thanks to all of you&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ve tried to make as few breaking changes as possible and to provide deprecation path in the code, especially in the case of anything called in the DAG. That said, please read through UPDATING.md to check what might affect you. For example: We have re-organized the layout of operators (they now all live under airflow.providers.*) but the old names should continue to work - you&amp;rsquo;ll just notice a lot of DeprecationWarnings that need to be fixed up.&lt;/p&gt;
&lt;p&gt;Thank you so much to all the contributors who got us to this point, in no particular order: Kaxil Naik, Daniel Imberman, Jarek Potiuk, Tomek Urbaszek, Kamil Bregu≈Ça, Gerard Casas Saez, Xiaodong DENG, Kevin Yang, James Timmins, Yingbo Wang, Qian Yu, Ryan Hamilton and the 100s of others who keep making Airflow better for everyone.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 1.10.12</title>
      <link>/blog/airflow-1.10.12/</link>
      <pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-1.10.12/</guid>
      <description>
        
        
        &lt;p&gt;Airflow 1.10.12 contains 113 commits since 1.10.11 and includes 5 new features, 23 improvements, 23 bug fixes,
and several doc changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyPI&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/apache-airflow/1.10.12/&#34;&gt;https://pypi.org/project/apache-airflow/1.10.12/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.12/&#34;&gt;https://airflow.apache.org/docs/1.10.12/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changelog&lt;/strong&gt;: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.12/changelog.html&#34;&gt;http://airflow.apache.org/docs/1.10.12/changelog.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Airflow 1.10.11 has breaking changes with respect to
KubernetesExecutor &amp;amp; KubernetesPodOperator so I recommend users to directly upgrade to Airflow 1.10.12 instead&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Some of the noteworthy new features (user-facing) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/8560&#34;&gt;Allow defining custom XCom class&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/9645&#34;&gt;Get Airflow configs with sensitive data from Secret Backends&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/10282&#34;&gt;Add AirflowClusterPolicyViolation support to Airflow local settings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;allow-defining-custom-xcom-class&#34;&gt;Allow defining Custom XCom class&lt;/h3&gt;
&lt;p&gt;Until Airflow 1.10.11, the XCom data was only stored in Airflow Metadatabase. From Airflow 1.10.12, users
would be able to define custom XCom classes. This will allow users to transfer larger data between tasks.
An example here would be to store XCom in S3 or GCS Bucket if the size of data that needs to be stored is larger
than &lt;code&gt;XCom.MAX_XCOM_SIZE&lt;/code&gt; (48 KB).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PR&lt;/strong&gt;: &lt;a href=&#34;https://github.com/apache/airflow/pull/8560&#34;&gt;https://github.com/apache/airflow/pull/8560&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;get-airflow-configs-with-sensitive-data-from-secret-backends&#34;&gt;Get Airflow configs with sensitive data from Secret Backends&lt;/h3&gt;
&lt;p&gt;Users would be able to get the following Airflow configs from Secrets Backend like Hashicorp Vault:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sql_alchemy_conn&lt;/code&gt; in [core] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fernet_key&lt;/code&gt; in [core] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;broker_url&lt;/code&gt; in [celery] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flower_basic_auth&lt;/code&gt; in [celery] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;result_backend&lt;/code&gt; in [celery] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;password&lt;/code&gt; in [atlas] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;smtp_password&lt;/code&gt; in [smtp] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bind_password&lt;/code&gt; in [ldap] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git_password&lt;/code&gt; in [kubernetes] section&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further improving Airflow&amp;rsquo;s Secret Management story, from Airflow 1.10.12, users don&amp;rsquo;t need to hardcode
the &lt;strong&gt;sensitive&lt;/strong&gt; config value in airflow.cfg nor then need to use an Environment variable to set this config.&lt;/p&gt;
&lt;p&gt;For example, the metadata database connection string can either be set in airflow.cfg like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span class=&#34;k&#34;&gt;[core]&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;sql_alchemy_conn_secret&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;sql_alchemy_conn&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will retrieve config option from the set Secret Backends.&lt;/p&gt;
&lt;p&gt;As you can see you just need to add a &lt;code&gt;_secret&lt;/code&gt; suffix at the end of the actual config option
and the value needs to be the &lt;strong&gt;key&lt;/strong&gt; which the Secrets backend will look for.&lt;/p&gt;
&lt;p&gt;Similarly, &lt;code&gt;_secret&lt;/code&gt; config options can also be set using a corresponding environment variable. For example:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;export AIRFLOW__CORE__SQL_ALCHEMY_CONN_SECRET=sql_alchemy_conn
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;More details: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.12/howto/set-config.html&#34;&gt;http://airflow.apache.org/docs/1.10.12/howto/set-config.html&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;add-airflowclusterpolicyviolation-support-to-airflow_local_settingspy&#34;&gt;Add AirflowClusterPolicyViolation support to airflow_local_settings.py&lt;/h3&gt;
&lt;p&gt;Users can use Cluster Policies to apply cluster-wide checks on Airflow
tasks. You can raise &lt;a href=&#34;http://airflow.apache.org/docs/1.10.12/_api/airflow/exceptions/index.html#airflow.exceptions.AirflowClusterPolicyViolation&#34;&gt;AirflowClusterPolicyViolation&lt;/a&gt;
in a policy or task mutation hook to prevent a DAG from being
imported or prevent a task from being executed if the task is not compliant with
your check.&lt;/p&gt;
&lt;p&gt;These checks are intended to help teams using Airflow to protect against common
beginner errors that may get past a code reviewer, rather than as technical
security controls.&lt;/p&gt;
&lt;p&gt;For example, don&amp;rsquo;t run tasks without &lt;code&gt;airflow&lt;/code&gt; owners:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;task_must_have_owners&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;operators&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;default_owner&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AirflowClusterPolicyViolation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
            &lt;span class=&#34;s1&#34;&gt;&amp;#39;Task must have non-None non-default owner. Current value: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;More details: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.12/concepts.html#cluster-policies-for-custom-task-checks&#34;&gt;http://airflow.apache.org/docs/1.10.12/concepts.html#cluster-policies-for-custom-task-checks&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;launch-pods-via-yaml-files-when-using-kubernetesexecutor-and-kubernetespodoperator&#34;&gt;Launch Pods via YAML files when using KubernetesExecutor and KubernetesPodOperator&lt;/h3&gt;
&lt;p&gt;As of 1.10.12, users can launch pods via YAML files instead of passing various configurations.&lt;/p&gt;
&lt;p&gt;To allow greater flexibility we have deprecated Airflow&amp;rsquo;s Pod class and instead now use classes and
objects from the official Kubernetes API. The POD class will still work but raise a deprecation
warning. This feature involved a pretty extensive rewrite of all of our pod creation code.&lt;/p&gt;
&lt;p&gt;Initially, we were going to hold off on these features until Airflow 2.0. However, we soon
realized that exposing these features in 1.10.x is crucial in preparing users for the 2.0 release to come.&lt;/p&gt;
&lt;p&gt;Details: &lt;a href=&#34;https://github.com/apache/airflow/pull/6230&#34;&gt;https://github.com/apache/airflow/pull/6230&lt;/a&gt; (&lt;a href=&#34;https://github.com/apache/airflow/commit/7aa0f472b57985a952a3e3d0a38f1b2535d93413&#34;&gt;Backport commit&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;updating-guide&#34;&gt;Updating Guide&lt;/h2&gt;
&lt;p&gt;If you are updating Apache Airflow from a previous version to &lt;code&gt;1.10.12&lt;/code&gt;, please take a note of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;airflow upgradedb&lt;/code&gt; after &lt;code&gt;pip install -U apache-airflow==1.10.12&lt;/code&gt; as &lt;code&gt;1.10.12&lt;/code&gt; contains 1 database migration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As of airflow 1.10.12, using the &lt;code&gt;airflow.contrib.kubernetes.Pod&lt;/code&gt; class in the &lt;code&gt;pod_mutation_hook&lt;/code&gt; is now
deprecated. Instead we recommend that users treat the pod parameter as a &lt;code&gt;kubernetes.client.models.V1Pod&lt;/code&gt; object.
This means that users now have access to the full Kubernetes API when modifying airflow pods for mutating POD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Previously, when tasks skipped by SkipMixin (such as &lt;code&gt;BranchPythonOperator&lt;/code&gt;, &lt;code&gt;BaseBranchOperator&lt;/code&gt; and
&lt;code&gt;ShortCircuitOperator&lt;/code&gt;) are cleared, they execute. Since 1.10.12, when such skipped tasks are cleared,
they will be skipped again by the newly introduced &lt;code&gt;NotPreviouslySkippedDep&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;special-note&#34;&gt;Special Note&lt;/h2&gt;
&lt;h3 id=&#34;python-2&#34;&gt;Python 2&lt;/h3&gt;
&lt;p&gt;Python 2 has reached end of its life on Jan 2020. Airflow Master no longer supports Python 2.
Airflow 1.10.* would be the last series to support Python 2.&lt;/p&gt;
&lt;p&gt;We strongly recommend users to use Python &amp;gt;= 3.6&lt;/p&gt;
&lt;h3 id=&#34;use-airflow-rbac-ui&#34;&gt;Use Airflow RBAC UI&lt;/h3&gt;
&lt;p&gt;Airflow 1.10.12 ships with 2 UIs, the default is non-RBAC Flask-admin based UI and Flask-appbuilder based UI.&lt;/p&gt;
&lt;p&gt;The Flask-AppBuilder (FAB) based UI allows Role-based Access Control and has more advanced features compared to
the legacy Flask-admin based UI. This UI can be enabled by setting &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in
your &lt;code&gt;airflow.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Flask-admin based UI is deprecated and new features won&amp;rsquo;t be ported to it. This UI will still be the default
for 1.10.* series but would no longer be available from Airflow 2.0&lt;/p&gt;
&lt;h3 id=&#34;we-have-moved-to-github-issues&#34;&gt;We have moved to GitHub Issues&lt;/h3&gt;
&lt;p&gt;The Airflow Project has moved from &lt;a href=&#34;https://issues.apache.org/jira/projects/AIRFLOW/issues&#34;&gt;JIRA&lt;/a&gt; to
&lt;a href=&#34;https://github.com/apache/airflow/issues&#34;&gt;GitHub&lt;/a&gt; for tracking issues.&lt;/p&gt;
&lt;p&gt;So if you find any bugs in Airflow 1.10.12 please create a GitHub Issue for it.&lt;/p&gt;
&lt;h2 id=&#34;list-of-contributors&#34;&gt;List of Contributors&lt;/h2&gt;
&lt;p&gt;According to git shortlog, the following people contributed to the 1.10.12 release. Thank you to all contributors!&lt;/p&gt;
&lt;p&gt;Alexander Sutcliffe, Andy, Aneesh Joseph, Ash Berlin-Taylor, Aviral Agrawal, BaoshanGu, Beni Ben zikry,
Daniel Imberman, Daniel Standish, Danylo Baibak, Ephraim Anierobi, Felix Uellendall, Greg Neiheisel,
Hartorn, Jacob Ferriero, Jannik F, Jarek Potiuk, Jinhui Zhang, Kamil Bregu≈Ça, Kaxil Naik, Kurganov,
Luis Magana, Max Arrich, Pete DeJoy, Sumit Maheshwari, Tomek Urbaszek, Vicken Simonian, Vinnie Guimaraes,
William Tran, Xiaodong Deng, YI FU, Zikun Zhu, dewaldabrie, pulsar314, retornam, yuqian90&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 1.10.10</title>
      <link>/blog/airflow-1.10.10/</link>
      <pubDate>Thu, 09 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-1.10.10/</guid>
      <description>
        
        
        &lt;p&gt;Airflow 1.10.10 contains 199 commits since 1.10.9 and includes 11 new features, 43 improvements, 44 bug fixes, and several doc changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyPI&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/apache-airflow/1.10.10/&#34;&gt;https://pypi.org/project/apache-airflow/1.10.10/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/&#34;&gt;https://airflow.apache.org/docs/1.10.10/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changelog&lt;/strong&gt;: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.10/changelog.html&#34;&gt;http://airflow.apache.org/docs/1.10.10/changelog.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the noteworthy new features (user-facing) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/8046&#34;&gt;Allow user to chose timezone to use in the RBAC UI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7832&#34;&gt;Add Production Docker image support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html&#34;&gt;Allow Retrieving Airflow Connections &amp;amp; Variables from various Secrets backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://airflow.apache.org/docs/1.10.10/dag-serialization.html&#34;&gt;Stateless Webserver using DAG Serialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7880&#34;&gt;Tasks with Dummy Operators are no longer sent to executor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7312&#34;&gt;Allow passing DagRun conf when triggering dags via UI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;allow-user-to-chose-timezone-to-use-in-the-rbac-ui&#34;&gt;Allow user to chose timezone to use in the RBAC UI&lt;/h3&gt;
&lt;p&gt;By default the Web UI will show times in UTC. It is possible to change the timezone shown by using the menu in the top
right (click on the clock to activate it):&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Screenshot&lt;/strong&gt;:
&lt;img src=&#34;rbac-ui-timezone.gif&#34; alt=&#34;Allow user to chose timezone to use in the RBAC UI&#34;&gt;&lt;/p&gt;
&lt;p&gt;Details: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/timezone.html#web-ui&#34;&gt;https://airflow.apache.org/docs/1.10.10/timezone.html#web-ui&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This feature is only available for the RBAC UI (enabled using &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in your &lt;code&gt;airflow.cfg&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id=&#34;add-production-docker-image-support&#34;&gt;Add Production Docker image support&lt;/h3&gt;
&lt;p&gt;There are brand new production images (alpha quality) available for Airflow 1.10.10. You can pull them from the
&lt;a href=&#34;https://hub.docker.com/r/apache/airflow&#34;&gt;Apache Airflow Dockerhub&lt;/a&gt; repository and start using it.&lt;/p&gt;
&lt;p&gt;More information about using production images can be found in &lt;a href=&#34;https://github.com/apache/airflow/blob/master/IMAGES.rst#using-the-images&#34;&gt;https://github.com/apache/airflow/blob/master/IMAGES.rst#using-the-images&lt;/a&gt;. Soon it will be updated with
information how to use images using official helm chart.&lt;/p&gt;
&lt;p&gt;To pull the images you can run one of the following commands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10-python2.7&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10-python3.5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10-python3.6&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10-python3.7&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10&lt;/code&gt; (uses Python 3.6)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;allow-retrieving-airflow-connections--variables-from-various-secrets-backend&#34;&gt;Allow Retrieving Airflow Connections &amp;amp; Variables from various Secrets backend&lt;/h3&gt;
&lt;p&gt;From Airflow 1.10.10, users would be able to get Airflow Variables from Environment Variables.&lt;/p&gt;
&lt;p&gt;Details: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/concepts.html#storing-variables-in-environment-variables&#34;&gt;https://airflow.apache.org/docs/1.10.10/concepts.html#storing-variables-in-environment-variables&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A new concept of Secrets Backend has been introduced to retrieve Airflow Connections and Variables.&lt;/p&gt;
&lt;p&gt;From Airflow 1.10.10, users can retrieve Connections &amp;amp; Variables using the same syntax (no DAG code change is required),
from a secret backend defined in &lt;code&gt;airflow.cfg&lt;/code&gt;. If no backend is defined, Airflow falls-back to Environment Variables
and then Metadata DB.&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html#configuration&#34;&gt;https://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html#configuration&lt;/a&gt; for details on how-to
configure Secrets backend.&lt;/p&gt;
&lt;p&gt;As of 1.10.10, Airflow supports the following Secret Backends:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hashicorp Vault&lt;/li&gt;
&lt;li&gt;GCP Secrets Manager&lt;/li&gt;
&lt;li&gt;AWS Parameters Store&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Details: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html&#34;&gt;https://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Example configuration to use Hashicorp Vault as the backend:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span class=&#34;k&#34;&gt;[secrets]&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;backend&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;airflow.contrib.secrets.hashicorp_vault.VaultBackend&lt;/span&gt;
&lt;span class=&#34;na&#34;&gt;backend_kwargs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{&amp;#34;url&amp;#34;: &amp;#34;http://127.0.0.1:8200&amp;#34;, &amp;#34;connections_path&amp;#34;: &amp;#34;connections&amp;#34;, &amp;#34;variables_path&amp;#34;: &amp;#34;variables&amp;#34;, &amp;#34;mount_point&amp;#34;: &amp;#34;airflow&amp;#34;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;stateless-webserver-using-dag-serialization&#34;&gt;Stateless Webserver using DAG Serialization&lt;/h3&gt;
&lt;p&gt;The Webserver can now run without access to DAG Files when DAG Serialization is turned on.
The 2 limitations we had in 1.10.7-1.10.9 (
&lt;a href=&#34;https://airflow.apache.org/docs/1.10.7/dag-serialization.html#limitations&#34;&gt;https://airflow.apache.org/docs/1.10.7/dag-serialization.html#limitations&lt;/a&gt;)
have been resolved.&lt;/p&gt;
&lt;p&gt;The main advantage of this would be reduction in Webserver startup time for large number of DAGs.
Without DAG Serialization all the DAGs are loaded in the DagBag during the
Webserver startup.&lt;/p&gt;
&lt;p&gt;With DAG Serialization, an empty DagBag is created and
Dags are loaded from DB only when needed (i.e. when a particular DAG is
clicked on in the home page)&lt;/p&gt;
&lt;p&gt;Details: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.10/dag-serialization.html&#34;&gt;http://airflow.apache.org/docs/1.10.10/dag-serialization.html&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;tasks-using-dummy-operators-are-no-longer-sent-to-executor&#34;&gt;Tasks using Dummy Operators are no longer sent to executor&lt;/h3&gt;
&lt;p&gt;The Dummy operators does not actually do any work and are mostly used for organizing/grouping tasks along
with BranchPythonOperator.&lt;/p&gt;
&lt;p&gt;Previously, when using Kubernetes Executor, the executor would spin up a whole worker pod to execute a dummy task.
With Airflow 1.10.10 tasks using Dummy Operators would be scheduled &amp;amp; evaluated by the Scheduler but not sent to the
Executor. This should significantly improve execution time and resource usage.&lt;/p&gt;
&lt;h3 id=&#34;allow-passing-dagrun-conf-when-triggering-dags-via-ui&#34;&gt;Allow passing DagRun conf when triggering dags via UI&lt;/h3&gt;
&lt;p&gt;When triggering a DAG from the CLI or the REST API, it s possible to pass configuration for the DAG run as a JSON blob.&lt;/p&gt;
&lt;p&gt;From Airflow 1.10.10, when a user clicks on Trigger Dag button, a new screen confirming the trigger request, and allowing the user to pass a JSON configuration
blob would be show.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Screenshot&lt;/strong&gt;:
&lt;img src=&#34;trigger-dag-conf.png&#34; alt=&#34;Allow passing DagRun conf when triggering dags via UI&#34;&gt;&lt;/p&gt;
&lt;p&gt;Details: &lt;a href=&#34;https://github.com/apache/airflow/pull/7312&#34;&gt;https://github.com/apache/airflow/pull/7312&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;updating-guide&#34;&gt;Updating Guide&lt;/h2&gt;
&lt;p&gt;If you are updating Apache Airflow from a previous version to &lt;code&gt;1.10.10&lt;/code&gt;, please take a note of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;airflow upgradedb&lt;/code&gt; after &lt;code&gt;pip install -U apache-airflow==1.10.10&lt;/code&gt; as &lt;code&gt;1.10.10&lt;/code&gt; contains 3 database migrations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you have used &lt;code&gt;none_failed&lt;/code&gt; trigger rule in your DAG, change it to use the new &lt;code&gt;none_failed_or_skipped&lt;/code&gt; trigger rule.
As previously implemented, the actual behavior of &lt;code&gt;none_failed&lt;/code&gt; trigger rule would skip the current task if all parents of the task
had also skipped. This was not in-line with what was documented about that trigger rule. We have changed the implementation to match
the documentation, hence if you need the old behavior use &lt;code&gt;none_failed_or_skipped&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;More details in &lt;a href=&#34;https://github.com/apache/airflow/pull/7464&#34;&gt;https://github.com/apache/airflow/pull/7464&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setting empty string to a Airflow Variable will now return an empty string, it previously returned &lt;code&gt;None&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; Variable.set(&#39;test_key&#39;, &#39;&#39;)
&amp;gt;&amp;gt; Variable.get(&#39;test_key&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code returned &lt;code&gt;None&lt;/code&gt; previously, now it will return &amp;lsquo;&amp;rsquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When a task is marked as &lt;code&gt;success&lt;/code&gt; by a user in Airflow UI, function defined in &lt;code&gt;on_success_callback&lt;/code&gt; will be called.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;special-note--deprecations&#34;&gt;Special Note / Deprecations&lt;/h2&gt;
&lt;h3 id=&#34;python-2&#34;&gt;Python 2&lt;/h3&gt;
&lt;p&gt;Python 2 has reached end of its life on Jan 2020. Airflow Master no longer supports Python 2.
Airflow 1.10.* would be the last series to support Python 2.&lt;/p&gt;
&lt;p&gt;We strongly recommend users to use Python &amp;gt;= 3.6&lt;/p&gt;
&lt;h3 id=&#34;use-airflow-rbac-ui&#34;&gt;Use Airflow RBAC UI&lt;/h3&gt;
&lt;p&gt;Airflow 1.10.10 ships with 2 UIs, the default is non-RBAC Flask-admin based UI and Flask-appbuilder based UI.&lt;/p&gt;
&lt;p&gt;The Flask-AppBuilder (FAB) based UI allows Role-based Access Control and has more advanced features compared to
the legacy Flask-admin based UI. This UI can be enabled by setting &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in your &lt;code&gt;airflow.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Flask-admin based UI is deprecated and new features won&amp;rsquo;t be ported to it. This UI will still be the default
for 1.10.* series but would no longer be available from Airflow 2.0&lt;/p&gt;
&lt;h3 id=&#34;running-airflow-on-macos&#34;&gt;Running Airflow on MacOS&lt;/h3&gt;
&lt;p&gt;Run &lt;code&gt;export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES&lt;/code&gt; in your scheduler environmentIf you are running Airflow on MacOS
and get the following error in the Scheduler logs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;objc[1873]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[1873]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This error occurs because of added security to restrict multiprocessing &amp;amp; multithreading in Mac OS High Sierra and above.&lt;/p&gt;
&lt;h3 id=&#34;we-have-moved-to-github-issues&#34;&gt;We have moved to GitHub Issues&lt;/h3&gt;
&lt;p&gt;The Airflow Project has moved from &lt;a href=&#34;https://issues.apache.org/jira/projects/AIRFLOW/issues&#34;&gt;JIRA&lt;/a&gt; to
&lt;a href=&#34;https://github.com/apache/airflow/issues&#34;&gt;GitHub&lt;/a&gt; for tracking issues.&lt;/p&gt;
&lt;p&gt;So if you find any bugs in Airflow 1.10.10 please create a GitHub Issue for it.&lt;/p&gt;
&lt;h2 id=&#34;list-of-contributors&#34;&gt;List of Contributors&lt;/h2&gt;
&lt;p&gt;According to git shortlog, the following people contributed to the 1.10.10 release. Thank you to all contributors!&lt;/p&gt;
&lt;p&gt;ANiteckiP, Alex Guziel, Alex Lue, Anita Fronczak, Ash Berlin-Taylor, Benji Visser, Bhavika Tekwani, Brad Dettmer, Chris McLennon, Cooper Gillan, Daniel Imberman, Daniel Standish, Felix Uellendall, Jarek Potiuk, Jiajie Zhong, Jithin Sukumar, Kamil Bregu≈Ça, Kaxil Naik, Kengo Seki, Kris, Kumpan Anton, Lokesh Lal, Louis Guitton, Louis Simoneau, Luyao Yang, No√´l Bardelot, Omair Khan, Philipp Gro√üelfinger, Ping Zhang, RasPavel, Ray, Robin Edwards, Ry Walker, Saurabh, Sebastian Brandt, Tomek Kzukowski, Tomek Urbaszek, Van-Duyet Le, Xiaodong Deng, Xinbin Huang, Yu Qian, Zacharya, atrbgithub, cong-zhu, retornam&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 1.10.8 &amp; 1.10.9</title>
      <link>/blog/airflow-1.10.8-1.10.9/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-1.10.8-1.10.9/</guid>
      <description>
        
        
        &lt;p&gt;Airflow 1.10.8 contains 160 commits since 1.10.7 and includes 4 new features, 42 improvements, 36 bug fixes, and several doc changes.&lt;/p&gt;
&lt;p&gt;We released 1.10.9 on the same day as one of the Flask dependencies (Werkzeug) released 1.0 which broke Airflow 1.10.8.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyPI&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/apache-airflow/1.10.9/&#34;&gt;https://pypi.org/project/apache-airflow/1.10.9/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.9/&#34;&gt;https://airflow.apache.org/docs/1.10.9/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changelog (1.10.8)&lt;/strong&gt;: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.8/changelog.html#airflow-1-10-8-2020-01-07&#34;&gt;http://airflow.apache.org/docs/1.10.8/changelog.html#airflow-1-10-8-2020-01-07&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changelog (1.10.9)&lt;/strong&gt;: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.9/changelog.html#airflow-1-10-9-2020-02-10&#34;&gt;http://airflow.apache.org/docs/1.10.9/changelog.html#airflow-1-10-9-2020-02-10&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the noteworthy new features (user-facing) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/6489&#34;&gt;Add tags to DAGs and use it for filtering in the UI (RBAC only)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://airflow.apache.org/docs/1.10.9/executor/debug.html&#34;&gt;New Executor: DebugExecutor for Local debugging from your IDE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7281&#34;&gt;Allow passing conf in &amp;ldquo;Add DAG Run&amp;rdquo; (Triggered Dags) view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7038&#34;&gt;Allow dags to run for future execution dates for manually triggered DAGs (only if &lt;code&gt;schedule_interval=None&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/1.10.9/configurations-ref.html&#34;&gt;Dedicated page in documentation for all configs in airflow.cfg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;add-tags-to-dags-and-use-it-for-filtering-in-the-ui&#34;&gt;Add tags to DAGs and use it for filtering in the UI&lt;/h3&gt;
&lt;p&gt;In order to filter DAGs (e.g by team), you can add tags in each dag. The filter is saved in a cookie and can be reset by the reset button.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;p&gt;In your Dag file, pass a list of tags you want to add to DAG object:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dag_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;example_dag_tag&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;schedule_interval&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;0 0 * * *&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;tags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;example&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Screenshot&lt;/strong&gt;:
&lt;img src=&#34;airflow-dag-tags.png&#34; alt=&#34;Add filter by DAG tags&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This feature is only available for the RBAC UI (enabled using &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in your &lt;code&gt;airflow.cfg&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id=&#34;special-note--deprecations&#34;&gt;Special Note / Deprecations&lt;/h2&gt;
&lt;h3 id=&#34;python-2&#34;&gt;Python 2&lt;/h3&gt;
&lt;p&gt;Python 2 has reached end of its life on Jan 2020. Airflow Master no longer supports Python 2.
Airflow 1.10.* would be the last series to support Python 2.&lt;/p&gt;
&lt;p&gt;We strongly recommend users to use Python &amp;gt;= 3.6&lt;/p&gt;
&lt;h3 id=&#34;use-airflow-rbac-ui&#34;&gt;Use Airflow RBAC UI&lt;/h3&gt;
&lt;p&gt;Airflow 1.10.9 ships with 2 UIs, the default is non-RBAC Flask-admin based UI and Flask-appbuilder based UI.&lt;/p&gt;
&lt;p&gt;The Flask-AppBuilder (FAB) based UI is allows Role-based Access Control and has more advanced features compared to
the legacy Flask-admin based UI. This UI can be enabled by setting &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in your &lt;code&gt;airflow.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Flask-admin based UI is deprecated and new features won&amp;rsquo;t be ported to it. This UI will still be the default
for 1.10.* series but would no longer be available from Airflow 2.0&lt;/p&gt;
&lt;h2 id=&#34;list-of-contributors&#34;&gt;List of Contributors&lt;/h2&gt;
&lt;p&gt;According to git shortlog, the following people contributed to the 1.10.8 and 1.10.9 release. Thank you to all contributors!&lt;/p&gt;
&lt;p&gt;Anita Fronczak, Ash Berlin-Taylor, BasPH, Bharat Kashyap, Bharath Palaksha, Bhavika Tekwani, Bjorn Olsen, Brian Phillips, Cooper Gillan, Daniel Cohen, Daniel Imberman, Daniel Standish, Gabriel Eckers, Hossein Torabi, Igor Khrol, Jacob, Jarek Potiuk, Jay, Jiajie Zhong, Jithin Sukumar, Kamil Bregu≈Ça, Kaxil Naik, Kousuke Saruta, Mustafa G√∂k, No√´l Bardelot, Oluwafemi Sule, Pete DeJoy, QP Hou, Qian Yu, Robin Edwards, Ry Walker, Steven van Rossum, Tomek Urbaszek, Xinbin Huang, Yuen-Kuei Hsueh, Yu Qian, Zacharya, ZxMYS, rconroy293, tooptoop4&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
