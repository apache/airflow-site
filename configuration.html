

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Configuration &mdash; Airflow Documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Airflow Documentation" href="index.html"/>
        <link rel="next" title="UI / Screenshots" href="ui.html"/>
        <link rel="prev" title="Tutorial" href="tutorial.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Airflow
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#setting-configuration-options">Setting Configuration Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setting-up-a-backend">Setting up a Backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="#connections">Connections</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scaling-out-with-celery">Scaling Out with Celery</a></li>
<li class="toctree-l2"><a class="reference internal" href="#logs">Logs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scaling-out-on-mesos-community-contributed">Scaling Out on Mesos (community contributed)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#integration-with-systemd">Integration with systemd</a></li>
<li class="toctree-l2"><a class="reference internal" href="#integration-with-upstart">Integration with upstart</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ui.html">UI / Screenshots</a></li>
<li class="toctree-l1"><a class="reference internal" href="concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiling.html">Data Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="scheduler.html">Scheduling &amp; Triggers</a></li>
<li class="toctree-l1"><a class="reference internal" href="plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="code.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Airflow</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Configuration</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/configuration.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="configuration">
<h1>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h1>
<p>Setting up the sandbox in the <a class="reference internal" href="start.html"><span class="doc">Quick Start</span></a> section was easy;
building a production-grade environment requires a bit more work!</p>
<div class="section" id="setting-configuration-options">
<h2>Setting Configuration Options<a class="headerlink" href="#setting-configuration-options" title="Permalink to this headline">¶</a></h2>
<p>The first time you run Airflow, it will create a file called <code class="docutils literal"><span class="pre">airflow.cfg</span></code> in
your <code class="docutils literal"><span class="pre">$AIRFLOW_HOME</span></code> directory (<code class="docutils literal"><span class="pre">~/airflow</span></code> by default). This file contains Airflow&#8217;s configuration and you
can edit it to change any of the settings. You can also set options with environment variables by using this format:
<code class="docutils literal"><span class="pre">$AIRFLOW__{SECTION}__{KEY}</span></code> (note the double underscores).</p>
<p>For example, the
metadata database connection string can either be set in <code class="docutils literal"><span class="pre">airflow.cfg</span></code> like this:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="o">[</span>core<span class="o">]</span>
<span class="nv">sql_alchemy_conn</span> <span class="o">=</span> my_conn_string
</pre></div>
</div>
<p>or by creating a corresponding environment variable:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">AIRFLOW__CORE__SQL_ALCHEMY_CONN</span><span class="o">=</span>my_conn_string
</pre></div>
</div>
<p>You can also derive the connection string at run time by appending <code class="docutils literal"><span class="pre">_cmd</span></code> to the key like this:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="o">[</span>core<span class="o">]</span>
<span class="nv">sql_alchemy_conn_cmd</span> <span class="o">=</span> bash_command_to_run
</pre></div>
</div>
<p>But only three such configuration elements namely sql_alchemy_conn, broker_url and celery_result_backend can be fetched as a command. The idea behind this is to not store passwords on boxes in plain text files. The order of precedence is as follows -</p>
<ol class="arabic simple">
<li>environment variable</li>
<li>configuration in airflow.cfg</li>
<li>command in airflow.cfg</li>
<li>default</li>
</ol>
</div>
<div class="section" id="setting-up-a-backend">
<h2>Setting up a Backend<a class="headerlink" href="#setting-up-a-backend" title="Permalink to this headline">¶</a></h2>
<p>If you want to take a real test drive of Airflow, you should consider
setting up a real database backend and switching to the LocalExecutor.</p>
<p>As Airflow was built to interact with its metadata using the great SqlAlchemy
library, you should be able to use any database backend supported as a
SqlAlchemy backend. We recommend using <strong>MySQL</strong> or <strong>Postgres</strong>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you decide to use <strong>Postgres</strong>, we recommend using the <code class="docutils literal"><span class="pre">psycopg2</span></code>
driver and specifying it in your SqlAlchemy connection string.
Also note that since SqlAlchemy does not expose a way to target a
specific schema in the Postgres connection URI, you may
want to set a default schema for your role with a
command similar to <code class="docutils literal"><span class="pre">ALTER</span> <span class="pre">ROLE</span> <span class="pre">username</span> <span class="pre">SET</span> <span class="pre">search_path</span> <span class="pre">=</span> <span class="pre">airflow,</span> <span class="pre">foobar;</span></code></p>
</div>
<p>Once you&#8217;ve setup your database to host Airflow, you&#8217;ll need to alter the
SqlAlchemy connection string located in your configuration file
<code class="docutils literal"><span class="pre">$AIRFLOW_HOME/airflow.cfg</span></code>. You should then also change the &#8220;executor&#8221;
setting to use &#8220;LocalExecutor&#8221;, an executor that can parallelize task
instances locally.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># initialize the database</span>
airflow initdb
</pre></div>
</div>
</div>
<div class="section" id="connections">
<h2>Connections<a class="headerlink" href="#connections" title="Permalink to this headline">¶</a></h2>
<p>Airflow needs to know how to connect to your environment. Information
such as hostname, port, login and passwords to other systems and services is
handled in the <code class="docutils literal"><span class="pre">Admin-&gt;Connection</span></code> section of the UI. The pipeline code you
will author will reference the &#8216;conn_id&#8217; of the Connection objects.</p>
<img alt="_images/connections.png" src="_images/connections.png" />
<p>By default, Airflow will save the passwords for the connection in plain text
within the metadata database. The <code class="docutils literal"><span class="pre">crypto</span></code> package is highly recommended
during installation. The <code class="docutils literal"><span class="pre">crypto</span></code> package does require that your operating
system have libffi-dev installed.</p>
<p>Connections in Airflow pipelines can be created using environment variables.
The environment variable needs to have a prefix of <code class="docutils literal"><span class="pre">AIRFLOW_CONN_</span></code> for
Airflow with the value in a URI format to use the connection properly. Please
see the <a class="reference internal" href="concepts.html"><span class="doc">Concepts</span></a> documentation for more information on environment
variables and connections.</p>
</div>
<div class="section" id="scaling-out-with-celery">
<h2>Scaling Out with Celery<a class="headerlink" href="#scaling-out-with-celery" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal"><span class="pre">CeleryExecutor</span></code> is one of the ways you can scale out the number of workers. For this
to work, you need to setup a Celery backend (<strong>RabbitMQ</strong>, <strong>Redis</strong>, ...) and
change your <code class="docutils literal"><span class="pre">airflow.cfg</span></code> to point the executor parameter to
<code class="docutils literal"><span class="pre">CeleryExecutor</span></code> and provide the related Celery settings.</p>
<p>For more information about setting up a Celery broker, refer to the
exhaustive <a class="reference external" href="http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html">Celery documentation on the topic</a>.</p>
<p>Here are a few imperative requirements for your workers:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">airflow</span></code> needs to be installed, and the CLI needs to be in the path</li>
<li>Airflow configuration settings should be homogeneous across the cluster</li>
<li>Operators that are executed on the worker need to have their dependencies
met in that context. For example, if you use the <code class="docutils literal"><span class="pre">HiveOperator</span></code>,
the hive CLI needs to be installed on that box, or if you use the
<code class="docutils literal"><span class="pre">MySqlOperator</span></code>, the required Python library needs to be available in
the <code class="docutils literal"><span class="pre">PYTHONPATH</span></code> somehow</li>
<li>The worker needs to have access to its <code class="docutils literal"><span class="pre">DAGS_FOLDER</span></code>, and you need to
synchronize the filesystems by your own means. A common setup would be to
store your DAGS_FOLDER in a Git repository and sync it across machines using
Chef, Puppet, Ansible, or whatever you use to configure machines in your
environment. If all your boxes have a common mount point, having your
pipelines files shared there should work as well</li>
</ul>
<p>To kick off a worker, you need to setup Airflow and kick off the worker
subcommand</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>airflow worker
</pre></div>
</div>
<p>Your worker should start picking up tasks as soon as they get fired in
its direction.</p>
<p>Note that you can also run &#8220;Celery Flower&#8221;, a web UI built on top of Celery,
to monitor your workers. You can use the shortcut command <code class="docutils literal"><span class="pre">airflow</span> <span class="pre">flower</span></code>
to start a Flower web server.</p>
</div>
<div class="section" id="logs">
<h2>Logs<a class="headerlink" href="#logs" title="Permalink to this headline">¶</a></h2>
<p>Users can specify a logs folder in <code class="docutils literal"><span class="pre">airflow.cfg</span></code>. By default, it is in
the <code class="docutils literal"><span class="pre">AIRFLOW_HOME</span></code> directory.</p>
<p>In addition, users can supply a remote location for storing logs and log backups
in cloud storage. At this time, Amazon S3 and Google Cloud Storage are supported.
To enable this feature, <code class="docutils literal"><span class="pre">airflow.cfg</span></code> must be configured as in this example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="o">[</span>core<span class="o">]</span>
<span class="c1"># Airflow can store logs remotely in AWS S3 or Google Cloud Storage. Users</span>
<span class="c1"># must supply a remote location URL (starting with either &#39;s3://...&#39; or</span>
<span class="c1"># &#39;gs://...&#39;) and an Airflow connection id that provides access to the storage</span>
<span class="c1"># location.</span>
<span class="nv">remote_base_log_folder</span> <span class="o">=</span> s3://my-bucket/path/to/logs
<span class="nv">remote_log_conn_id</span> <span class="o">=</span> MyS3Conn
<span class="c1"># Use server-side encryption for logs stored in S3</span>
<span class="nv">encrypt_s3_logs</span> <span class="o">=</span> False
</pre></div>
</div>
<p>Remote logging uses an existing Airflow connection to read/write logs. If you don&#8217;t
have a connection properly setup, this will fail. In the above example, Airflow will
try to use <code class="docutils literal"><span class="pre">S3Hook('MyS3Conn')</span></code>.</p>
<p>In the Airflow Web UI, local logs take precedance over remote logs. If local logs
can not be found or accessed, the remote logs will be displayed. Note that logs
are only sent to remote storage once a task completes (including failure). In other
words, remote logs for running tasks are unavailable.</p>
</div>
<div class="section" id="scaling-out-on-mesos-community-contributed">
<h2>Scaling Out on Mesos (community contributed)<a class="headerlink" href="#scaling-out-on-mesos-community-contributed" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal"><span class="pre">MesosExecutor</span></code> allows you to schedule airflow tasks on a Mesos cluster.
For this to work, you need a running mesos cluster and you must perform the following
steps -</p>
<ol class="arabic simple">
<li>Install airflow on a machine where web server and scheduler will run,
let&#8217;s refer to this as the &#8220;Airflow server&#8221;.</li>
<li>On the Airflow server, install mesos python eggs from <a class="reference external" href="http://open.mesosphere.com/downloads/mesos/">mesos downloads</a>.</li>
<li>On the Airflow server, use a database (such as mysql) which can be accessed from mesos
slave machines and add configuration in <code class="docutils literal"><span class="pre">airflow.cfg</span></code>.</li>
<li>Change your <code class="docutils literal"><span class="pre">airflow.cfg</span></code> to point executor parameter to
<cite>MesosExecutor</cite> and provide related Mesos settings.</li>
<li>On all mesos slaves, install airflow. Copy the <code class="docutils literal"><span class="pre">airflow.cfg</span></code> from
Airflow server (so that it uses same sql alchemy connection).</li>
<li>On all mesos slaves, run the following for serving logs:</li>
</ol>
<div class="highlight-bash"><div class="highlight"><pre><span></span>airflow serve_logs
</pre></div>
</div>
<ol class="arabic simple" start="7">
<li>On Airflow server, to start processing/scheduling DAGs on mesos, run:</li>
</ol>
<div class="highlight-bash"><div class="highlight"><pre><span></span>airflow scheduler -p
</pre></div>
</div>
<p>Note: We need -p parameter to pickle the DAGs.</p>
<p>You can now see the airflow framework and corresponding tasks in mesos UI.
The logs for airflow tasks can be seen in airflow UI as usual.</p>
<p>For more information about mesos, refer to <a class="reference external" href="http://mesos.apache.org/documentation/latest/">mesos documentation</a>.
For any queries/bugs on <cite>MesosExecutor</cite>, please contact <a class="reference external" href="https://github.com/kapil-malik">&#64;kapil-malik</a>.</p>
</div>
<div class="section" id="integration-with-systemd">
<h2>Integration with systemd<a class="headerlink" href="#integration-with-systemd" title="Permalink to this headline">¶</a></h2>
<p>Airflow can integrate with systemd based systems. This makes watching your
daemons easy as systemd can take care of restarting a daemon on failure.
In the <code class="docutils literal"><span class="pre">scripts/systemd</span></code> directory you can find unit files that
have been tested on Redhat based systems. You can copy those to
<code class="docutils literal"><span class="pre">/usr/lib/systemd/system</span></code>. It is assumed that Airflow will run under
<code class="docutils literal"><span class="pre">airflow:airflow</span></code>. If not (or if you are running on a non Redhat
based system) you probably need to adjust the unit files.</p>
<p>Environment configuration is picked up from <code class="docutils literal"><span class="pre">/etc/sysconfig/airflow</span></code>.
An example file is supplied. Make sure to specify the <code class="docutils literal"><span class="pre">SCHEDULER_RUNS</span></code>
variable in this file when you run the scheduler. You
can also define here, for example, <code class="docutils literal"><span class="pre">AIRFLOW_HOME</span></code> or <code class="docutils literal"><span class="pre">AIRFLOW_CONFIG</span></code>.</p>
</div>
<div class="section" id="integration-with-upstart">
<h2>Integration with upstart<a class="headerlink" href="#integration-with-upstart" title="Permalink to this headline">¶</a></h2>
<p>Airflow can integrate with upstart based systems. Upstart automatically starts all airflow services for which you
have a corresponding <code class="docutils literal"><span class="pre">*.conf</span></code> file in <code class="docutils literal"><span class="pre">/etc/init</span></code> upon system boot. On failure, upstart automatically restarts
the process (until it reaches re-spawn limit set in a <code class="docutils literal"><span class="pre">*.conf</span></code> file).</p>
<p>You can find sample upstart job files in the <code class="docutils literal"><span class="pre">scripts/upstart</span></code> directory. These files have been tested on
Ubuntu 14.04 LTS. You may have to adjust <code class="docutils literal"><span class="pre">start</span> <span class="pre">on</span></code> and <code class="docutils literal"><span class="pre">stop</span> <span class="pre">on</span></code> stanzas to make it work on other upstart
systems. Some of the possible options are listed in <code class="docutils literal"><span class="pre">scripts/upstart/README</span></code>.</p>
<p>Modify <code class="docutils literal"><span class="pre">*.conf</span></code> files as needed and copy to <code class="docutils literal"><span class="pre">/etc/init</span></code> directory. It is assumed that airflow will run
under <code class="docutils literal"><span class="pre">airflow:airflow</span></code>. Change <code class="docutils literal"><span class="pre">setuid</span></code> and <code class="docutils literal"><span class="pre">setgid</span></code> in <code class="docutils literal"><span class="pre">*.conf</span></code> files if you use other user/group</p>
<p>You can use <code class="docutils literal"><span class="pre">initctl</span></code> to manually start, stop, view status of the airflow process that has been
integrated with upstart</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>initctl airflow-webserver status
</pre></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ui.html" class="btn btn-neutral float-right" title="UI / Screenshots" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tutorial.html" class="btn btn-neutral" title="Tutorial" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>