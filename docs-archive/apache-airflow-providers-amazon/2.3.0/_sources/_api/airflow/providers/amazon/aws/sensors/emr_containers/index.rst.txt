:mod:`airflow.providers.amazon.aws.sensors.emr_containers`
==========================================================

.. py:module:: airflow.providers.amazon.aws.sensors.emr_containers


Module Contents
---------------

.. py:class:: EMRContainerSensor(*, virtual_cluster_id: str, job_id: str, max_retries: Optional[int] = None, aws_conn_id: str = 'aws_default', poll_interval: int = 10, **kwargs)

   Bases: :class:`airflow.sensors.base.BaseSensorOperator`

   Asks for the state of the job run until it reaches a failure state or success state.
   If the job run fails, the task will fail.

   :param job_id: job_id to check the state of
   :type job_id: str
   :param max_retries: Number of times to poll for query state before
       returning the current state, defaults to None
   :type max_retries: int
   :param aws_conn_id: aws connection to use, defaults to 'aws_default'
   :type aws_conn_id: str
   :param poll_interval: Time in seconds to wait between two consecutive call to
       check query status on athena, defaults to 10
   :type poll_interval: int

   .. attribute:: INTERMEDIATE_STATES
      :annotation: = ['PENDING', 'SUBMITTED', 'RUNNING']

      

   .. attribute:: FAILURE_STATES
      :annotation: = ['FAILED', 'CANCELLED', 'CANCEL_PENDING']

      

   .. attribute:: SUCCESS_STATES
      :annotation: = ['COMPLETED']

      

   .. attribute:: template_fields
      :annotation: = ['virtual_cluster_id', 'job_id']

      

   .. attribute:: template_ext
      :annotation: = []

      

   .. attribute:: ui_color
      :annotation: = #66c3ff

      

   
   .. method:: poke(self, context: dict)



   
   .. method:: hook(self)

      Create and return an EMRContainerHook




