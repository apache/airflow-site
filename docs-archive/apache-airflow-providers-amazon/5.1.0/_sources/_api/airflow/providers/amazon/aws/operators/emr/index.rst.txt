:py:mod:`airflow.providers.amazon.aws.operators.emr`
====================================================

.. py:module:: airflow.providers.amazon.aws.operators.emr


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.amazon.aws.operators.emr.EmrAddStepsOperator
   airflow.providers.amazon.aws.operators.emr.EmrEksCreateClusterOperator
   airflow.providers.amazon.aws.operators.emr.EmrContainerOperator
   airflow.providers.amazon.aws.operators.emr.EmrCreateJobFlowOperator
   airflow.providers.amazon.aws.operators.emr.EmrModifyClusterOperator
   airflow.providers.amazon.aws.operators.emr.EmrTerminateJobFlowOperator
   airflow.providers.amazon.aws.operators.emr.EmrServerlessCreateApplicationOperator
   airflow.providers.amazon.aws.operators.emr.EmrServerlessStartJobOperator
   airflow.providers.amazon.aws.operators.emr.EmrServerlessDeleteApplicationOperator




.. py:class:: EmrAddStepsOperator(*, job_flow_id = None, job_flow_name = None, cluster_states = None, aws_conn_id = 'aws_default', steps = None, **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   An operator that adds steps to an existing EMR job_flow.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrAddStepsOperator`

   :param job_flow_id: id of the JobFlow to add steps to. (templated)
   :param job_flow_name: name of the JobFlow to add steps to. Use as an alternative to passing
       job_flow_id. will search for id of JobFlow with matching name in one of the states in
       param cluster_states. Exactly one cluster like this should exist or will fail. (templated)
   :param cluster_states: Acceptable cluster states when searching for JobFlow id by job_flow_name.
       (templated)
   :param aws_conn_id: aws connection to uses
   :param steps: boto3 style steps or reference to a steps file (must be '.json') to
       be added to the jobflow. (templated)
   :param do_xcom_push: if True, job_flow_id is pushed to XCom with key job_flow_id.

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['job_flow_id', 'job_flow_name', 'cluster_states', 'steps']

      

   .. py:attribute:: template_ext
      :annotation: :Sequence[str] = ['.json']

      

   .. py:attribute:: template_fields_renderers
      

      

   .. py:attribute:: ui_color
      :annotation: = #f9c915

      

   .. py:attribute:: operator_extra_links
      

      

   .. py:method:: execute(context)

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



.. py:class:: EmrEksCreateClusterOperator(*, virtual_cluster_name, eks_cluster_name, eks_namespace, virtual_cluster_id = '', aws_conn_id = 'aws_default', tags = None, **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   An operator that creates EMR on EKS virtual clusters.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrEksCreateClusterOperator`

   :param virtual_cluster_name: The name of the EMR EKS virtual cluster to create.
   :param eks_cluster_name: The EKS cluster used by the EMR virtual cluster.
   :param eks_namespace: namespace used by the EKS cluster.
   :param virtual_cluster_id: The EMR on EKS virtual cluster id.
   :param aws_conn_id: The Airflow connection used for AWS credentials.
   :param tags: The tags assigned to created cluster.
       Defaults to None

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['virtual_cluster_name', 'eks_cluster_name', 'eks_namespace']

      

   .. py:attribute:: ui_color
      :annotation: = #f9c915

      

   .. py:method:: hook()

      Create and return an EmrContainerHook.


   .. py:method:: execute(context)

      Create EMR on EKS virtual Cluster



.. py:class:: EmrContainerOperator(*, name, virtual_cluster_id, execution_role_arn, release_label, job_driver, configuration_overrides = None, client_request_token = None, aws_conn_id = 'aws_default', wait_for_completion = True, poll_interval = 30, max_tries = None, tags = None, **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   An operator that submits jobs to EMR on EKS virtual clusters.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrContainerOperator`

   :param name: The name of the job run.
   :param virtual_cluster_id: The EMR on EKS virtual cluster ID
   :param execution_role_arn: The IAM role ARN associated with the job run.
   :param release_label: The Amazon EMR release version to use for the job run.
   :param job_driver: Job configuration details, e.g. the Spark job parameters.
   :param configuration_overrides: The configuration overrides for the job run,
       specifically either application configuration or monitoring configuration.
   :param client_request_token: The client idempotency token of the job run request.
       Use this if you want to specify a unique ID to prevent two jobs from getting started.
       If no token is provided, a UUIDv4 token will be generated for you.
   :param aws_conn_id: The Airflow connection used for AWS credentials.
   :param wait_for_completion: Whether or not to wait in the operator for the job to complete.
   :param poll_interval: Time (in seconds) to wait between two consecutive calls to check query status on EMR
   :param max_tries: Maximum number of times to wait for the job run to finish.
       Defaults to None, which will poll until the job is *not* in a pending, submitted, or running state.
   :param tags: The tags assigned to job runs.
       Defaults to None

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['name', 'virtual_cluster_id', 'execution_role_arn', 'release_label', 'job_driver']

      

   .. py:attribute:: ui_color
      :annotation: = #f9c915

      

   .. py:method:: hook()

      Create and return an EmrContainerHook.


   .. py:method:: execute(context)

      Run job on EMR Containers


   .. py:method:: on_kill()

      Cancel the submitted job run



.. py:class:: EmrCreateJobFlowOperator(*, aws_conn_id = 'aws_default', emr_conn_id = 'emr_default', job_flow_overrides = None, region_name = None, **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   Creates an EMR JobFlow, reading the config from the EMR connection.
   A dictionary of JobFlow overrides can be passed that override
   the config from the connection.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrCreateJobFlowOperator`

   :param aws_conn_id: The Airflow connection used for AWS credentials.
       If this is None or empty then the default boto3 behaviour is used. If
       running Airflow in a distributed manner and aws_conn_id is None or
       empty, then default boto3 configuration would be used (and must be
       maintained on each worker node)
   :param emr_conn_id: emr connection to use for run_job_flow request body.
       This will be overridden by the job_flow_overrides param
   :param job_flow_overrides: boto3 style arguments or reference to an arguments file
       (must be '.json') to override emr_connection extra. (templated)
   :param region_name: Region named passed to EmrHook

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['job_flow_overrides']

      

   .. py:attribute:: template_ext
      :annotation: :Sequence[str] = ['.json']

      

   .. py:attribute:: template_fields_renderers
      

      

   .. py:attribute:: ui_color
      :annotation: = #f9c915

      

   .. py:attribute:: operator_extra_links
      

      

   .. py:method:: execute(context)

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



.. py:class:: EmrModifyClusterOperator(*, cluster_id, step_concurrency_level, aws_conn_id = 'aws_default', **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   An operator that modifies an existing EMR cluster.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrModifyClusterOperator`

   :param cluster_id: cluster identifier
   :param step_concurrency_level: Concurrency of the cluster
   :param aws_conn_id: aws connection to uses
   :param do_xcom_push: if True, cluster_id is pushed to XCom with key cluster_id.

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['cluster_id', 'step_concurrency_level']

      

   .. py:attribute:: template_ext
      :annotation: :Sequence[str] = []

      

   .. py:attribute:: ui_color
      :annotation: = #f9c915

      

   .. py:attribute:: operator_extra_links
      

      

   .. py:method:: execute(context)

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



.. py:class:: EmrTerminateJobFlowOperator(*, job_flow_id, aws_conn_id = 'aws_default', **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   Operator to terminate EMR JobFlows.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrTerminateJobFlowOperator`

   :param job_flow_id: id of the JobFlow to terminate. (templated)
   :param aws_conn_id: aws connection to uses

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['job_flow_id']

      

   .. py:attribute:: template_ext
      :annotation: :Sequence[str] = []

      

   .. py:attribute:: ui_color
      :annotation: = #f9c915

      

   .. py:attribute:: operator_extra_links
      

      

   .. py:method:: execute(context)

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



.. py:class:: EmrServerlessCreateApplicationOperator(release_label, job_type, client_request_token = '', config = None, wait_for_completion = True, aws_conn_id = 'aws_default', **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   Operator to create Serverless EMR Application

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrServerlessCreateApplicationOperator`

   :param release_label: The EMR release version associated with the application.
   :param job_type: The type of application you want to start, such as Spark or Hive.
   :param wait_for_completion: If true, wait for the Application to start before returning. Default to True
   :param client_request_token: The client idempotency token of the application to create.
     Its value must be unique for each request.
   :param config: Optional dictionary for arbitrary parameters to the boto API create_application call.
   :param aws_conn_id: AWS connection to use

   .. py:method:: hook()

      Create and return an EmrServerlessHook.


   .. py:method:: execute(context)

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



.. py:class:: EmrServerlessStartJobOperator(application_id, execution_role_arn, job_driver, configuration_overrides, client_request_token = '', config = None, wait_for_completion = True, aws_conn_id = 'aws_default', **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   Operator to start EMR Serverless job.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrServerlessStartJobOperator`

   :param application_id: ID of the EMR Serverless application to start.
   :param execution_role_arn: ARN of role to perform action.
   :param job_driver: Driver that the job runs on.
   :param configuration_overrides: Configuration specifications to override existing configurations.
   :param client_request_token: The client idempotency token of the application to create.
     Its value must be unique for each request.
   :param config: Optional dictionary for arbitrary parameters to the boto API start_job_run call.
   :param wait_for_completion: If true, waits for the job to start before returning. Defaults to True.
   :param aws_conn_id: AWS connection to use

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['application_id', 'execution_role_arn', 'job_driver', 'configuration_overrides']

      

   .. py:method:: hook()

      Create and return an EmrServerlessHook.


   .. py:method:: execute(context)

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



.. py:class:: EmrServerlessDeleteApplicationOperator(application_id, wait_for_completion = True, aws_conn_id = 'aws_default', **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   Operator to delete EMR Serverless application

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:EmrServerlessDeleteApplicationOperator`

   :param application_id: ID of the EMR Serverless application to delete.
   :param wait_for_completion: If true, wait for the Application to start before returning. Default to True
   :param aws_conn_id: AWS connection to use

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['application_id']

      

   .. py:method:: hook()

      Create and return an EmrServerlessHook.


   .. py:method:: execute(context)

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



