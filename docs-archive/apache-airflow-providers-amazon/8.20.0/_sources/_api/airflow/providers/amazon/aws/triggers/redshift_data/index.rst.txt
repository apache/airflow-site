:py:mod:`airflow.providers.amazon.aws.triggers.redshift_data`
=============================================================

.. py:module:: airflow.providers.amazon.aws.triggers.redshift_data


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.amazon.aws.triggers.redshift_data.RedshiftDataTrigger




.. py:class:: RedshiftDataTrigger(statement_id, task_id, poll_interval, aws_conn_id = 'aws_default', region_name = None, verify = None, botocore_config = None)


   Bases: :py:obj:`airflow.triggers.base.BaseTrigger`

   RedshiftDataTrigger is fired as deferred class with params to run the task in triggerer.

   :param statement_id: the UUID of the statement
   :param task_id: task ID of the Dag
   :param poll_interval:  polling period in seconds to check for the status
   :param aws_conn_id: AWS connection ID for redshift
   :param region_name: aws region to use

   .. py:method:: serialize()

      Serialize RedshiftDataTrigger arguments and classpath.


   .. py:method:: hook()


   .. py:method:: run()
      :async:

      Run the trigger in an asynchronous context.

      The trigger should yield an Event whenever it wants to fire off
      an event, and return None if it is finished. Single-event triggers
      should thus yield and then immediately return.

      If it yields, it is likely that it will be resumed very quickly,
      but it may not be (e.g. if the workload is being moved to another
      triggerer process, or a multi-event trigger was being used for a
      single-event task defer).

      In either case, Trigger classes should assume they will be persisted,
      and then rely on cleanup() being called when they are no longer needed.



