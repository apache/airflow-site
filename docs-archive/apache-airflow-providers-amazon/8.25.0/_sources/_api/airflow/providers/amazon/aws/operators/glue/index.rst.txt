:py:mod:`airflow.providers.amazon.aws.operators.glue`
=====================================================

.. py:module:: airflow.providers.amazon.aws.operators.glue


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.amazon.aws.operators.glue.GlueJobOperator
   airflow.providers.amazon.aws.operators.glue.GlueDataQualityOperator
   airflow.providers.amazon.aws.operators.glue.GlueDataQualityRuleSetEvaluationRunOperator
   airflow.providers.amazon.aws.operators.glue.GlueDataQualityRuleRecommendationRunOperator




.. py:class:: GlueJobOperator(*, job_name = 'aws_glue_default_job', job_desc = 'AWS Glue Job with Airflow', script_location = None, concurrent_run_limit = None, script_args = None, retry_limit = 0, num_of_dpus = None, aws_conn_id = 'aws_default', region_name = None, s3_bucket = None, iam_role_name = None, iam_role_arn = None, create_job_kwargs = None, run_job_kwargs = None, wait_for_completion = True, deferrable = conf.getboolean('operators', 'default_deferrable', fallback=False), verbose = False, replace_script_file = False, update_config = False, job_poll_interval = 6, stop_job_run_on_kill = False, **kwargs)


   Bases: :py:obj:`airflow.models.BaseOperator`

   Create an AWS Glue Job.

   AWS Glue is a serverless Spark ETL service for running Spark Jobs on the AWS
   cloud. Language support: Python and Scala.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:GlueJobOperator`

   :param job_name: unique job name per AWS Account
   :param script_location: location of ETL script. Must be a local or S3 path
   :param job_desc: job description details
   :param concurrent_run_limit: The maximum number of concurrent runs allowed for a job
   :param script_args: etl script arguments and AWS Glue arguments (templated)
   :param retry_limit: The maximum number of times to retry this job if it fails
   :param num_of_dpus: Number of AWS Glue DPUs to allocate to this Job.
   :param region_name: aws region name (example: us-east-1)
   :param s3_bucket: S3 bucket where logs and local etl script will be uploaded
   :param iam_role_name: AWS IAM Role for Glue Job Execution. If set `iam_role_arn` must equal None.
   :param iam_role_arn: AWS IAM ARN for Glue Job Execution. If set `iam_role_name` must equal None.
   :param create_job_kwargs: Extra arguments for Glue Job Creation
   :param run_job_kwargs: Extra arguments for Glue Job Run
   :param wait_for_completion: Whether to wait for job run completion. (default: True)
   :param deferrable: If True, the operator will wait asynchronously for the job to complete.
       This implies waiting for completion. This mode requires aiobotocore module to be installed.
       (default: False)
   :param verbose: If True, Glue Job Run logs show in the Airflow Task Logs.  (default: False)
   :param update_config: If True, Operator will update job configuration.  (default: False)
   :param replace_script_file: If True, the script file will be replaced in S3. (default: False)
   :param stop_job_run_on_kill: If True, Operator will stop the job run when task is killed.

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('job_name', 'script_location', 'script_args', 'create_job_kwargs', 's3_bucket',...

      

   .. py:attribute:: template_ext
      :type: Sequence[str]
      :value: ()

      

   .. py:attribute:: template_fields_renderers

      

   .. py:attribute:: ui_color
      :value: '#ededed'

      

   .. py:attribute:: operator_extra_links
      :value: ()

      

   .. py:method:: glue_job_hook()


   .. py:method:: execute(context)

      Execute AWS Glue Job from Airflow.

      :return: the current Glue job ID.


   .. py:method:: execute_complete(context, event = None)


   .. py:method:: on_kill()

      Cancel the running AWS Glue Job.



.. py:class:: GlueDataQualityOperator(*, name, ruleset, description = 'AWS Glue Data Quality Rule Set With Airflow', update_rule_set = False, data_quality_ruleset_kwargs = None, aws_conn_id = 'aws_default', **kwargs)


   Bases: :py:obj:`airflow.providers.amazon.aws.operators.base_aws.AwsBaseOperator`\ [\ :py:obj:`airflow.providers.amazon.aws.hooks.glue.GlueDataQualityHook`\ ]

   Creates a data quality ruleset with DQDL rules applied to a specified Glue table.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:GlueDataQualityOperator`

   :param name: A unique name for the data quality ruleset.
   :param ruleset: A Data Quality Definition Language (DQDL) ruleset.
       For more information, see the Glue developer guide.
   :param description: A description of the data quality ruleset.
   :param update_rule_set: To update existing ruleset, Set this flag to True. (default: False)
   :param data_quality_ruleset_kwargs: Extra arguments for RuleSet.

   :param aws_conn_id: The Airflow connection used for AWS credentials.
       If this is ``None`` or empty then the default boto3 behaviour is used. If
       running Airflow in a distributed manner and aws_conn_id is None or
       empty, then default boto3 configuration would be used (and must be
       maintained on each worker node).
   :param region_name: AWS region_name. If not specified then the default boto3 behaviour is used.
   :param verify: Whether or not to verify SSL certificates. See:
       https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html
   :param botocore_config: Configuration dictionary (key-values) for botocore client. See:
       https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html

   .. py:attribute:: aws_hook_class

      

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('name', 'ruleset', 'description', 'data_quality_ruleset_kwargs')

      

   .. py:attribute:: template_fields_renderers

      

   .. py:attribute:: ui_color
      :value: '#ededed'

      

   .. py:method:: validate_inputs()


   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



.. py:class:: GlueDataQualityRuleSetEvaluationRunOperator(*, datasource, role, rule_set_names, number_of_workers = 5, timeout = 2880, verify_result_status = True, show_results = True, rule_set_evaluation_run_kwargs = None, wait_for_completion = True, waiter_delay = 60, waiter_max_attempts = 20, deferrable = conf.getboolean('operators', 'default_deferrable', fallback=False), aws_conn_id = 'aws_default', **kwargs)


   Bases: :py:obj:`airflow.providers.amazon.aws.operators.base_aws.AwsBaseOperator`\ [\ :py:obj:`airflow.providers.amazon.aws.hooks.glue.GlueDataQualityHook`\ ]

   Evaluate a ruleset against a data source (Glue table).

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:GlueDataQualityRuleSetEvaluationRunOperator`

   :param datasource: The data source (Glue table) associated with this run. (templated)
   :param role: IAM role supplied for job execution. (templated)
   :param rule_set_names: A list of ruleset names for evaluation. (templated)
   :param number_of_workers: The number of G.1X workers to be used in the run. (default: 5)
   :param timeout: The timeout for a run in minutes. This is the maximum time that a run can consume resources
       before it is terminated and enters TIMEOUT status. (default: 2,880)
   :param verify_result_status: Validate all the ruleset rules evaluation run results,
       If any of the rule status is Fail or Error then an exception is thrown. (default: True)
   :param show_results: Displays all the ruleset rules evaluation run results. (default: True)
   :param rule_set_evaluation_run_kwargs: Extra arguments for evaluation run. (templated)
   :param wait_for_completion: Whether to wait for job to stop. (default: True)
   :param waiter_delay: Time in seconds to wait between status checks. (default: 60)
   :param waiter_max_attempts: Maximum number of attempts to check for job completion. (default: 20)
   :param deferrable: If True, the operator will wait asynchronously for the job to stop.
       This implies waiting for completion. This mode requires aiobotocore module to be installed.
       (default: False)

   :param aws_conn_id: The Airflow connection used for AWS credentials.
       If this is ``None`` or empty then the default boto3 behaviour is used. If
       running Airflow in a distributed manner and aws_conn_id is None or
       empty, then default boto3 configuration would be used (and must be
       maintained on each worker node).
   :param region_name: AWS region_name. If not specified then the default boto3 behaviour is used.
   :param verify: Whether or not to verify SSL certificates. See:
       https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html
   :param botocore_config: Configuration dictionary (key-values) for botocore client. See:
       https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html

   .. py:attribute:: aws_hook_class

      

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('datasource', 'role', 'rule_set_names', 'rule_set_evaluation_run_kwargs')

      

   .. py:attribute:: template_fields_renderers

      

   .. py:attribute:: ui_color
      :value: '#ededed'

      

   .. py:method:: validate_inputs()


   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.


   .. py:method:: execute_complete(context, event = None)



.. py:class:: GlueDataQualityRuleRecommendationRunOperator(*, datasource, role, number_of_workers = 5, timeout = 2880, show_results = True, recommendation_run_kwargs = None, wait_for_completion = True, waiter_delay = 60, waiter_max_attempts = 20, deferrable = conf.getboolean('operators', 'default_deferrable', fallback=False), aws_conn_id = 'aws_default', **kwargs)


   Bases: :py:obj:`airflow.providers.amazon.aws.operators.base_aws.AwsBaseOperator`\ [\ :py:obj:`airflow.providers.amazon.aws.hooks.glue.GlueDataQualityHook`\ ]

   Starts a recommendation run that is used to generate rules, Glue Data Quality analyzes the data and comes up with recommendations for a potential ruleset.

   Recommendation runs are automatically deleted after 90 days.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:GlueDataQualityRuleRecommendationRunOperator`

   :param datasource: The data source (Glue table) associated with this run. (templated)
   :param role: IAM role supplied for job execution. (templated)
   :param number_of_workers: The number of G.1X workers to be used in the run. (default: 5)
   :param timeout: The timeout for a run in minutes. This is the maximum time that a run can consume resources
       before it is terminated and enters TIMEOUT status. (default: 2,880)
   :param show_results: Displays the recommended ruleset (a set of rules), when recommendation run completes. (default: True)
   :param recommendation_run_kwargs: Extra arguments for recommendation run. (templated)
   :param wait_for_completion: Whether to wait for job to stop. (default: True)
   :param waiter_delay: Time in seconds to wait between status checks. (default: 60)
   :param waiter_max_attempts: Maximum number of attempts to check for job completion. (default: 20)
   :param deferrable: If True, the operator will wait asynchronously for the job to stop.
       This implies waiting for completion. This mode requires aiobotocore module to be installed.
       (default: False)

   :param aws_conn_id: The Airflow connection used for AWS credentials.
       If this is ``None`` or empty then the default boto3 behaviour is used. If
       running Airflow in a distributed manner and aws_conn_id is None or
       empty, then default boto3 configuration would be used (and must be
       maintained on each worker node).
   :param region_name: AWS region_name. If not specified then the default boto3 behaviour is used.
   :param verify: Whether or not to verify SSL certificates. See:
       https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html
   :param botocore_config: Configuration dictionary (key-values) for botocore client. See:
       https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html

   .. py:attribute:: aws_hook_class

      

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('datasource', 'role', 'recommendation_run_kwargs')

      

   .. py:attribute:: template_fields_renderers

      

   .. py:attribute:: ui_color
      :value: '#ededed'

      

   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.


   .. py:method:: execute_complete(context, event = None)



