airflow.providers.amazon.aws.sensors.mwaa
=========================================

.. py:module:: airflow.providers.amazon.aws.sensors.mwaa


Classes
-------

.. autoapisummary::

   airflow.providers.amazon.aws.sensors.mwaa.MwaaDagRunSensor


Module Contents
---------------

.. py:class:: MwaaDagRunSensor(*, external_env_name, external_dag_id, external_dag_run_id, success_states = None, failure_states = None, deferrable = conf.getboolean('operators', 'default_deferrable', fallback=False), poke_interval = 60, max_retries = 720, **kwargs)

   Bases: :py:obj:`airflow.providers.amazon.aws.sensors.base_aws.AwsBaseSensor`\ [\ :py:obj:`airflow.providers.amazon.aws.hooks.mwaa.MwaaHook`\ ]


   Waits for a DAG Run in an MWAA Environment to complete.

   If the DAG Run fails, an AirflowException is thrown.

   .. seealso::
       For more information on how to use this sensor, take a look at the guide:
       :ref:`howto/sensor:MwaaDagRunSensor`

   :param external_env_name: The external MWAA environment name that contains the DAG Run you want to wait for
       (templated)
   :param external_dag_id: The DAG ID in the external MWAA environment that contains the DAG Run you want to wait for
       (templated)
   :param external_dag_run_id: The DAG Run ID in the external MWAA environment that you want to wait for (templated)
   :param success_states: Collection of DAG Run states that would make this task marked as successful, default is
       ``{airflow.utils.state.DagRunState.SUCCESS}`` (templated)
   :param failure_states: Collection of DAG Run states that would make this task marked as failed and raise an
       AirflowException, default is ``{airflow.utils.state.DagRunState.FAILED}`` (templated)
   :param deferrable: If True, the sensor will operate in deferrable mode. This mode requires aiobotocore
       module to be installed.
       (default: False, but can be overridden in config file by setting default_deferrable to True)
   :param poke_interval: Polling period in seconds to check for the status of the job. (default: 60)
   :param max_retries: Number of times before returning the current state. (default: 720)
   :param aws_conn_id: The Airflow connection used for AWS credentials.
       If this is ``None`` or empty then the default boto3 behaviour is used. If
       running Airflow in a distributed manner and aws_conn_id is None or
       empty, then default boto3 configuration would be used (and must be
       maintained on each worker node).
   :param region_name: AWS region_name. If not specified then the default boto3 behaviour is used.
   :param verify: Whether or not to verify SSL certificates. See:
       https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html
   :param botocore_config: Configuration dictionary (key-values) for botocore client. See:
       https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html


   .. py:attribute:: aws_hook_class


   .. py:attribute:: template_fields
      :type:  collections.abc.Sequence[str]


   .. py:attribute:: success_states


   .. py:attribute:: failure_states


   .. py:attribute:: external_env_name


   .. py:attribute:: external_dag_id


   .. py:attribute:: external_dag_run_id


   .. py:attribute:: deferrable
      :value: True



   .. py:attribute:: poke_interval
      :value: 60



   .. py:attribute:: max_retries
      :value: 720



   .. py:method:: poke(context)

      Override when deriving this class.



   .. py:method:: execute_complete(context, event = None)


   .. py:method:: execute(context)

      Derive when creating an operator.

      The main method to execute the task. Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



