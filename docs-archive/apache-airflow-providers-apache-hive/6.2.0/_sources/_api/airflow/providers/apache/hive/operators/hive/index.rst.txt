:py:mod:`airflow.providers.apache.hive.operators.hive`
======================================================

.. py:module:: airflow.providers.apache.hive.operators.hive


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.apache.hive.operators.hive.HiveOperator




.. py:class:: HiveOperator(*, hql, hive_cli_conn_id = 'hive_cli_default', schema = 'default', hiveconfs = None, hiveconf_jinja_translate = False, script_begin_tag = None, run_as_owner = False, mapred_queue = None, mapred_queue_priority = None, mapred_job_name = None, hive_cli_params = '', auth = None, **kwargs)


   Bases: :py:obj:`airflow.models.BaseOperator`

   Executes hql code or hive script in a specific Hive database.

   :param hql: the hql to be executed. Note that you may also use
       a relative path from the dag file of a (template) hive
       script. (templated)
   :param hive_cli_conn_id: Reference to the
       :ref:`Hive CLI connection id <howto/connection:hive_cli>`. (templated)
   :param hiveconfs: if defined, these key value pairs will be passed
       to hive as ``-hiveconf "key"="value"``
   :param hiveconf_jinja_translate: when True, hiveconf-type templating
       ${var} gets translated into jinja-type templating {{ var }} and
       ${hiveconf:var} gets translated into jinja-type templating {{ var }}.
       Note that you may want to use this along with the
       ``DAG(user_defined_macros=myargs)`` parameter. View the DAG
       object documentation for more details.
   :param script_begin_tag: If defined, the operator will get rid of the
       part of the script before the first occurrence of `script_begin_tag`
   :param run_as_owner: Run HQL code as a DAG's owner.
   :param mapred_queue: queue used by the Hadoop CapacityScheduler. (templated)
   :param mapred_queue_priority: priority within CapacityScheduler queue.
       Possible settings include: VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW
   :param mapred_job_name: This name will appear in the jobtracker.
       This can make monitoring easier.
   :param hive_cli_params: parameters passed to hive CLO
   :param auth: optional authentication option passed for the Hive connection

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('hql', 'schema', 'hive_cli_conn_id', 'mapred_queue', 'hiveconfs', 'mapred_job_name',...

      

   .. py:attribute:: template_ext
      :type: Sequence[str]
      :value: ('.hql', '.sql')

      

   .. py:attribute:: template_fields_renderers

      

   .. py:attribute:: ui_color
      :value: '#f0e4ec'

      

   .. py:method:: hook()

      Get Hive cli hook.


   .. py:method:: get_hook()

      Get Hive cli hook.


   .. py:method:: prepare_template()

      Execute after the templated fields get replaced by their content.

      If you need your object to alter the content of the file before the
      template is rendered, it should override this method to do so.


   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.


   .. py:method:: dry_run()

      Perform dry run for the operator - just render template fields.


   .. py:method:: on_kill()

      Override this method to clean up subprocesses when a task instance gets killed.

      Any use of the threading, subprocess or multiprocessing module within an
      operator needs to be cleaned up, or it will leave ghost processes behind.


   .. py:method:: clear_airflow_vars()

      Reset airflow environment variables to prevent existing ones from impacting behavior.



