:py:mod:`airflow.providers.apache.pinot.hooks.pinot`
====================================================

.. py:module:: airflow.providers.apache.pinot.hooks.pinot


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.apache.pinot.hooks.pinot.PinotAdminHook
   airflow.providers.apache.pinot.hooks.pinot.PinotDbApiHook




.. py:class:: PinotAdminHook(conn_id = 'pinot_admin_default', cmd_path = 'pinot-admin.sh', pinot_admin_system_exit = False)


   Bases: :py:obj:`airflow.hooks.base.BaseHook`

   This hook is a wrapper around the pinot-admin.sh script.

   For now, only small subset of its subcommands are implemented,
   which are required to ingest offline data into Apache Pinot
   (i.e., AddSchema, AddTable, CreateSegment, and UploadSegment).
   Their command options are based on Pinot v0.1.0.

   Unfortunately, as of v0.1.0, pinot-admin.sh always exits with
   status code 0. To address this behavior, users can use the
   pinot_admin_system_exit flag. If its value is set to false,
   this hook evaluates the result based on the output message
   instead of the status code. This Pinot's behavior is supposed
   to be improved in the next release, which will include the
   following PR: https://github.com/apache/incubator-pinot/pull/4110

   :param conn_id: The name of the connection to use.
   :param cmd_path: Do not modify the parameter. It used to be the filepath to the pinot-admin.sh
          executable but in version 4.0.0 of apache-pinot provider, value of this parameter must
          remain the default value: `pinot-admin.sh`. It is left here to not accidentally override
          the `pinot_admin_system_exit` in case positional parameters were used to initialize the hook.
   :param pinot_admin_system_exit: If true, the result is evaluated based on the status code.
                                   Otherwise, the result is evaluated as a failure if "Error" or
                                   "Exception" is in the output message.

   .. py:attribute:: conn_name_attr
      :value: 'conn_id'

      

   .. py:attribute:: default_conn_name
      :value: 'pinot_admin_default'

      

   .. py:attribute:: conn_type
      :value: 'pinot_admin'

      

   .. py:attribute:: hook_name
      :value: 'Pinot Admin'

      

   .. py:method:: get_conn()

      Return connection for the hook.


   .. py:method:: add_schema(schema_file, with_exec = True)

      Add Pinot schema by run AddSchema command.

      :param schema_file: Pinot schema file
      :param with_exec: bool


   .. py:method:: add_table(file_path, with_exec = True)

      Add Pinot table with run AddTable command.

      :param file_path: Pinot table configure file
      :param with_exec: bool


   .. py:method:: create_segment(generator_config_file = None, data_dir = None, segment_format = None, out_dir = None, overwrite = None, table_name = None, segment_name = None, time_column_name = None, schema_file = None, reader_config_file = None, enable_star_tree_index = None, star_tree_index_spec_file = None, hll_size = None, hll_columns = None, hll_suffix = None, num_threads = None, post_creation_verification = None, retry = None)

      Create Pinot segment by run CreateSegment command.


   .. py:method:: upload_segment(segment_dir, table_name = None)

      Upload Segment with run UploadSegment command.

      :param segment_dir:
      :param table_name:
      :return:


   .. py:method:: run_cli(cmd, verbose = True)

      Run command with pinot-admin.sh.

      :param cmd: List of command going to be run by pinot-admin.sh script
      :param verbose:



.. py:class:: PinotDbApiHook(*args, schema = None, log_sql = True, **kwargs)


   Bases: :py:obj:`airflow.providers.common.sql.hooks.sql.DbApiHook`

   Interact with Pinot Broker Query API.

   This hook uses standard-SQL endpoint since PQL endpoint is soon to be deprecated.
   https://docs.pinot.apache.org/users/api/querying-pinot-using-standard-sql

   .. py:attribute:: conn_name_attr
      :value: 'pinot_broker_conn_id'

      

   .. py:attribute:: default_conn_name
      :value: 'pinot_broker_default'

      

   .. py:attribute:: conn_type
      :value: 'pinot'

      

   .. py:attribute:: hook_name
      :value: 'Pinot Broker'

      

   .. py:attribute:: supports_autocommit
      :value: False

      

   .. py:method:: get_conn()

      Establish a connection to pinot broker through pinot dbapi.


   .. py:method:: get_uri()

      Get the connection uri for pinot broker.

      e.g: http://localhost:9000/query/sql


   .. py:method:: get_records(sql, parameters = None, **kwargs)

      Execute the sql and returns a set of records.

      :param sql: the sql statement to be executed (str) or a list of
          sql statements to execute
      :param parameters: The parameters to render the SQL query with.


   .. py:method:: get_first(sql, parameters = None)

      Execute the sql and returns the first resulting row.

      :param sql: the sql statement to be executed (str) or a list of
          sql statements to execute
      :param parameters: The parameters to render the SQL query with.


   .. py:method:: set_autocommit(conn, autocommit)
      :abstractmethod:

      Set the autocommit flag on the connection.


   .. py:method:: insert_rows(table, rows, target_fields = None, commit_every = 1000, replace = False, **kwargs)
      :abstractmethod:

      Insert a collection of tuples into a table.

      Rows are inserted in chunks, each chunk (of size ``commit_every``) is
      done in a new transaction.

      :param table: Name of the target table
      :param rows: The rows to insert into the table
      :param target_fields: The names of the columns to fill in the table
      :param commit_every: The maximum number of rows to insert in one
          transaction. Set to 0 to insert all rows in one transaction.
      :param replace: Whether to replace instead of insert
      :param executemany: (Deprecated) If True, all rows are inserted at once in
          chunks defined by the commit_every parameter. This only works if all rows
          have same number of column names, but leads to better performance.



