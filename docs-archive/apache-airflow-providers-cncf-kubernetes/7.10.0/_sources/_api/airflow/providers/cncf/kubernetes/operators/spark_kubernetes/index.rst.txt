:py:mod:`airflow.providers.cncf.kubernetes.operators.spark_kubernetes`
======================================================================

.. py:module:: airflow.providers.cncf.kubernetes.operators.spark_kubernetes


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.cncf.kubernetes.operators.spark_kubernetes.SparkKubernetesOperator




.. py:class:: SparkKubernetesOperator(*, application_file, namespace = None, kubernetes_conn_id = 'kubernetes_default', api_group = 'sparkoperator.k8s.io', api_version = 'v1beta2', in_cluster = None, cluster_context = None, config_file = None, watch = False, **kwargs)


   Bases: :py:obj:`airflow.models.BaseOperator`

   Creates sparkApplication object in kubernetes cluster.

   .. seealso::
       For more detail about Spark Application Object have a look at the reference:
       https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/v1beta2-1.1.0-2.4.5/docs/api-docs.md#sparkapplication

   :param application_file: Defines Kubernetes 'custom_resource_definition' of 'sparkApplication' as either a
       path to a '.yaml' file, '.json' file, YAML string or JSON string.
   :param namespace: kubernetes namespace to put sparkApplication
   :param kubernetes_conn_id: The :ref:`kubernetes connection id <howto/connection:kubernetes>`
       for the to Kubernetes cluster.
   :param api_group: kubernetes api group of sparkApplication
   :param api_version: kubernetes api version of sparkApplication
   :param watch: whether to watch the job status and logs or not

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('application_file', 'namespace')

      

   .. py:attribute:: template_ext
      :type: Sequence[str]
      :value: ('.yaml', '.yml', '.json')

      

   .. py:attribute:: ui_color
      :value: '#f4a460'

      

   .. py:method:: hook()


   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.


   .. py:method:: on_kill()

      Override this method to clean up subprocesses when a task instance gets killed.

      Any use of the threading, subprocess or multiprocessing module within an
      operator needs to be cleaned up, or it will leave ghost processes behind.



