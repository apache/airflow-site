:py:mod:`airflow.providers.google.cloud.hooks.vertex_ai.generative_model`
=========================================================================

.. py:module:: airflow.providers.google.cloud.hooks.vertex_ai.generative_model

.. autoapi-nested-parse::

   This module contains a Google Cloud Vertex AI Generative Model hook.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.google.cloud.hooks.vertex_ai.generative_model.GenerativeModelHook




.. py:class:: GenerativeModelHook(gcp_conn_id = 'google_cloud_default', impersonation_chain = None, **kwargs)


   Bases: :py:obj:`airflow.providers.google.common.hooks.base_google.GoogleBaseHook`

   Hook for Google Cloud Vertex AI Generative Model APIs.

   .. py:method:: get_text_generation_model(pretrained_model)

      Return a Model Garden Model object based on Text Generation.


   .. py:method:: get_text_embedding_model(pretrained_model)

      Return a Model Garden Model object based on Text Embedding.


   .. py:method:: get_generative_model(pretrained_model, system_instruction = None, generation_config = None, safety_settings = None, tools = None)

      Return a Generative Model object.


   .. py:method:: get_eval_task(dataset, metrics, experiment)

      Return an EvalTask object.


   .. py:method:: get_generative_model_part(content_gcs_path, content_mime_type = None)

      Return a Generative Model Part object.


   .. py:method:: prompt_language_model(prompt, pretrained_model, temperature, max_output_tokens, top_p, top_k, location, project_id = PROVIDE_PROJECT_ID)

      Use the Vertex AI PaLM API to generate natural language text.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param prompt: Required. Inputs or queries that a user or a program gives
          to the Vertex AI PaLM API, in order to elicit a specific response.
      :param pretrained_model: A pre-trained model optimized for performing natural
          language tasks such as classification, summarization, extraction, content
          creation, and ideation.
      :param temperature: Temperature controls the degree of randomness in token
          selection.
      :param max_output_tokens: Token limit determines the maximum amount of text
          output.
      :param top_p: Tokens are selected from most probable to least until the sum
          of their probabilities equals the top_p value. Defaults to 0.8.
      :param top_k: A top_k of 1 means the selected token is the most probable
          among all tokens.


   .. py:method:: generate_text_embeddings(prompt, pretrained_model, location, project_id = PROVIDE_PROJECT_ID)

      Use the Vertex AI PaLM API to generate text embeddings.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param prompt: Required. Inputs or queries that a user or a program gives
          to the Vertex AI PaLM API, in order to elicit a specific response.
      :param pretrained_model: A pre-trained model optimized for generating text embeddings.


   .. py:method:: prompt_multimodal_model(prompt, location, generation_config = None, safety_settings = None, pretrained_model = 'gemini-pro', project_id = PROVIDE_PROJECT_ID)

      Use the Vertex AI Gemini Pro foundation model to generate natural language text.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param prompt: Required. Inputs or queries that a user or a program gives
          to the Multi-modal model, in order to elicit a specific response.
      :param generation_config: Optional. Generation configuration settings.
      :param safety_settings: Optional. Per request settings for blocking unsafe content.
      :param pretrained_model: By default uses the pre-trained model `gemini-pro`,
          supporting prompts with text-only input, including natural language
          tasks, multi-turn text and code chat, and code generation. It can
          output text and code.


   .. py:method:: prompt_multimodal_model_with_media(prompt, location, media_gcs_path, mime_type, generation_config = None, safety_settings = None, pretrained_model = 'gemini-pro-vision', project_id = PROVIDE_PROJECT_ID)

      Use the Vertex AI Gemini Pro foundation model to generate natural language text.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param prompt: Required. Inputs or queries that a user or a program gives
          to the Multi-modal model, in order to elicit a specific response.
      :param generation_config: Optional. Generation configuration settings.
      :param safety_settings: Optional. Per request settings for blocking unsafe content.
      :param pretrained_model: By default uses the pre-trained model `gemini-pro-vision`,
          supporting prompts with text-only input, including natural language
          tasks, multi-turn text and code chat, and code generation. It can
          output text and code.
      :param media_gcs_path: A GCS path to a content file such as an image or a video.
          Can be passed to the multi-modal model as part of the prompt. Used with vision models.
      :param mime_type: Validates the media type presented by the file in the media_gcs_path.


   .. py:method:: text_generation_model_predict(prompt, pretrained_model, temperature, max_output_tokens, top_p, top_k, location, project_id = PROVIDE_PROJECT_ID)

      Use the Vertex AI PaLM API to generate natural language text.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param prompt: Required. Inputs or queries that a user or a program gives
          to the Vertex AI PaLM API, in order to elicit a specific response.
      :param pretrained_model: A pre-trained model optimized for performing natural
          language tasks such as classification, summarization, extraction, content
          creation, and ideation.
      :param temperature: Temperature controls the degree of randomness in token
          selection.
      :param max_output_tokens: Token limit determines the maximum amount of text
          output.
      :param top_p: Tokens are selected from most probable to least until the sum
          of their probabilities equals the top_p value. Defaults to 0.8.
      :param top_k: A top_k of 1 means the selected token is the most probable
          among all tokens.


   .. py:method:: text_embedding_model_get_embeddings(prompt, pretrained_model, location, project_id = PROVIDE_PROJECT_ID)

      Use the Vertex AI PaLM API to generate text embeddings.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param prompt: Required. Inputs or queries that a user or a program gives
          to the Vertex AI PaLM API, in order to elicit a specific response.
      :param pretrained_model: A pre-trained model optimized for generating text embeddings.


   .. py:method:: generative_model_generate_content(contents, location, tools = None, generation_config = None, safety_settings = None, system_instruction = None, pretrained_model = 'gemini-pro', project_id = PROVIDE_PROJECT_ID)

      Use the Vertex AI Gemini Pro foundation model to generate natural language text.

      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param contents: Required. The multi-part content of a message that a user or a program
          gives to the generative model, in order to elicit a specific response.
      :param generation_config: Optional. Generation configuration settings.
      :param safety_settings: Optional. Per request settings for blocking unsafe content.
      :param tools: Optional. A list of tools available to the model during evaluation, such as a data store.
      :param system_instruction: Optional. An instruction given to the model to guide its behavior.
      :param pretrained_model: By default uses the pre-trained model `gemini-pro`,
          supporting prompts with text-only input, including natural language
          tasks, multi-turn text and code chat, and code generation. It can
          output text and code.


   .. py:method:: supervised_fine_tuning_train(source_model, train_dataset, location, tuned_model_display_name = None, validation_dataset = None, epochs = None, adapter_size = None, learning_rate_multiplier = None, project_id = PROVIDE_PROJECT_ID)

      Use the Supervised Fine Tuning API to create a tuning job.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param source_model: Required. A pre-trained model optimized for performing natural
          language tasks such as classification, summarization, extraction, content
          creation, and ideation.
      :param train_dataset: Required. Cloud Storage URI of your training dataset. The dataset
          must be formatted as a JSONL file. For best results, provide at least 100 to 500 examples.
      :param tuned_model_display_name: Optional. Display name of the TunedModel. The name can be up
          to 128 characters long and can consist of any UTF-8 characters.
      :param validation_dataset: Optional. Cloud Storage URI of your training dataset. The dataset must be
          formatted as a JSONL file. For best results, provide at least 100 to 500 examples.
      :param epochs: Optional. To optimize performance on a specific dataset, try using a higher
        epoch value. Increasing the number of epochs might improve results. However, be cautious
        about over-fitting, especially when dealing with small datasets. If over-fitting occurs,
        consider lowering the epoch number.
      :param adapter_size: Optional. Adapter size for tuning.
      :param learning_rate_multiplier: Optional. Multiplier for adjusting the default learning rate.


   .. py:method:: count_tokens(contents, location, pretrained_model = 'gemini-pro', project_id = PROVIDE_PROJECT_ID)

      Use the Vertex AI Count Tokens API to calculate the number of input tokens before sending a request to the Gemini API.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param contents: Required. The multi-part content of a message that a user or a program
          gives to the generative model, in order to elicit a specific response.
      :param pretrained_model: By default uses the pre-trained model `gemini-pro`,
          supporting prompts with text-only input, including natural language
          tasks, multi-turn text and code chat, and code generation. It can
          output text and code.


   .. py:method:: run_evaluation(pretrained_model, eval_dataset, metrics, experiment_name, experiment_run_name, prompt_template, location, generation_config = None, safety_settings = None, system_instruction = None, tools = None, project_id = PROVIDE_PROJECT_ID)

      Use the Rapid Evaluation API to evaluate a model.

      :param project_id: Required. The ID of the Google Cloud project that the service belongs to.
      :param location: Required. The ID of the Google Cloud location that the service belongs to.
      :param pretrained_model: Required. A pre-trained model optimized for performing natural
          language tasks such as classification, summarization, extraction, content
          creation, and ideation.
      :param eval_dataset: Required. A fixed dataset for evaluating a model against. Adheres to Rapid Evaluation API.
      :param metrics: Required. A list of evaluation metrics to be used in the experiment. Adheres to Rapid Evaluation API.
      :param experiment_name: Required. The name of the evaluation experiment.
      :param experiment_run_name: Required. The specific run name or ID for this experiment.
      :param prompt_template: Required. The template used to format the model's prompts during evaluation. Adheres to Rapid Evaluation API.
      :param generation_config: Optional. A dictionary containing generation parameters for the model.
      :param safety_settings: Optional. A dictionary specifying harm category thresholds for blocking model outputs.
      :param system_instruction: Optional. An instruction given to the model to guide its behavior.
      :param tools: Optional. A list of tools available to the model during evaluation, such as a data store.



