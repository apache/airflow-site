:py:mod:`airflow.models.dagbag`
===============================

.. py:module:: airflow.models.dagbag


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.models.dagbag.FileLoadStat
   airflow.models.dagbag.DagBag




.. py:class:: FileLoadStat

   Bases: :py:obj:`NamedTuple`

   Information about single file

   .. py:attribute:: file
      :annotation: :str

      

   .. py:attribute:: duration
      :annotation: :datetime.timedelta

      

   .. py:attribute:: dag_num
      :annotation: :int

      

   .. py:attribute:: task_num
      :annotation: :int

      

   .. py:attribute:: dags
      :annotation: :str

      


.. py:class:: DagBag(dag_folder = None, include_examples = NOTSET, safe_mode = NOTSET, read_dags_from_db = False, store_serialized_dags = None, load_op_links = True, collect_dags = True)

   Bases: :py:obj:`airflow.utils.log.logging_mixin.LoggingMixin`

   A dagbag is a collection of dags, parsed out of a folder tree and has high
   level configuration settings, like what database to use as a backend and
   what executor to use to fire off tasks. This makes it easier to run
   distinct environments for say production and development, tests, or for
   different teams or security profiles. What would have been system level
   settings are now dagbag level so that one system can run multiple,
   independent settings sets.

   :param dag_folder: the folder to scan to find DAGs
   :param include_examples: whether to include the examples that ship
       with airflow or not
   :param read_dags_from_db: Read DAGs from DB if ``True`` is passed.
       If ``False`` DAGs are read from python files.
   :param load_op_links: Should the extra operator link be loaded via plugins when
       de-serializing the DAG? This flag is set to False in Scheduler so that Extra Operator links
       are not loaded to not run User code in Scheduler.

   .. py:property:: store_serialized_dags
      :type: bool

      Whether or not to read dags from DB


   .. py:property:: dag_ids
      :type: list[str]

      Get DAG ids.

      :return: a list of DAG IDs in this bag


   .. py:method:: size()

      :return: the amount of dags contained in this dagbag


   .. py:method:: get_dag(dag_id, session = None)

      Gets the DAG out of the dictionary, and refreshes it if expired

      :param dag_id: DAG Id


   .. py:method:: process_file(filepath, only_if_updated=True, safe_mode=True)

      Given a path to a python module or zip file, this method imports
      the module and look for dag objects within it.


   .. py:method:: bag_dag(dag, root_dag)

      Adds the DAG into the bag, recurses into sub dags.

      :raises: AirflowDagCycleException if a cycle is detected in this dag or its subdags.
      :raises: AirflowDagDuplicatedIdException if this dag or its subdags already exists in the bag.


   .. py:method:: collect_dags(dag_folder = None, only_if_updated = True, include_examples = conf.getboolean('core', 'LOAD_EXAMPLES'), safe_mode = conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE'))

      Given a file path or a folder, this method looks for python modules,
      imports them and adds them to the dagbag collection.

      Note that if a ``.airflowignore`` file is found while processing
      the directory, it will behave much like a ``.gitignore``,
      ignoring files that match any of the patterns specified
      in the file.

      **Note**: The patterns in ``.airflowignore`` are interpreted as either
      un-anchored regexes or gitignore-like glob expressions, depending on
      the ``DAG_IGNORE_FILE_SYNTAX`` configuration parameter.


   .. py:method:: collect_dags_from_db()

      Collects DAGs from database.


   .. py:method:: dagbag_report()

      Prints a report around DagBag loading stats


   .. py:method:: sync_to_db(processor_subdir = None, session = None)

      Save attributes about list of DAG to the DB.



