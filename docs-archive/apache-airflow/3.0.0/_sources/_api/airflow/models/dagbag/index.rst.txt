airflow.models.dagbag
=====================

.. py:module:: airflow.models.dagbag


Classes
-------

.. autoapisummary::

   airflow.models.dagbag.FileLoadStat
   airflow.models.dagbag.DagBag
   airflow.models.dagbag.DagPriorityParsingRequest


Functions
---------

.. autoapisummary::

   airflow.models.dagbag.generate_md5_hash


Module Contents
---------------

.. py:class:: FileLoadStat

   Bases: :py:obj:`NamedTuple`


   Information about single file.

   :param file: Loaded file.
   :param duration: Time spent on process file.
   :param dag_num: Total number of DAGs loaded in this file.
   :param task_num: Total number of Tasks loaded in this file.
   :param dags: DAGs names loaded in this file.
   :param warning_num: Total number of warnings captured from processing this file.


   .. py:attribute:: file
      :type:  str


   .. py:attribute:: duration
      :type:  datetime.timedelta


   .. py:attribute:: dag_num
      :type:  int


   .. py:attribute:: task_num
      :type:  int


   .. py:attribute:: dags
      :type:  str


   .. py:attribute:: warning_num
      :type:  int


.. py:class:: DagBag(dag_folder = None, include_examples = NOTSET, safe_mode = NOTSET, read_dags_from_db = False, load_op_links = True, collect_dags = True, known_pools = None, bundle_path = None)

   Bases: :py:obj:`airflow.utils.log.logging_mixin.LoggingMixin`


   A dagbag is a collection of dags, parsed out of a folder tree and has high level configuration settings.

   Some possible setting are database to use as a backend and what executor
   to use to fire off tasks. This makes it easier to run distinct environments
   for say production and development, tests, or for different teams or security
   profiles. What would have been system level settings are now dagbag level so
   that one system can run multiple, independent settings sets.

   :param dag_folder: the folder to scan to find DAGs
   :param include_examples: whether to include the examples that ship
       with airflow or not
   :param safe_mode: when ``False``, scans all python modules for dags.
       When ``True`` uses heuristics (files containing ``DAG`` and ``airflow`` strings)
       to filter python modules to scan for dags.
   :param read_dags_from_db: Read DAGs from DB if ``True`` is passed.
       If ``False`` DAGs are read from python files.
   :param load_op_links: Should the extra operator link be loaded via plugins when
       de-serializing the DAG? This flag is set to False in Scheduler so that Extra Operator links
       are not loaded to not run User code in Scheduler.
   :param collect_dags: when True, collects dags during class initialization.
   :param known_pools: If not none, then generate warnings if a Task attempts to use an unknown pool.


   .. py:attribute:: bundle_path
      :type:  pathlib.Path | None
      :value: None



   .. py:attribute:: dag_folder


   .. py:attribute:: dags
      :type:  dict[str, airflow.models.dag.DAG]


   .. py:attribute:: file_last_changed
      :type:  dict[str, datetime.datetime]


   .. py:attribute:: import_errors
      :type:  dict[str, str]


   .. py:attribute:: captured_warnings
      :type:  dict[str, tuple[str, Ellipsis]]


   .. py:attribute:: has_logged
      :value: False



   .. py:attribute:: read_dags_from_db
      :value: False



   .. py:attribute:: dags_last_fetched
      :type:  dict[str, datetime.datetime]


   .. py:attribute:: dags_hash
      :type:  dict[str, str]


   .. py:attribute:: known_pools
      :value: None



   .. py:attribute:: dagbag_import_error_tracebacks
      :value: True



   .. py:attribute:: dagbag_import_error_traceback_depth


   .. py:attribute:: load_op_links
      :value: True



   .. py:method:: size()

      :return: the amount of dags contained in this dagbag



   .. py:property:: dag_ids
      :type: list[str]


      Get DAG ids.

      :return: a list of DAG IDs in this bag



   .. py:method:: get_dag(dag_id, session = None)

      Get the DAG out of the dictionary, and refreshes it if expired.

      :param dag_id: DAG ID



   .. py:method:: process_file(filepath, only_if_updated=True, safe_mode=True)

      Given a path to a python module or zip file, import the module and look for dag objects within.



   .. py:property:: dag_warnings
      :type: set[airflow.models.dagwarning.DagWarning]


      Get the set of DagWarnings for the bagged dags.



   .. py:method:: bag_dag(dag)

      Add the DAG into the bag.

      :raises: AirflowDagCycleException if a cycle is detected.
      :raises: AirflowDagDuplicatedIdException if this dag already exists in the bag.



   .. py:method:: collect_dags(dag_folder = None, only_if_updated = True, include_examples = conf.getboolean('core', 'LOAD_EXAMPLES'), safe_mode = conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE'))

      Look for python modules in a given path, import them, and add them to the dagbag collection.

      Note that if a ``.airflowignore`` file is found while processing
      the directory, it will behave much like a ``.gitignore``,
      ignoring files that match any of the patterns specified
      in the file.

      **Note**: The patterns in ``.airflowignore`` are interpreted as either
      un-anchored regexes or gitignore-like glob expressions, depending on
      the ``DAG_IGNORE_FILE_SYNTAX`` configuration parameter.



   .. py:method:: collect_dags_from_db()

      Collect DAGs from database.



   .. py:method:: dagbag_report()

      Print a report around DagBag loading stats.



   .. py:method:: sync_to_db(bundle_name, bundle_version, session = NEW_SESSION)

      Save attributes about list of DAG to the DB.



.. py:function:: generate_md5_hash(context)

.. py:class:: DagPriorityParsingRequest(bundle_name, relative_fileloc)

   Bases: :py:obj:`airflow.models.base.Base`


   Model to store the dag parsing requests that will be prioritized when parsing files.


   .. py:attribute:: __tablename__
      :value: 'dag_priority_parsing_request'



   .. py:attribute:: id


   .. py:attribute:: bundle_name


   .. py:attribute:: relative_fileloc


   .. py:method:: __repr__()


