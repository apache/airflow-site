:py:mod:`airflow.providers.alibaba.cloud.operators.analyticdb_spark`
====================================================================

.. py:module:: airflow.providers.alibaba.cloud.operators.analyticdb_spark


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.alibaba.cloud.operators.analyticdb_spark.AnalyticDBSparkBaseOperator
   airflow.providers.alibaba.cloud.operators.analyticdb_spark.AnalyticDBSparkSQLOperator
   airflow.providers.alibaba.cloud.operators.analyticdb_spark.AnalyticDBSparkBatchOperator




.. py:class:: AnalyticDBSparkBaseOperator(*, adb_spark_conn_id = 'adb_spark_default', region = None, polling_interval = 0, **kwargs)


   Bases: :py:obj:`airflow.models.BaseOperator`

   Abstract base class that defines how users develop AnalyticDB Spark.

   .. py:method:: hook()

      Get valid hook.


   .. py:method:: get_hook()

      Get valid hook.


   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.


   .. py:method:: monitor_application()


   .. py:method:: poll_for_termination(app_id)

      Pool for spark application termination.

      :param app_id: id of the spark application to monitor


   .. py:method:: on_kill()

      Override this method to clean up subprocesses when a task instance gets killed.

      Any use of the threading, subprocess or multiprocessing module within an
      operator needs to be cleaned up, or it will leave ghost processes behind.


   .. py:method:: kill()

      Delete the specified application.



.. py:class:: AnalyticDBSparkSQLOperator(*, sql, conf = None, driver_resource_spec = None, executor_resource_spec = None, num_executors = None, name = None, cluster_id, rg_name, **kwargs)


   Bases: :py:obj:`AnalyticDBSparkBaseOperator`

   Submits a Spark SQL application to the underlying cluster; wraps the AnalyticDB Spark REST API.

   :param sql: The SQL query to execute.
   :param conf: Spark configuration properties.
   :param driver_resource_spec: The resource specifications of the Spark driver.
   :param executor_resource_spec: The resource specifications of each Spark executor.
   :param num_executors: number of executors to launch for this application.
   :param name: name of this application.
   :param cluster_id: The cluster ID of AnalyticDB MySQL 3.0 Data Lakehouse.
   :param rg_name: The name of resource group in AnalyticDB MySQL 3.0 Data Lakehouse cluster.

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('spark_params',)

      

   .. py:attribute:: template_fields_renderers

      

   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



.. py:class:: AnalyticDBSparkBatchOperator(*, file, class_name = None, args = None, conf = None, jars = None, py_files = None, files = None, driver_resource_spec = None, executor_resource_spec = None, num_executors = None, archives = None, name = None, cluster_id, rg_name, **kwargs)


   Bases: :py:obj:`AnalyticDBSparkBaseOperator`

   Submits a Spark batch application to the underlying cluster; wraps the AnalyticDB Spark REST API.

   :param file: path of the file containing the application to execute.
   :param class_name: name of the application Java/Spark main class.
   :param args: application command line arguments.
   :param conf: Spark configuration properties.
   :param jars: jars to be used in this application.
   :param py_files: python files to be used in this application.
   :param files: files to be used in this application.
   :param driver_resource_spec: The resource specifications of the Spark driver.
   :param executor_resource_spec: The resource specifications of each Spark executor.
   :param num_executors: number of executors to launch for this application.
   :param archives: archives to be used in this application.
   :param name: name of this application.
   :param cluster_id: The cluster ID of AnalyticDB MySQL 3.0 Data Lakehouse.
   :param rg_name: The name of resource group in AnalyticDB MySQL 3.0 Data Lakehouse cluster.

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('spark_params',)

      

   .. py:attribute:: template_fields_renderers

      

   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



