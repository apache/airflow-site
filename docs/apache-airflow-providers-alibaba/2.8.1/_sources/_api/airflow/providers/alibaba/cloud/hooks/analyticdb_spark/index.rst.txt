:py:mod:`airflow.providers.alibaba.cloud.hooks.analyticdb_spark`
================================================================

.. py:module:: airflow.providers.alibaba.cloud.hooks.analyticdb_spark


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.alibaba.cloud.hooks.analyticdb_spark.AppState
   airflow.providers.alibaba.cloud.hooks.analyticdb_spark.AnalyticDBSparkHook




.. py:class:: AppState


   Bases: :py:obj:`enum.Enum`

   AnalyticDB Spark application states.

   See:
   https://www.alibabacloud.com/help/en/analyticdb-for-mysql/latest/api-doc-adb-2021-12-01-api-struct
   -sparkappinfo.


   .. py:attribute:: SUBMITTED
      :value: 'SUBMITTED'

      

   .. py:attribute:: STARTING
      :value: 'STARTING'

      

   .. py:attribute:: RUNNING
      :value: 'RUNNING'

      

   .. py:attribute:: FAILING
      :value: 'FAILING'

      

   .. py:attribute:: FAILED
      :value: 'FAILED'

      

   .. py:attribute:: KILLING
      :value: 'KILLING'

      

   .. py:attribute:: KILLED
      :value: 'KILLED'

      

   .. py:attribute:: SUCCEEDING
      :value: 'SUCCEEDING'

      

   .. py:attribute:: COMPLETED
      :value: 'COMPLETED'

      

   .. py:attribute:: FATAL
      :value: 'FATAL'

      

   .. py:attribute:: UNKNOWN
      :value: 'UNKNOWN'

      


.. py:class:: AnalyticDBSparkHook(adb_spark_conn_id = 'adb_spark_default', region = None, *args, **kwargs)


   Bases: :py:obj:`airflow.hooks.base.BaseHook`, :py:obj:`airflow.utils.log.logging_mixin.LoggingMixin`

   Hook for AnalyticDB MySQL Spark through the REST API.

   :param adb_spark_conn_id: The Airflow connection used for AnalyticDB MySQL Spark credentials.
   :param region: AnalyticDB MySQL region you want to submit spark application.

   .. py:attribute:: TERMINAL_STATES

      

   .. py:attribute:: conn_name_attr
      :value: 'alibabacloud_conn_id'

      

   .. py:attribute:: default_conn_name
      :value: 'adb_spark_default'

      

   .. py:attribute:: conn_type
      :value: 'adb_spark'

      

   .. py:attribute:: hook_name
      :value: 'AnalyticDB Spark'

      

   .. py:method:: submit_spark_app(cluster_id, rg_name, *args, **kwargs)

      Perform request to submit spark application.

      :param cluster_id: The cluster ID of AnalyticDB MySQL 3.0 Data Lakehouse.
      :param rg_name: The name of resource group in AnalyticDB MySQL 3.0 Data Lakehouse cluster.


   .. py:method:: submit_spark_sql(cluster_id, rg_name, *args, **kwargs)

      Perform request to submit spark sql.

      :param cluster_id: The cluster ID of AnalyticDB MySQL 3.0 Data Lakehouse.
      :param rg_name: The name of resource group in AnalyticDB MySQL 3.0 Data Lakehouse cluster.


   .. py:method:: get_spark_state(app_id)

      Fetch the state of the specified spark application.

      :param app_id: identifier of the spark application


   .. py:method:: get_spark_web_ui_address(app_id)

      Fetch the web ui address of the specified spark application.

      :param app_id: identifier of the spark application


   .. py:method:: get_spark_log(app_id)

      Get the logs for a specified spark application.

      :param app_id: identifier of the spark application


   .. py:method:: kill_spark_app(app_id)

      Kill the specified spark application.

      :param app_id: identifier of the spark application


   .. py:method:: build_submit_app_data(file = None, class_name = None, args = None, conf = None, jars = None, py_files = None, files = None, driver_resource_spec = None, executor_resource_spec = None, num_executors = None, archives = None, name = None)
      :staticmethod:

      Build the submit application request data.

      :param file: path of the file containing the application to execute.
      :param class_name: name of the application Java/Spark main class.
      :param args: application command line arguments.
      :param conf: Spark configuration properties.
      :param jars: jars to be used in this application.
      :param py_files: python files to be used in this application.
      :param files: files to be used in this application.
      :param driver_resource_spec: The resource specifications of the Spark driver.
      :param executor_resource_spec: The resource specifications of each Spark executor.
      :param num_executors: number of executors to launch for this application.
      :param archives: archives to be used in this application.
      :param name: name of this application.


   .. py:method:: build_submit_sql_data(sql = None, conf = None, driver_resource_spec = None, executor_resource_spec = None, num_executors = None, name = None)
      :staticmethod:

      Build the submit spark sql request data.

      :param sql: The SQL query to execute. (templated)
      :param conf: Spark configuration properties.
      :param driver_resource_spec: The resource specifications of the Spark driver.
      :param executor_resource_spec: The resource specifications of each Spark executor.
      :param num_executors: number of executors to launch for this application.
      :param name: name of this application.


   .. py:method:: get_adb_spark_client()

      Get valid AnalyticDB MySQL Spark client.


   .. py:method:: get_default_region()

      Get default region from connection.



