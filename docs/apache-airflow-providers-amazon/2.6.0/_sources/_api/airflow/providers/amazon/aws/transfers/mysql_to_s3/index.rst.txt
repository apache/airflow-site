:py:mod:`airflow.providers.amazon.aws.transfers.mysql_to_s3`
============================================================

.. py:module:: airflow.providers.amazon.aws.transfers.mysql_to_s3


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.amazon.aws.transfers.mysql_to_s3.MySQLToS3Operator




Attributes
~~~~~~~~~~

.. autoapisummary::

   airflow.providers.amazon.aws.transfers.mysql_to_s3.FILE_FORMAT
   airflow.providers.amazon.aws.transfers.mysql_to_s3.FileOptions
   airflow.providers.amazon.aws.transfers.mysql_to_s3.FILE_OPTIONS_MAP


.. py:data:: FILE_FORMAT
   

   

.. py:data:: FileOptions
   

   

.. py:data:: FILE_OPTIONS_MAP
   

   

.. py:class:: MySQLToS3Operator(*, query: str, s3_bucket: str, s3_key: str, replace: bool = False, mysql_conn_id: str = 'mysql_default', aws_conn_id: str = 'aws_default', verify: Optional[Union[bool, str]] = None, pd_csv_kwargs: Optional[dict] = None, index: bool = False, header: bool = False, file_format: typing_extensions.Literal[csv, parquet] = 'csv', pd_kwargs: Optional[dict] = None, **kwargs)

   Bases: :py:obj:`airflow.models.BaseOperator`

   Saves data from an specific MySQL query into a file in S3.

   :param query: the sql query to be executed. If you want to execute a file, place the absolute path of it,
       ending with .sql extension. (templated)
   :type query: str
   :param s3_bucket: bucket where the data will be stored. (templated)
   :type s3_bucket: str
   :param s3_key: desired key for the file. It includes the name of the file. (templated)
   :type s3_key: str
   :param replace: whether or not to replace the file in S3 if it previously existed
   :type replace: bool
   :param mysql_conn_id: Reference to :ref:`mysql connection id <howto/connection:mysql>`.
   :type mysql_conn_id: str
   :param aws_conn_id: reference to a specific S3 connection
   :type aws_conn_id: str
   :param verify: Whether or not to verify SSL certificates for S3 connection.
       By default SSL certificates are verified.
       You can provide the following values:

       - ``False``: do not validate SSL certificates. SSL will still be used
               (unless use_ssl is False), but SSL certificates will not be verified.
       - ``path/to/cert/bundle.pem``: A filename of the CA cert bundle to uses.
               You can specify this argument if you want to use a different
               CA cert bundle than the one used by botocore.
   :type verify: bool or str
   :param pd_csv_kwargs: arguments to include in pd.to_csv (header, index, columns...)
   :type pd_csv_kwargs: dict
   :param index: whether to have the index or not in the dataframe
   :type index: str
   :param header: whether to include header or not into the S3 file
   :type header: bool
   :param file_format: the destination file format, only string 'csv' or 'parquet' is accepted.
   :type file_format: str
   :param pd_kwargs: arguments to include in ``DataFrame.to_parquet()`` or
       ``DataFrame.to_csv()``. This is preferred than ``pd_csv_kwargs``.
   :type pd_kwargs: dict

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['s3_bucket', 's3_key', 'query']

      

   .. py:attribute:: template_ext
      :annotation: :Sequence[str] = ['.sql']

      

   .. py:attribute:: template_fields_renderers
      

      

   .. py:method:: execute(self, context: airflow.utils.context.Context) -> None

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.



