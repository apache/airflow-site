:py:mod:`airflow.providers.amazon.aws.operators.glue_databrew`
==============================================================

.. py:module:: airflow.providers.amazon.aws.operators.glue_databrew


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.amazon.aws.operators.glue_databrew.GlueDataBrewStartJobOperator




.. py:class:: GlueDataBrewStartJobOperator(job_name, wait_for_completion = True, delay = 30, deferrable = conf.getboolean('operators', 'default_deferrable', fallback=False), aws_conn_id = 'aws_default', **kwargs)


   Bases: :py:obj:`airflow.models.BaseOperator`

   Start an AWS Glue DataBrew job.

   AWS Glue DataBrew is a visual data preparation tool that makes it easier
   for data analysts and data scientists to clean and normalize data
   to prepare it for analytics and machine learning (ML).

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:GlueDataBrewStartJobOperator`

   :param job_name: unique job name per AWS Account
   :param wait_for_completion: Whether to wait for job run completion. (default: True)
   :param deferrable: If True, the operator will wait asynchronously for the job to complete.
       This implies waiting for completion. This mode requires aiobotocore module to be installed.
       (default: False)
   :param delay: Time in seconds to wait between status checks. Default is 30.
   :return: dictionary with key run_id and value of the resulting job's run_id.

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('job_name', 'wait_for_completion', 'delay', 'deferrable')

      

   .. py:method:: hook()


   .. py:method:: execute(context)

      Derive when creating an operator.

      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.


   .. py:method:: execute_complete(context, event=None)



