:py:mod:`airflow.providers.amazon.aws.triggers.redshift_cluster`
================================================================

.. py:module:: airflow.providers.amazon.aws.triggers.redshift_cluster


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.amazon.aws.triggers.redshift_cluster.RedshiftCreateClusterTrigger
   airflow.providers.amazon.aws.triggers.redshift_cluster.RedshiftPauseClusterTrigger
   airflow.providers.amazon.aws.triggers.redshift_cluster.RedshiftCreateClusterSnapshotTrigger
   airflow.providers.amazon.aws.triggers.redshift_cluster.RedshiftResumeClusterTrigger
   airflow.providers.amazon.aws.triggers.redshift_cluster.RedshiftDeleteClusterTrigger




.. py:class:: RedshiftCreateClusterTrigger(cluster_identifier, poll_interval, max_attempt, aws_conn_id)


   Bases: :py:obj:`airflow.triggers.base.BaseTrigger`

   Trigger for RedshiftCreateClusterOperator.

   The trigger will asynchronously poll the boto3 API and wait for the
   Redshift cluster to be in the `available` state.

   :param cluster_identifier:  A unique identifier for the cluster.
   :param poll_interval: The amount of time in seconds to wait between attempts.
   :param max_attempt: The maximum number of attempts to be made.
   :param aws_conn_id: The Airflow connection used for AWS credentials.

   .. py:method:: serialize()

      Returns the information needed to reconstruct this Trigger.

      :return: Tuple of (class path, keyword arguments needed to re-instantiate).


   .. py:method:: hook()


   .. py:method:: run()
      :async:

      Runs the trigger in an asynchronous context.

      The trigger should yield an Event whenever it wants to fire off
      an event, and return None if it is finished. Single-event triggers
      should thus yield and then immediately return.

      If it yields, it is likely that it will be resumed very quickly,
      but it may not be (e.g. if the workload is being moved to another
      triggerer process, or a multi-event trigger was being used for a
      single-event task defer).

      In either case, Trigger classes should assume they will be persisted,
      and then rely on cleanup() being called when they are no longer needed.



.. py:class:: RedshiftPauseClusterTrigger(cluster_identifier, poll_interval, max_attempts, aws_conn_id)


   Bases: :py:obj:`airflow.triggers.base.BaseTrigger`

   Trigger for RedshiftPauseClusterOperator.

   The trigger will asynchronously poll the boto3 API and wait for the
   Redshift cluster to be in the `paused` state.

   :param cluster_identifier:  A unique identifier for the cluster.
   :param poll_interval: The amount of time in seconds to wait between attempts.
   :param max_attempts: The maximum number of attempts to be made.
   :param aws_conn_id: The Airflow connection used for AWS credentials.

   .. py:method:: serialize()

      Returns the information needed to reconstruct this Trigger.

      :return: Tuple of (class path, keyword arguments needed to re-instantiate).


   .. py:method:: hook()


   .. py:method:: run()
      :async:

      Runs the trigger in an asynchronous context.

      The trigger should yield an Event whenever it wants to fire off
      an event, and return None if it is finished. Single-event triggers
      should thus yield and then immediately return.

      If it yields, it is likely that it will be resumed very quickly,
      but it may not be (e.g. if the workload is being moved to another
      triggerer process, or a multi-event trigger was being used for a
      single-event task defer).

      In either case, Trigger classes should assume they will be persisted,
      and then rely on cleanup() being called when they are no longer needed.



.. py:class:: RedshiftCreateClusterSnapshotTrigger(cluster_identifier, poll_interval, max_attempts, aws_conn_id)


   Bases: :py:obj:`airflow.triggers.base.BaseTrigger`

   Trigger for RedshiftCreateClusterSnapshotOperator.

   The trigger will asynchronously poll the boto3 API and wait for the
   Redshift cluster snapshot to be in the `available` state.

   :param cluster_identifier:  A unique identifier for the cluster.
   :param poll_interval: The amount of time in seconds to wait between attempts.
   :param max_attempts: The maximum number of attempts to be made.
   :param aws_conn_id: The Airflow connection used for AWS credentials.

   .. py:method:: serialize()

      Returns the information needed to reconstruct this Trigger.

      :return: Tuple of (class path, keyword arguments needed to re-instantiate).


   .. py:method:: hook()


   .. py:method:: run()
      :async:

      Runs the trigger in an asynchronous context.

      The trigger should yield an Event whenever it wants to fire off
      an event, and return None if it is finished. Single-event triggers
      should thus yield and then immediately return.

      If it yields, it is likely that it will be resumed very quickly,
      but it may not be (e.g. if the workload is being moved to another
      triggerer process, or a multi-event trigger was being used for a
      single-event task defer).

      In either case, Trigger classes should assume they will be persisted,
      and then rely on cleanup() being called when they are no longer needed.



.. py:class:: RedshiftResumeClusterTrigger(cluster_identifier, poll_interval, max_attempts, aws_conn_id)


   Bases: :py:obj:`airflow.triggers.base.BaseTrigger`

   Trigger for RedshiftResumeClusterOperator.

   The trigger will asynchronously poll the boto3 API and wait for the
   Redshift cluster to be in the `available` state.

   :param cluster_identifier:  A unique identifier for the cluster.
   :param poll_interval: The amount of time in seconds to wait between attempts.
   :param max_attempts: The maximum number of attempts to be made.
   :param aws_conn_id: The Airflow connection used for AWS credentials.

   .. py:method:: serialize()

      Returns the information needed to reconstruct this Trigger.

      :return: Tuple of (class path, keyword arguments needed to re-instantiate).


   .. py:method:: hook()


   .. py:method:: run()
      :async:

      Runs the trigger in an asynchronous context.

      The trigger should yield an Event whenever it wants to fire off
      an event, and return None if it is finished. Single-event triggers
      should thus yield and then immediately return.

      If it yields, it is likely that it will be resumed very quickly,
      but it may not be (e.g. if the workload is being moved to another
      triggerer process, or a multi-event trigger was being used for a
      single-event task defer).

      In either case, Trigger classes should assume they will be persisted,
      and then rely on cleanup() being called when they are no longer needed.



.. py:class:: RedshiftDeleteClusterTrigger(cluster_identifier, max_attempts = 30, aws_conn_id = 'aws_default', poll_interval = 30)


   Bases: :py:obj:`airflow.triggers.base.BaseTrigger`

   Trigger for RedshiftDeleteClusterOperator.

   :param cluster_identifier:  A unique identifier for the cluster.
   :param max_attempts: The maximum number of attempts to be made.
   :param aws_conn_id: The Airflow connection used for AWS credentials.
   :param poll_interval: The amount of time in seconds to wait between attempts.

   .. py:method:: serialize()

      Returns the information needed to reconstruct this Trigger.

      :return: Tuple of (class path, keyword arguments needed to re-instantiate).


   .. py:method:: hook()


   .. py:method:: run()
      :async:

      Runs the trigger in an asynchronous context.

      The trigger should yield an Event whenever it wants to fire off
      an event, and return None if it is finished. Single-event triggers
      should thus yield and then immediately return.

      If it yields, it is likely that it will be resumed very quickly,
      but it may not be (e.g. if the workload is being moved to another
      triggerer process, or a multi-event trigger was being used for a
      single-event task defer).

      In either case, Trigger classes should assume they will be persisted,
      and then rely on cleanup() being called when they are no longer needed.



