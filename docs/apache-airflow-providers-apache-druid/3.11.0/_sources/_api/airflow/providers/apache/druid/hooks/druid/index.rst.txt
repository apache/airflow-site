:py:mod:`airflow.providers.apache.druid.hooks.druid`
====================================================

.. py:module:: airflow.providers.apache.druid.hooks.druid


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.apache.druid.hooks.druid.IngestionType
   airflow.providers.apache.druid.hooks.druid.DruidHook
   airflow.providers.apache.druid.hooks.druid.DruidDbApiHook




.. py:class:: IngestionType


   Bases: :py:obj:`enum.Enum`

   Druid Ingestion Type. Could be Native batch ingestion or SQL-based ingestion.

   https://druid.apache.org/docs/latest/ingestion/index.html

   .. py:attribute:: BATCH
      :value: 1

      

   .. py:attribute:: MSQ
      :value: 2

      


.. py:class:: DruidHook(druid_ingest_conn_id = 'druid_ingest_default', timeout = 1, max_ingestion_time = None, verify_ssl = True)


   Bases: :py:obj:`airflow.hooks.base.BaseHook`

   Connection to Druid overlord for ingestion.

   To connect to a Druid cluster that is secured with the druid-basic-security
   extension, add the username and password to the druid ingestion connection.

   :param druid_ingest_conn_id: The connection id to the Druid overlord machine
                                which accepts index jobs
   :param timeout: The interval between polling
                   the Druid job for the status of the ingestion job.
                   Must be greater than or equal to 1
   :param max_ingestion_time: The maximum ingestion time before assuming the job failed
   :param verify_ssl: Whether to use SSL encryption to submit indexing job. If set to False then checks
                      connection information for path to a CA bundle to use. Defaults to True

   .. py:method:: conn()


   .. py:method:: get_conn_url(ingestion_type = IngestionType.BATCH)

      Get Druid connection url.


   .. py:method:: get_auth()

      Return username and password from connections tab as requests.auth.HTTPBasicAuth object.

      If these details have not been set then returns None.


   .. py:method:: get_verify()


   .. py:method:: submit_indexing_job(json_index_spec, ingestion_type = IngestionType.BATCH)

      Submit Druid ingestion job.



.. py:class:: DruidDbApiHook(context = None, *args, **kwargs)


   Bases: :py:obj:`airflow.providers.common.sql.hooks.sql.DbApiHook`

   Interact with Druid broker.

   This hook is purely for users to query druid broker.
   For ingestion, please use druidHook.

   :param context: Optional query context parameters to pass to the SQL endpoint.
       Example: ``{"sqlFinalizeOuterSketches": True}``
       See: https://druid.apache.org/docs/latest/querying/sql-query-context/

   .. py:attribute:: conn_name_attr
      :value: 'druid_broker_conn_id'

      

   .. py:attribute:: default_conn_name
      :value: 'druid_broker_default'

      

   .. py:attribute:: conn_type
      :value: 'druid'

      

   .. py:attribute:: hook_name
      :value: 'Druid'

      

   .. py:attribute:: supports_autocommit
      :value: False

      

   .. py:method:: get_conn()

      Establish a connection to druid broker.


   .. py:method:: get_uri()

      Get the connection uri for druid broker.

      e.g: druid://localhost:8082/druid/v2/sql/


   .. py:method:: set_autocommit(conn, autocommit)
      :abstractmethod:

      Set the autocommit flag on the connection.


   .. py:method:: insert_rows(table, rows, target_fields = None, commit_every = 1000, replace = False, **kwargs)
      :abstractmethod:

      Insert a collection of tuples into a table.

      Rows are inserted in chunks, each chunk (of size ``commit_every``) is
      done in a new transaction.

      :param table: Name of the target table
      :param rows: The rows to insert into the table
      :param target_fields: The names of the columns to fill in the table
      :param commit_every: The maximum number of rows to insert in one
          transaction. Set to 0 to insert all rows in one transaction.
      :param replace: Whether to replace instead of insert
      :param executemany: If True, all rows are inserted at once in
          chunks defined by the commit_every parameter. This only works if all rows
          have same number of column names, but leads to better performance.
      :param autocommit: What to set the connection's autocommit setting to
          before executing the query.



