:py:mod:`airflow.providers.apache.hive.operators.hive`
======================================================

.. py:module:: airflow.providers.apache.hive.operators.hive


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.apache.hive.operators.hive.HiveOperator




.. py:class:: HiveOperator(*, hql: str, hive_cli_conn_id: str = 'hive_cli_default', schema: str = 'default', hiveconfs: Optional[Dict[Any, Any]] = None, hiveconf_jinja_translate: bool = False, script_begin_tag: Optional[str] = None, run_as_owner: bool = False, mapred_queue: Optional[str] = None, mapred_queue_priority: Optional[str] = None, mapred_job_name: Optional[str] = None, **kwargs: Any)

   Bases: :py:obj:`airflow.models.BaseOperator`

   Executes hql code or hive script in a specific Hive database.

   :param hql: the hql to be executed. Note that you may also use
       a relative path from the dag file of a (template) hive
       script. (templated)
   :type hql: str
   :param hive_cli_conn_id: Reference to the
       :ref:`Hive CLI connection id <howto/connection:hive_cli>`. (templated)
   :type hive_cli_conn_id: str
   :param hiveconfs: if defined, these key value pairs will be passed
       to hive as ``-hiveconf "key"="value"``
   :type hiveconfs: dict
   :param hiveconf_jinja_translate: when True, hiveconf-type templating
       ${var} gets translated into jinja-type templating {{ var }} and
       ${hiveconf:var} gets translated into jinja-type templating {{ var }}.
       Note that you may want to use this along with the
       ``DAG(user_defined_macros=myargs)`` parameter. View the DAG
       object documentation for more details.
   :type hiveconf_jinja_translate: bool
   :param script_begin_tag: If defined, the operator will get rid of the
       part of the script before the first occurrence of `script_begin_tag`
   :type script_begin_tag: str
   :param run_as_owner: Run HQL code as a DAG's owner.
   :type run_as_owner: bool
   :param mapred_queue: queue used by the Hadoop CapacityScheduler. (templated)
   :type  mapred_queue: str
   :param mapred_queue_priority: priority within CapacityScheduler queue.
       Possible settings include: VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW
   :type  mapred_queue_priority: str
   :param mapred_job_name: This name will appear in the jobtracker.
       This can make monitoring easier.
   :type  mapred_job_name: str

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['hql', 'schema', 'hive_cli_conn_id', 'mapred_queue', 'hiveconfs', 'mapred_job_name',...

      

   .. py:attribute:: template_ext
      :annotation: :Sequence[str] = ['.hql', '.sql']

      

   .. py:attribute:: ui_color
      :annotation: = #f0e4ec

      

   .. py:method:: get_hook(self) -> airflow.providers.apache.hive.hooks.hive.HiveCliHook

      Get Hive cli hook


   .. py:method:: prepare_template(self) -> None

      Hook that is triggered after the templated fields get replaced
      by their content. If you need your operator to alter the
      content of the file before the template is rendered,
      it should override this method to do so.


   .. py:method:: execute(self, context: airflow.utils.context.Context) -> None

      This is the main method to derive when creating an operator.
      Context is the same dictionary used as when rendering jinja templates.

      Refer to get_template_context for more context.


   .. py:method:: dry_run(self) -> None

      Performs dry run for the operator - just render template fields.


   .. py:method:: on_kill(self) -> None

      Override this method to cleanup subprocesses when a task instance
      gets killed. Any use of the threading, subprocess or multiprocessing
      module within an operator needs to be cleaned up or it will leave
      ghost processes behind.


   .. py:method:: clear_airflow_vars(self) -> None

      Reset airflow environment variables to prevent existing ones from impacting behavior.



