:py:mod:`airflow.providers.cncf.kubernetes.operators.spark_kubernetes`
======================================================================

.. py:module:: airflow.providers.cncf.kubernetes.operators.spark_kubernetes


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.cncf.kubernetes.operators.spark_kubernetes.SparkKubernetesOperator




.. py:class:: SparkKubernetesOperator(*, image = None, code_path = None, namespace = 'default', name = 'default', application_file = None, template_spec=None, get_logs = True, do_xcom_push = False, success_run_history_limit = 1, startup_timeout_seconds=600, log_events_on_failure = False, reattach_on_restart = True, delete_on_termination = True, kubernetes_conn_id = 'kubernetes_default', **kwargs)


   Bases: :py:obj:`airflow.providers.cncf.kubernetes.operators.pod.KubernetesPodOperator`

   Creates sparkApplication object in kubernetes cluster.

   .. seealso::
       For more detail about Spark Application Object have a look at the reference:
       https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/v1beta2-1.3.3-3.1.1/docs/api-docs.md#sparkapplication

   :param application_file: filepath to kubernetes custom_resource_definition of sparkApplication
   :param kubernetes_conn_id: the connection to Kubernetes cluster
   :param image: Docker image you wish to launch. Defaults to hub.docker.com,
   :param code_path: path to the spark code in image,
   :param namespace: kubernetes namespace to put sparkApplication
   :param cluster_context: context of the cluster
   :param application_file: yaml file if passed
   :param get_logs: get the stdout of the container as logs of the tasks.
   :param do_xcom_push: If True, the content of the file
       /airflow/xcom/return.json in the container will also be pushed to an
       XCom when the container completes.
   :param success_run_history_limit: Number of past successful runs of the application to keep.
   :param delete_on_termination: What to do when the pod reaches its final
       state, or the execution is interrupted. If True (default), delete the
       pod; if False, leave the pod.
   :param startup_timeout_seconds: timeout in seconds to startup the pod.
   :param log_events_on_failure: Log the pod's events if a failure occurs
   :param reattach_on_restart: if the scheduler dies while the pod is running, reattach and monitor

   .. py:property:: template_body

      Templated body for CustomObjectLauncher.


   .. py:attribute:: template_fields
      :value: ['application_file', 'namespace', 'template_spec']

      

   .. py:attribute:: template_fields_renderers

      

   .. py:attribute:: template_ext
      :value: ('yaml', 'yml', 'json')

      

   .. py:attribute:: ui_color
      :value: '#f4a460'

      

   .. py:method:: manage_template_specs()


   .. py:method:: create_job_name()


   .. py:method:: create_labels_for_pod(context = None, include_try_number = True)
      :staticmethod:

      Generate labels for the pod to track the pod in case of Operator crash.

      :param include_try_number: add try number to labels
      :param context: task context provided by airflow DAG
      :return: dict.


   .. py:method:: pod_manager()


   .. py:method:: find_spark_job(context)


   .. py:method:: get_or_create_spark_crd(launcher, context)


   .. py:method:: process_pod_deletion(pod, *, reraise=True)


   .. py:method:: hook()


   .. py:method:: client()


   .. py:method:: custom_obj_api()


   .. py:method:: execute(context)

      Based on the deferrable parameter runs the pod asynchronously or synchronously.


   .. py:method:: on_kill()

      Override this method to clean up subprocesses when a task instance gets killed.

      Any use of the threading, subprocess or multiprocessing module within an
      operator needs to be cleaned up, or it will leave ghost processes behind.


   .. py:method:: patch_already_checked(pod, *, reraise=True)

      Add an "already checked" annotation to ensure we don't reattach on retries.


   .. py:method:: dry_run()

      Print out the spark job that would be created by this operator.



