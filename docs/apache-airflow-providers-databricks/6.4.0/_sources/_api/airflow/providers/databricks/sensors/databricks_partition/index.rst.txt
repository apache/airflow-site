:py:mod:`airflow.providers.databricks.sensors.databricks_partition`
===================================================================

.. py:module:: airflow.providers.databricks.sensors.databricks_partition

.. autoapi-nested-parse::

   This module contains Databricks sensors.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.databricks.sensors.databricks_partition.DatabricksPartitionSensor




.. py:class:: DatabricksPartitionSensor(*, databricks_conn_id = DatabricksSqlHook.default_conn_name, http_path = None, sql_warehouse_name = None, session_configuration=None, http_headers = None, catalog = '', schema = 'default', table_name, partitions, partition_operator = '=', handler = fetch_all_handler, client_parameters = None, **kwargs)


   Bases: :py:obj:`airflow.sensors.base.BaseSensorOperator`

   Sensor to detect the presence of table partitions in Databricks.

   :param databricks_conn_id: Reference to :ref:`Databricks
       connection id<howto/connection:databricks>` (templated), defaults to
       DatabricksSqlHook.default_conn_name.
   :param sql_warehouse_name: Optional name of Databricks SQL warehouse. If not specified, ``http_path``
       must be provided as described below, defaults to None
   :param http_path: Optional string specifying HTTP path of Databricks SQL warehouse or All Purpose cluster.
       If not specified, it should be either specified in the Databricks connection's
       extra parameters, or ``sql_warehouse_name`` must be specified.
   :param session_configuration: An optional dictionary of Spark session parameters. If not specified,
       it could be specified in the Databricks connection's extra parameters, defaults to None
   :param http_headers: An optional list of (k, v) pairs
       that will be set as HTTP headers on every request. (templated).
   :param catalog: An optional initial catalog to use.
       Requires Databricks Runtime version 9.0+ (templated), defaults to ""
   :param schema: An optional initial schema to use.
       Requires Databricks Runtime version 9.0+ (templated), defaults to "default"
   :param table_name: Name of the table to check partitions.
   :param partitions: Name of the partitions to check.
       Example: {"date": "2023-01-03", "name": ["abc", "def"]}
   :param partition_operator: Optional comparison operator for partitions, such as >=.
   :param handler: Handler for DbApiHook.run() to return results, defaults to fetch_all_handler
   :param client_parameters: Additional parameters internal to Databricks SQL connector parameters.

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('databricks_conn_id', 'catalog', 'schema', 'table_name', 'partitions', 'http_headers')

      

   .. py:attribute:: template_ext
      :type: Sequence[str]
      :value: ('.sql',)

      

   .. py:attribute:: template_fields_renderers

      

   .. py:method:: poke(context)

      Check the table partitions and return the results.



