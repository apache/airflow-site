:py:mod:`airflow.providers.databricks.sensors.databricks_sql`
=============================================================

.. py:module:: airflow.providers.databricks.sensors.databricks_sql

.. autoapi-nested-parse::

   This module contains Databricks sensors.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.databricks.sensors.databricks_sql.DatabricksSqlSensor




.. py:class:: DatabricksSqlSensor(*, databricks_conn_id = DatabricksSqlHook.default_conn_name, http_path = None, sql_warehouse_name = None, session_configuration=None, http_headers = None, catalog = '', schema = 'default', sql, handler = fetch_all_handler, client_parameters = None, **kwargs)


   Bases: :py:obj:`airflow.sensors.base.BaseSensorOperator`

   Sensor that runs a SQL query on Databricks.

   :param databricks_conn_id: Reference to :ref:`Databricks
       connection id<howto/connection:databricks>` (templated), defaults to
       DatabricksSqlHook.default_conn_name.
   :param sql_warehouse_name: Optional name of Databricks SQL warehouse. If not specified, ``http_path``
       must be provided as described below, defaults to None
   :param http_path: Optional string specifying HTTP path of Databricks SQL warehouse or All Purpose cluster.
       If not specified, it should be either specified in the Databricks connection's
       extra parameters, or ``sql_warehouse_name`` must be specified.
   :param session_configuration: An optional dictionary of Spark session parameters. If not specified,
       it could be specified in the Databricks connection's extra parameters, defaults to None
   :param http_headers: An optional list of (k, v) pairs
       that will be set as HTTP headers on every request. (templated).
   :param catalog: An optional initial catalog to use.
       Requires Databricks Runtime version 9.0+ (templated), defaults to ""
   :param schema: An optional initial schema to use.
       Requires Databricks Runtime version 9.0+ (templated), defaults to "default"
   :param sql: SQL statement to be executed.
   :param handler: Handler for DbApiHook.run() to return results, defaults to fetch_all_handler
   :param client_parameters: Additional parameters internal to Databricks SQL connector parameters.

   .. py:attribute:: template_fields
      :type: Sequence[str]
      :value: ('databricks_conn_id', 'sql', 'catalog', 'schema', 'http_headers')

      

   .. py:attribute:: template_ext
      :type: Sequence[str]
      :value: ('.sql',)

      

   .. py:attribute:: template_fields_renderers

      

   .. py:method:: hook()

      Creates and returns a DatabricksSqlHook object.


   .. py:method:: poke(context)

      Sensor poke function to get and return results from the SQL sensor.



