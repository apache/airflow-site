:py:mod:`airflow.providers.databricks.operators.databricks_workflow`
====================================================================

.. py:module:: airflow.providers.databricks.operators.databricks_workflow


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.databricks.operators.databricks_workflow.WorkflowRunMetadata
   airflow.providers.databricks.operators.databricks_workflow.DatabricksWorkflowTaskGroup




.. py:class:: WorkflowRunMetadata


   Metadata for a Databricks workflow run.

   :param run_id: The ID of the Databricks workflow run.
   :param job_id: The ID of the Databricks workflow job.
   :param conn_id: The connection ID used to connect to Databricks.

   .. py:attribute:: conn_id
      :type: str

      

   .. py:attribute:: job_id
      :type: str

      

   .. py:attribute:: run_id
      :type: int

      


.. py:class:: DatabricksWorkflowTaskGroup(databricks_conn_id, existing_clusters = None, extra_job_params = None, jar_params = None, job_clusters = None, max_concurrent_runs = 1, notebook_packages = None, notebook_params = None, python_params = None, spark_submit_params = None, **kwargs)


   Bases: :py:obj:`airflow.utils.task_group.TaskGroup`

   A task group that takes a list of tasks and creates a databricks workflow.

   The DatabricksWorkflowTaskGroup takes a list of tasks and creates a databricks workflow
   based on the metadata produced by those tasks. For a task to be eligible for this
   TaskGroup, it must contain the ``_convert_to_databricks_workflow_task`` method. If any tasks
   do not contain this method then the Taskgroup will raise an error at parse time.

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:DatabricksWorkflowTaskGroup`

   :param databricks_conn_id: The name of the databricks connection to use.
   :param existing_clusters: A list of existing clusters to use for this workflow.
   :param extra_job_params: A dictionary containing properties which will override the default
       Databricks Workflow Job definitions.
   :param jar_params: A list of jar parameters to pass to the workflow. These parameters will be passed to all jar
       tasks in the workflow.
   :param job_clusters: A list of job clusters to use for this workflow.
   :param max_concurrent_runs: The maximum number of concurrent runs for this workflow.
   :param notebook_packages: A list of dictionary of Python packages to be installed. Packages defined
       at the workflow task group level are installed for each of the notebook tasks under it. And
       packages defined at the notebook task level are installed specific for the notebook task.
   :param notebook_params: A dictionary of notebook parameters to pass to the workflow. These parameters
       will be passed to all notebook tasks in the workflow.
   :param python_params: A list of python parameters to pass to the workflow. These parameters will be passed to
       all python tasks in the workflow.
   :param spark_submit_params: A list of spark submit parameters to pass to the workflow. These parameters
       will be passed to all spark submit tasks.

   .. py:attribute:: is_databricks
      :value: True

      

   .. py:method:: __exit__(_type, _value, _tb)

      Exit the context manager and add tasks to a single ``_CreateDatabricksWorkflowOperator``.



