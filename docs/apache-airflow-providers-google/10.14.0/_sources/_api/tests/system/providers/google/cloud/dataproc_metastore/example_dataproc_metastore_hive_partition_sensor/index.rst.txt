:py:mod:`tests.system.providers.google.cloud.dataproc_metastore.example_dataproc_metastore_hive_partition_sensor`
=================================================================================================================

.. py:module:: tests.system.providers.google.cloud.dataproc_metastore.example_dataproc_metastore_hive_partition_sensor

.. autoapi-nested-parse::

   Example Airflow DAG that show how to check Hive partitions existence
   using Dataproc Metastore Sensor.

   Note that Metastore service must be configured to use gRPC endpoints.



Module Contents
---------------

.. py:data:: DAG_ID
   :value: 'hive_partition_sensor'

   

.. py:data:: PROJECT_ID

   

.. py:data:: ENV_ID

   

.. py:data:: REGION
   :value: 'us-central1'

   

.. py:data:: NETWORK
   :value: 'default'

   

.. py:data:: METASTORE_SERVICE_ID

   

.. py:data:: METASTORE_TIMEOUT
   :value: 2400

   

.. py:data:: METASTORE_SERVICE

   

.. py:data:: METASTORE_SERVICE_QFN

   

.. py:data:: DATAPROC_CLUSTER_NAME

   

.. py:data:: DATAPROC_CLUSTER_CONFIG

   

.. py:data:: TABLE_NAME
   :value: 'transactions_partitioned'

   

.. py:data:: COLUMN
   :value: 'TransactionType'

   

.. py:data:: PARTITION_1

   

.. py:data:: PARTITION_2

   

.. py:data:: SOURCE_DATA_BUCKET
   :value: 'airflow-system-tests-resources'

   

.. py:data:: SOURCE_DATA_PATH
   :value: 'dataproc/hive'

   

.. py:data:: SOURCE_DATA_FILE_NAME
   :value: 'part-00000.parquet'

   

.. py:data:: EXTERNAL_TABLE_BUCKET
   :value: "{{task_instance.xcom_pull(task_ids='get_hive_warehouse_bucket_task', key='bucket')}}"

   

.. py:data:: QUERY_CREATE_EXTERNAL_TABLE

   

.. py:data:: QUERY_CREATE_PARTITIONED_TABLE

   

.. py:data:: QUERY_COPY_DATA_WITH_PARTITIONS

   

.. py:data:: create_metastore_service

   

.. py:data:: test_run

   

