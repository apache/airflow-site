:py:mod:`tests.system.providers.google.cloud.dataproc.example_dataproc_pyspark`
===============================================================================

.. py:module:: tests.system.providers.google.cloud.dataproc.example_dataproc_pyspark

.. autoapi-nested-parse::

   Example Airflow DAG for DataprocSubmitJobOperator with pyspark job.



Module Contents
---------------

.. py:data:: ENV_ID

   

.. py:data:: DAG_ID
   :value: 'dataproc_pyspark'

   

.. py:data:: PROJECT_ID

   

.. py:data:: BUCKET_NAME

   

.. py:data:: CLUSTER_NAME

   

.. py:data:: REGION
   :value: 'europe-west1'

   

.. py:data:: ZONE
   :value: 'europe-west1-b'

   

.. py:data:: CLUSTER_CONFIG

   

.. py:data:: JOB_FILE_NAME
   :value: 'dataproc-pyspark-job.py'

   

.. py:data:: JOB_FILE_LOCAL_PATH

   

.. py:data:: JOB_FILE_CONTENT
   :value: Multiline-String

    .. raw:: html

        <details><summary>Show Value</summary>

    .. code-block:: python

        """from operator import add
        from random import random
        
        from pyspark.sql import SparkSession
        
        
        def f(_: int) -> float:
            x = random() * 2 - 1
            y = random() * 2 - 1
            return 1 if x**2 + y**2 <= 1 else 0
        
        
        spark = SparkSession.builder.appName("PythonPi").getOrCreate()
        partitions = 2
        n = 100000 * partitions
        count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
        print(f"Pi is roughly {4.0 * count / n:f}")
        
        spark.stop()
        """

    .. raw:: html

        </details>

   

.. py:data:: PYSPARK_JOB

   

.. py:data:: create_bucket

   

.. py:data:: test_run

   

